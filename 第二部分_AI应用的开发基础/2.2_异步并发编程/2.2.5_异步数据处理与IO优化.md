# 2.2.5 异步数据处理与I/O优化

## 学习目标

优化AI应用中的数据处理性能，掌握异步I/O操作在文件处理、数据库操作、网络请求中的最佳实践。

## 2.2.5.1 AI应用中的I/O瓶颈分析

### I/O密集型场景识别

AI应用通常涉及大量的I/O操作，这些操作往往成为性能瓶颈：

1. **训练数据加载**：大量图片、文档、音频文件的读取
2. **模型文件操作**：模型加载、保存、版本管理
3. **知识库检索**：向量数据库查询、文档索引
4. **用户数据处理**：文件上传下载、数据预处理
5. **外部API调用**：第三方服务集成、模型推理API

### 同步I/O的性能问题

```java
// 传统同步I/O处理方式的问题示例
public class SyncIOExample {
    
    public List<ProcessedDocument> processDocuments(List<String> filePaths) {
        List<ProcessedDocument> results = new ArrayList<>();
        
        for (String filePath : filePaths) {
            // 问题1：串行文件读取
            String content = readFile(filePath);  // 阻塞500ms
            
            // 问题2：串行外部API调用
            String analysis = callAnalysisAPI(content);  // 阻塞2000ms
            
            // 问题3：串行数据库写入
            saveToDatabase(filePath, analysis);  // 阻塞200ms
            
            results.add(new ProcessedDocument(filePath, analysis));
        }
        
        return results; // 总时间：文件数量 × (500 + 2000 + 200)ms
    }
}
```

**问题分析：**
- 10个文件处理需要27秒（10 × 2.7秒）
- CPU大部分时间处于等待状态
- 无法充分利用系统资源
- 用户体验极差

## 2.2.5.2 异步文件处理优化

### 基于NIO的异步文件操作

```java
// AsyncFileProcessor.java - 异步文件处理器
import java.nio.file.*;
import java.nio.charset.StandardCharsets;
import java.util.concurrent.CompletableFuture;

public class AsyncFileProcessor {
    
    private final ExecutorService ioExecutor = Executors.newFixedThreadPool(10);
    
    /**
     * 异步并行文件读取
     */
    public CompletableFuture<List<FileContent>> readFilesAsync(List<Path> filePaths) {
        List<CompletableFuture<FileContent>> futures = filePaths.stream()
            .map(this::readSingleFileAsync)
            .collect(Collectors.toList());
        
        return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
            .thenApply(v -> futures.stream()
                .map(CompletableFuture::join)
                .collect(Collectors.toList()));
    }
    
    private CompletableFuture<FileContent> readSingleFileAsync(Path filePath) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                long startTime = System.currentTimeMillis();
                byte[] bytes = Files.readAllBytes(filePath);
                String content = new String(bytes, StandardCharsets.UTF_8);
                long duration = System.currentTimeMillis() - startTime;
                
                logger.info("File {} read in {}ms", filePath.getFileName(), duration);
                return new FileContent(filePath.toString(), content, bytes.length);
                
            } catch (IOException e) {
                logger.error("Failed to read file: {}", filePath, e);
                throw new UncheckedIOException(e);
            }
        }, ioExecutor);
    }
    
    /**
     * 异步文件写入（支持批量操作）
     */
    public CompletableFuture<Void> writeFilesAsync(List<FileWriteRequest> requests) {
        List<CompletableFuture<Void>> futures = requests.stream()
            .map(this::writeSingleFileAsync)
            .collect(Collectors.toList());
        
        return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]));
    }
    
    private CompletableFuture<Void> writeSingleFileAsync(FileWriteRequest request) {
        return CompletableFuture.runAsync(() -> {
            try {
                Path filePath = Paths.get(request.getFilePath());
                // 确保父目录存在
                Files.createDirectories(filePath.getParent());
                
                // 异步写入文件
                Files.write(filePath, request.getContent().getBytes(StandardCharsets.UTF_8),
                          StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING);
                
                logger.info("File {} written successfully", filePath.getFileName());
                
            } catch (IOException e) {
                logger.error("Failed to write file: {}", request.getFilePath(), e);
                throw new UncheckedIOException(e);
            }
        }, ioExecutor);
    }
    
    /**
     * 流式处理大文件
     */
    public CompletableFuture<ProcessingResult> processLargeFileAsync(Path filePath) {
        return CompletableFuture.supplyAsync(() -> {
            ProcessingResult result = new ProcessingResult();
            
            try (Stream<String> lines = Files.lines(filePath, StandardCharsets.UTF_8)) {
                lines.parallel()
                     .filter(line -> !line.trim().isEmpty())
                     .map(this::processLine)
                     .forEach(result::addProcessedLine);
                     
            } catch (IOException e) {
                throw new UncheckedIOException(e);
            }
            
            return result;
        }, ioExecutor);
    }
    
    private String processLine(String line) {
        // 行处理逻辑
        return line.toUpperCase().trim();
    }
}

// 文件内容封装类
public class FileContent {
    private final String filePath;
    private final String content;
    private final long size;
    
    public FileContent(String filePath, String content, long size) {
        this.filePath = filePath;
        this.content = content;
        this.size = size;
    }
    
    // getters...
}
```

### Python异步文件处理

基于Genie Tool项目中的文件处理实现：

```python
# async_file_processor.py - Python异步文件处理
import aiofiles
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor
import pandas as pd

class AsyncFileProcessor:
    """异步文件处理器"""
    
    def __init__(self, max_concurrent_files: int = 10):
        self.semaphore = asyncio.Semaphore(max_concurrent_files)
        self.executor = ThreadPoolExecutor(max_workers=5)
    
    async def download_files_parallel(self, file_names: List[str], work_dir: str) -> List[Dict[str, Any]]:
        """并行下载多个文件"""
        download_tasks = []
        
        for file_name in file_names:
            task = self._download_single_file(file_name, work_dir)
            download_tasks.append(task)
        
        # 并发执行下载任务
        results = await asyncio.gather(*download_tasks, return_exceptions=True)
        
        # 过滤掉异常结果
        successful_downloads = [
            result for result in results 
            if not isinstance(result, Exception) and result is not None
        ]
        
        return successful_downloads
    
    async def _download_single_file(self, file_name: str, work_dir: str) -> Optional[Dict[str, Any]]:
        """下载单个文件"""
        async with self.semaphore:  # 控制并发数
            try:
                # 模拟从文件服务下载
                file_url = f"http://file-service/download/{file_name}"
                file_path = os.path.join(work_dir, file_name)
                
                async with aiohttp.ClientSession() as session:
                    async with session.get(file_url) as response:
                        if response.status == 200:
                            content = await response.read()
                            
                            # 异步写入文件
                            async with aiofiles.open(file_path, 'wb') as f:
                                await f.write(content)
                            
                            return {
                                "file_name": file_name,
                                "file_path": file_path,
                                "size": len(content)
                            }
                        else:
                            logger.error(f"Failed to download {file_name}: {response.status}")
                            return None
                            
            except Exception as e:
                logger.error(f"Error downloading {file_name}: {e}")
                return None
    
    async def process_files_by_type(self, file_infos: List[Dict[str, Any]], 
                                   max_abstract_size: int = 2000) -> List[Dict[str, Any]]:
        """根据文件类型并行处理文件"""
        processing_tasks = []
        
        for file_info in file_infos:
            task = self._process_single_file(file_info, max_abstract_size)
            processing_tasks.append(task)
        
        # 并发处理文件
        processed_files = await asyncio.gather(*processing_tasks, return_exceptions=True)
        
        # 返回成功处理的文件
        return [f for f in processed_files if not isinstance(f, Exception) and f is not None]
    
    async def _process_single_file(self, file_info: Dict[str, Any], 
                                  max_abstract_size: int) -> Optional[Dict[str, Any]]:
        """处理单个文件"""
        file_name = file_info.get("file_name", "")
        file_path = file_info.get("file_path", "")
        
        if not file_name or not file_path:
            return None
        
        file_ext = file_name.split(".")[-1].lower()
        
        try:
            if file_ext in ["xlsx", "xls", "csv"]:
                return await self._process_spreadsheet_file(file_path, file_ext)
            elif file_ext in ["txt", "md", "html"]:
                return await self._process_text_file(file_path, max_abstract_size)
            elif file_ext in ["pdf"]:
                return await self._process_pdf_file(file_path, max_abstract_size)
            else:
                logger.warning(f"Unsupported file type: {file_ext}")
                return None
                
        except Exception as e:
            logger.error(f"Error processing file {file_name}: {e}")
            return None
    
    async def _process_spreadsheet_file(self, file_path: str, file_ext: str) -> Dict[str, Any]:
        """处理表格文件（CPU密集型，使用线程池）"""
        loop = asyncio.get_event_loop()
        
        def _read_spreadsheet():
            pd.set_option("display.max_columns", None)
            if file_ext == "csv":
                df = pd.read_csv(file_path)
            else:
                df = pd.read_excel(file_path)
            return str(df.head(10))
        
        # 在线程池中执行CPU密集型操作
        abstract = await loop.run_in_executor(self.executor, _read_spreadsheet)
        
        return {
            "path": file_path,
            "type": "spreadsheet",
            "abstract": abstract
        }
    
    async def _process_text_file(self, file_path: str, max_abstract_size: int) -> Dict[str, Any]:
        """处理文本文件"""
        async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
            content = await f.read()
            
        return {
            "path": file_path,
            "type": "text",
            "abstract": content[:max_abstract_size]
        }
    
    async def _process_pdf_file(self, file_path: str, max_abstract_size: int) -> Dict[str, Any]:
        """处理PDF文件（使用线程池处理）"""
        loop = asyncio.get_event_loop()
        
        def _extract_pdf_text():
            # 使用PyPDF2或其他PDF处理库
            # 这里是示例代码
            import PyPDF2
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page in pdf_reader.pages[:5]:  # 只处理前5页
                    text += page.extract_text()
                return text[:max_abstract_size]
        
        try:
            abstract = await loop.run_in_executor(self.executor, _extract_pdf_text)
            return {
                "path": file_path,
                "type": "pdf",
                "abstract": abstract
            }
        except Exception as e:
            logger.error(f"Failed to process PDF {file_path}: {e}")
            return {
                "path": file_path,
                "type": "pdf",
                "abstract": "PDF processing failed"
            }

# 使用示例
async def process_user_files(file_names: List[str], work_dir: str) -> List[Dict[str, Any]]:
    """处理用户上传的文件"""
    processor = AsyncFileProcessor(max_concurrent_files=5)
    
    # 第一阶段：并行下载文件
    logger.info(f"Downloading {len(file_names)} files...")
    downloaded_files = await processor.download_files_parallel(file_names, work_dir)
    logger.info(f"Downloaded {len(downloaded_files)} files successfully")
    
    # 第二阶段：并行处理文件
    logger.info("Processing files...")
    processed_files = await processor.process_files_by_type(downloaded_files)
    logger.info(f"Processed {len(processed_files)} files successfully")
    
    return processed_files
```

## 2.2.5.3 异步数据库操作

### SQLModel异步数据库访问

基于Genie Tool项目的数据库引擎实现：

```python
# async_db_operations.py - 异步数据库操作
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy import select, update, delete
from typing import List, Optional, Dict, Any
import asyncio

# 数据库引擎配置
SQLITE_DB_PATH = os.environ.get("SQLITE_DB_PATH", "ai_app.db")
async_engine = create_async_engine(
    f"sqlite+aiosqlite:///{SQLITE_DB_PATH}",
    poolclass=AsyncAdaptedQueuePool,
    pool_size=20,  # 增加连接池大小
    pool_recycle=3600,
    echo=False,
)

AsyncSessionLocal = sessionmaker(
    bind=async_engine, 
    class_=AsyncSession,
    expire_on_commit=False
)

class AsyncFileRepository:
    """异步文件信息仓库"""
    
    async def batch_insert_files(self, file_infos: List[Dict[str, Any]]) -> List[str]:
        """批量插入文件信息"""
        async with AsyncSessionLocal() as session:
            try:
                file_ids = []
                
                # 批量创建文件对象
                file_objects = []
                for file_info in file_infos:
                    file_obj = FileInfo(
                        file_name=file_info["file_name"],
                        file_path=file_info["file_path"],
                        file_size=file_info.get("file_size", 0),
                        file_type=file_info.get("file_type", "unknown"),
                        metadata=file_info.get("metadata", {})
                    )
                    file_objects.append(file_obj)
                    file_ids.append(file_obj.id)
                
                # 批量插入
                session.add_all(file_objects)
                await session.commit()
                
                logger.info(f"Batch inserted {len(file_objects)} files")
                return file_ids
                
            except Exception as e:
                await session.rollback()
                logger.error(f"Batch insert failed: {e}")
                raise
    
    async def parallel_query_files(self, query_conditions: List[Dict[str, Any]]) -> List[FileInfo]:
        """并行查询文件信息"""
        query_tasks = []
        
        for condition in query_conditions:
            task = self._query_files_by_condition(condition)
            query_tasks.append(task)
        
        # 并发执行查询
        results = await asyncio.gather(*query_tasks, return_exceptions=True)
        
        # 合并结果
        all_files = []
        for result in results:
            if not isinstance(result, Exception):
                all_files.extend(result)
        
        return all_files
    
    async def _query_files_by_condition(self, condition: Dict[str, Any]) -> List[FileInfo]:
        """根据条件查询文件"""
        async with AsyncSessionLocal() as session:
            try:
                query = select(FileInfo)
                
                # 动态构建查询条件
                if "file_type" in condition:
                    query = query.where(FileInfo.file_type == condition["file_type"])
                if "min_size" in condition:
                    query = query.where(FileInfo.file_size >= condition["min_size"])
                if "max_size" in condition:
                    query = query.where(FileInfo.file_size <= condition["max_size"])
                if "created_after" in condition:
                    query = query.where(FileInfo.created_at >= condition["created_after"])
                
                result = await session.execute(query)
                files = result.scalars().all()
                
                return list(files)
                
            except Exception as e:
                logger.error(f"Query failed: {e}")
                return []
    
    async def batch_update_files(self, updates: List[Dict[str, Any]]) -> int:
        """批量更新文件信息"""
        async with AsyncSessionLocal() as session:
            try:
                updated_count = 0
                
                for update_info in updates:
                    file_id = update_info["file_id"]
                    update_data = update_info["update_data"]
                    
                    query = update(FileInfo).where(FileInfo.id == file_id).values(**update_data)
                    result = await session.execute(query)
                    updated_count += result.rowcount
                
                await session.commit()
                logger.info(f"Batch updated {updated_count} files")
                return updated_count
                
            except Exception as e:
                await session.rollback()
                logger.error(f"Batch update failed: {e}")
                raise

class AsyncDatabasePool:
    """异步数据库连接池管理器"""
    
    def __init__(self, max_connections: int = 20):
        self.connection_semaphore = asyncio.Semaphore(max_connections)
        self.active_connections = 0
    
    async def execute_with_pool(self, operation):
        """使用连接池执行数据库操作"""
        async with self.connection_semaphore:
            try:
                self.active_connections += 1
                result = await operation()
                return result
            finally:
                self.active_connections -= 1
    
    async def execute_parallel_operations(self, operations: List[callable]) -> List[Any]:
        """并行执行多个数据库操作"""
        tasks = [
            self.execute_with_pool(operation)
            for operation in operations
        ]
        
        return await asyncio.gather(*tasks, return_exceptions=True)
```

### Java异步数据库访问

```java
// AsyncDatabaseService.java - Java异步数据库服务
@Service
public class AsyncDatabaseService {
    
    private final JdbcTemplate jdbcTemplate;
    private final CompletableFuture executor;
    
    @Autowired
    public AsyncDatabaseService(JdbcTemplate jdbcTemplate) {
        this.jdbcTemplate = jdbcTemplate;
    }
    
    /**
     * 异步批量插入
     */
    public CompletableFuture<Integer[]> batchInsertAsync(String sql, List<Object[]> batchArgs) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                return jdbcTemplate.batchUpdate(sql, batchArgs);
            } catch (Exception e) {
                logger.error("Batch insert failed", e);
                throw new RuntimeException(e);
            }
        });
    }
    
    /**
     * 异步并行查询
     */
    public CompletableFuture<List<Map<String, Object>>> parallelQueryAsync(List<String> queries) {
        List<CompletableFuture<List<Map<String, Object>>>> futures = queries.stream()
            .map(this::singleQueryAsync)
            .collect(Collectors.toList());
        
        return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
            .thenApply(v -> futures.stream()
                .map(CompletableFuture::join)
                .flatMap(List::stream)
                .collect(Collectors.toList()));
    }
    
    private CompletableFuture<List<Map<String, Object>>> singleQueryAsync(String sql) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                return jdbcTemplate.queryForList(sql);
            } catch (Exception e) {
                logger.error("Query failed: " + sql, e);
                return new ArrayList<>();
            }
        });
    }
    
    /**
     * 事务性异步操作
     */
    @Transactional
    public CompletableFuture<Boolean> transactionalOperationAsync(
            List<DatabaseOperation> operations) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                for (DatabaseOperation operation : operations) {
                    operation.execute(jdbcTemplate);
                }
                return true;
            } catch (Exception e) {
                logger.error("Transactional operation failed", e);
                throw new RuntimeException(e);
            }
        });
    }
}

// 数据库操作接口
@FunctionalInterface
public interface DatabaseOperation {
    void execute(JdbcTemplate jdbcTemplate) throws Exception;
}
```

## 2.2.5.4 网络请求异步优化

### HTTP客户端异步调用

```java
// AsyncHttpClient.java - 异步HTTP客户端
public class AsyncHttpClient {
    
    private final OkHttpClient httpClient;
    private final ObjectMapper objectMapper;
    
    public AsyncHttpClient() {
        this.httpClient = new OkHttpClient.Builder()
            .connectTimeout(30, TimeUnit.SECONDS)
            .readTimeout(60, TimeUnit.SECONDS)
            .writeTimeout(60, TimeUnit.SECONDS)
            .connectionPool(new ConnectionPool(20, 5, TimeUnit.MINUTES))
            .build();
        this.objectMapper = new ObjectMapper();
    }
    
    /**
     * 并行HTTP请求处理
     */
    public CompletableFuture<List<ApiResponse>> parallelRequests(List<ApiRequest> requests) {
        List<CompletableFuture<ApiResponse>> futures = requests.stream()
            .map(this::singleRequestAsync)
            .collect(Collectors.toList());
        
        return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
            .thenApply(v -> futures.stream()
                .map(future -> {
                    try {
                        return future.get();
                    } catch (Exception e) {
                        logger.error("Request failed", e);
                        return ApiResponse.error(e.getMessage());
                    }
                })
                .collect(Collectors.toList()));
    }
    
    private CompletableFuture<ApiResponse> singleRequestAsync(ApiRequest apiRequest) {
        CompletableFuture<ApiResponse> future = new CompletableFuture<>();
        
        RequestBody body = RequestBody.create(
            apiRequest.getBody(), 
            MediaType.parse("application/json")
        );
        
        Request request = new Request.Builder()
            .url(apiRequest.getUrl())
            .method(apiRequest.getMethod(), body)
            .build();
        
        httpClient.newCall(request).enqueue(new Callback() {
            @Override
            public void onFailure(Call call, IOException e) {
                future.complete(ApiResponse.error(e.getMessage()));
            }
            
            @Override
            public void onResponse(Call call, Response response) throws IOException {
                try (ResponseBody responseBody = response.body()) {
                    String responseText = responseBody != null ? responseBody.string() : "";
                    
                    if (response.isSuccessful()) {
                        future.complete(ApiResponse.success(responseText));
                    } else {
                        future.complete(ApiResponse.error("HTTP " + response.code() + ": " + responseText));
                    }
                } catch (Exception e) {
                    future.complete(ApiResponse.error(e.getMessage()));
                }
            }
        });
        
        return future;
    }
    
    /**
     * 带重试的异步请求
     */
    public CompletableFuture<ApiResponse> requestWithRetry(ApiRequest request, int maxRetries) {
        return attemptRequest(request, 0, maxRetries);
    }
    
    private CompletableFuture<ApiResponse> attemptRequest(ApiRequest request, int attempt, int maxRetries) {
        return singleRequestAsync(request)
            .thenCompose(response -> {
                if (response.isSuccess() || attempt >= maxRetries) {
                    return CompletableFuture.completedFuture(response);
                } else {
                    // 指数退避重试
                    long delay = Math.min(1000 * (1L << attempt), 30000);
                    return CompletableFuture
                        .delayedExecutor(delay, TimeUnit.MILLISECONDS)
                        .execute(() -> attemptRequest(request, attempt + 1, maxRetries));
                }
            });
    }
}
```

### Python异步网络请求

```python
# async_http_client.py - Python异步HTTP客户端
import aiohttp
import asyncio
from typing import List, Dict, Any, Optional
import json

class AsyncHttpClient:
    """异步HTTP客户端"""
    
    def __init__(self, max_concurrent_requests: int = 20, timeout: int = 30):
        self.semaphore = asyncio.Semaphore(max_concurrent_requests)
        self.timeout = aiohttp.ClientTimeout(total=timeout)
        
    async def parallel_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """并行发送多个HTTP请求"""
        async with aiohttp.ClientSession(timeout=self.timeout) as session:
            tasks = [
                self._single_request(session, request)
                for request in requests
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # 处理异常结果
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    processed_results.append({
                        "request_id": requests[i].get("id", f"req_{i}"),
                        "success": False,
                        "error": str(result),
                        "data": None
                    })
                else:
                    processed_results.append(result)
            
            return processed_results
    
    async def _single_request(self, session: aiohttp.ClientSession, 
                             request_config: Dict[str, Any]) -> Dict[str, Any]:
        """发送单个HTTP请求"""
        async with self.semaphore:
            try:
                method = request_config.get("method", "GET").upper()
                url = request_config["url"]
                headers = request_config.get("headers", {})
                data = request_config.get("data")
                
                if data:
                    headers["Content-Type"] = "application/json"
                    data = json.dumps(data)
                
                start_time = asyncio.get_event_loop().time()
                
                async with session.request(method, url, headers=headers, data=data) as response:
                    response_data = await response.text()
                    duration = asyncio.get_event_loop().time() - start_time
                    
                    return {
                        "request_id": request_config.get("id", url),
                        "success": response.status < 400,
                        "status": response.status,
                        "data": response_data,
                        "duration": duration
                    }
                    
            except Exception as e:
                return {
                    "request_id": request_config.get("id", request_config.get("url", "unknown")),
                    "success": False,
                    "error": str(e),
                    "data": None
                }
    
    async def request_with_retry(self, request_config: Dict[str, Any], 
                               max_retries: int = 3) -> Dict[str, Any]:
        """带重试机制的请求"""
        last_error = None
        
        for attempt in range(max_retries + 1):
            try:
                async with aiohttp.ClientSession(timeout=self.timeout) as session:
                    result = await self._single_request(session, request_config)
                    
                    if result["success"]:
                        if attempt > 0:
                            logger.info(f"Request succeeded after {attempt} retries")
                        return result
                    else:
                        last_error = result.get("error", "Unknown error")
                        
            except Exception as e:
                last_error = str(e)
            
            if attempt < max_retries:
                # 指数退避
                delay = min(2 ** attempt, 30)
                logger.warning(f"Request failed, retrying in {delay}s (attempt {attempt + 1}/{max_retries})")
                await asyncio.sleep(delay)
        
        return {
            "request_id": request_config.get("id", "unknown"),
            "success": False,
            "error": f"Failed after {max_retries} retries: {last_error}",
            "data": None
        }
```

## 2.2.5.5 缓存策略与数据预加载

### 多级异步缓存系统

```java
// AsyncCacheManager.java - 异步缓存管理器
@Component
public class AsyncCacheManager {
    
    private final ConcurrentHashMap<String, CompletableFuture<Object>> loadingCache = new ConcurrentHashMap<>();
    private final Cache<String, Object> localCache;
    private final RedisTemplate<String, Object> redisTemplate;
    
    public AsyncCacheManager(RedisTemplate<String, Object> redisTemplate) {
        this.redisTemplate = redisTemplate;
        this.localCache = Caffeine.newBuilder()
            .maximumSize(10000)
            .expireAfterWrite(10, TimeUnit.MINUTES)
            .buildAsync();
    }
    
    /**
     * 异步获取缓存数据
     */
    public CompletableFuture<Object> getAsync(String key, Supplier<CompletableFuture<Object>> loader) {
        // 先检查本地缓存
        Object localValue = localCache.getIfPresent(key);
        if (localValue != null) {
            return CompletableFuture.completedFuture(localValue);
        }
        
        // 检查是否正在加载
        return loadingCache.computeIfAbsent(key, k -> 
            loadFromDistributedCache(k)
                .thenCompose(value -> {
                    if (value != null) {
                        localCache.put(k, value);
                        return CompletableFuture.completedFuture(value);
                    } else {
                        return loader.get()
                            .thenApply(loadedValue -> {
                                localCache.put(k, loadedValue);
                                putToDistributedCacheAsync(k, loadedValue);
                                return loadedValue;
                            });
                    }
                })
                .whenComplete((v, ex) -> loadingCache.remove(k))
        );
    }
    
    private CompletableFuture<Object> loadFromDistributedCache(String key) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                return redisTemplate.opsForValue().get(key);
            } catch (Exception e) {
                logger.warn("Failed to load from Redis cache: " + key, e);
                return null;
            }
        });
    }
    
    private void putToDistributedCacheAsync(String key, Object value) {
        CompletableFuture.runAsync(() -> {
            try {
                redisTemplate.opsForValue().set(key, value, Duration.ofHours(1));
            } catch (Exception e) {
                logger.warn("Failed to put to Redis cache: " + key, e);
            }
        });
    }
    
    /**
     * 批量预加载缓存
     */
    public CompletableFuture<Void> preloadCache(Map<String, Supplier<CompletableFuture<Object>>> preloadTasks) {
        List<CompletableFuture<Void>> futures = preloadTasks.entrySet().stream()
            .map(entry -> getAsync(entry.getKey(), entry.getValue()).thenAccept(v -> {}))
            .collect(Collectors.toList());
        
        return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]));
    }
}
```

### 智能数据预取策略

```python
# smart_prefetcher.py - 智能数据预取器
import asyncio
import time
from typing import Dict, Any, List, Callable, Optional
from collections import defaultdict

class SmartDataPrefetcher:
    """智能数据预取器"""
    
    def __init__(self, cache_size: int = 1000):
        self.cache: Dict[str, Any] = {}
        self.access_patterns: Dict[str, List[float]] = defaultdict(list)
        self.cache_size = cache_size
        self.prefetch_threshold = 0.7  # 预测概率阈值
        
    async def get_with_prefetch(self, key: str, loader: Callable) -> Any:
        """获取数据并进行智能预取"""
        # 记录访问模式
        self._record_access(key)
        
        # 如果缓存中有数据，直接返回
        if key in self.cache:
            # 异步预取相关数据
            asyncio.create_task(self._prefetch_related_data(key))
            return self.cache[key]
        
        # 加载数据
        data = await loader()
        self._put_cache(key, data)
        
        # 预取相关数据
        asyncio.create_task(self._prefetch_related_data(key))
        
        return data
    
    def _record_access(self, key: str):
        """记录访问模式"""
        current_time = time.time()
        self.access_patterns[key].append(current_time)
        
        # 只保留最近的访问记录
        cutoff_time = current_time - 3600  # 1小时
        self.access_patterns[key] = [
            t for t in self.access_patterns[key] if t > cutoff_time
        ]
    
    async def _prefetch_related_data(self, accessed_key: str):
        """预取相关数据"""
        # 分析访问模式，预测可能需要的数据
        related_keys = self._predict_related_keys(accessed_key)
        
        # 并行预取
        prefetch_tasks = []
        for key in related_keys:
            if key not in self.cache:
                task = self._prefetch_single_key(key)
                prefetch_tasks.append(task)
        
        if prefetch_tasks:
            await asyncio.gather(*prefetch_tasks, return_exceptions=True)
    
    def _predict_related_keys(self, accessed_key: str) -> List[str]:
        """预测相关的键"""
        # 简单的模式匹配策略
        related_keys = []
        
        # 基于键的前缀匹配
        if ":" in accessed_key:
            prefix = accessed_key.split(":")[0]
            for key in self.access_patterns.keys():
                if key.startswith(prefix) and key != accessed_key:
                    # 计算访问相关性
                    correlation = self._calculate_correlation(accessed_key, key)
                    if correlation > self.prefetch_threshold:
                        related_keys.append(key)
        
        return related_keys[:5]  # 最多预取5个相关键
    
    def _calculate_correlation(self, key1: str, key2: str) -> float:
        """计算两个键的访问相关性"""
        accesses1 = self.access_patterns[key1]
        accesses2 = self.access_patterns[key2]
        
        if not accesses1 or not accesses2:
            return 0.0
        
        # 简单的时间窗口相关性计算
        correlation_count = 0
        window_size = 300  # 5分钟窗口
        
        for t1 in accesses1:
            for t2 in accesses2:
                if abs(t1 - t2) <= window_size:
                    correlation_count += 1
        
        max_possible = min(len(accesses1), len(accesses2))
        return correlation_count / max_possible if max_possible > 0 else 0.0
    
    async def _prefetch_single_key(self, key: str):
        """预取单个键的数据"""
        try:
            # 这里需要根据键获取对应的加载器
            loader = self._get_loader_for_key(key)
            if loader:
                data = await loader()
                self._put_cache(key, data)
        except Exception as e:
            logger.debug(f"Prefetch failed for key {key}: {e}")
    
    def _get_loader_for_key(self, key: str) -> Optional[Callable]:
        """根据键获取对应的数据加载器"""
        # 这里需要实现具体的加载器映射逻辑
        return None
    
    def _put_cache(self, key: str, data: Any):
        """将数据放入缓存"""
        if len(self.cache) >= self.cache_size:
            # LRU淘汰策略（简化实现）
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        self.cache[key] = data
```

## 2.2.5.6 数据流处理优化

### 响应式数据流

```java
// ReactiveDataProcessor.java - 响应式数据处理器
@Component
public class ReactiveDataProcessor {
    
    /**
     * 流式处理大数据集
     */
    public Flux<ProcessedData> processDataStream(Flux<RawData> dataStream) {
        return dataStream
            .buffer(100) // 批量处理
            .flatMap(this::processBatch, 4) // 4个并发批次
            .onBackpressureDrop() // 背压处理
            .doOnError(error -> logger.error("Stream processing error", error))
            .retry(3); // 重试机制
    }
    
    private Flux<ProcessedData> processBatch(List<RawData> batch) {
        return Flux.fromIterable(batch)
            .parallel(4)
            .runOn(Schedulers.parallel())
            .map(this::processData)
            .sequential();
    }
    
    private ProcessedData processData(RawData data) {
        // CPU密集型处理逻辑
        try {
            Thread.sleep(100); // 模拟处理时间
            return new ProcessedData(data.getId(), data.getContent().toUpperCase());
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new RuntimeException(e);
        }
    }
    
    /**
     * 异步数据聚合
     */
    public CompletableFuture<AggregationResult> aggregateDataAsync(
            List<DataSource> dataSources) {
        
        List<CompletableFuture<PartialResult>> futures = dataSources.stream()
            .map(this::processDataSourceAsync)
            .collect(Collectors.toList());
        
        return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
            .thenApply(v -> {
                List<PartialResult> results = futures.stream()
                    .map(CompletableFuture::join)
                    .collect(Collectors.toList());
                
                return aggregateResults(results);
            });
    }
    
    private CompletableFuture<PartialResult> processDataSourceAsync(DataSource dataSource) {
        return CompletableFuture.supplyAsync(() -> {
            // 处理单个数据源
            return new PartialResult(dataSource.process());
        });
    }
}
```

## 小结

异步数据处理与I/O优化是AI应用性能提升的关键技术。通过合理的异步文件处理、数据库访问、网络请求和缓存策略，可以显著提高应用的响应速度和吞吐量。

关键要点：
1. **I/O并行化**：将串行I/O操作转换为并行处理
2. **连接池管理**：合理配置数据库和HTTP连接池
3. **多级缓存**：本地缓存+分布式缓存的组合策略
4. **智能预取**：基于访问模式的数据预加载
5. **背压控制**：处理数据流中的速率不匹配问题

---

**性能优化建议：**
1. 识别应用中的I/O瓶颈点
2. 根据场景选择合适的异步模式
3. 实施有效的缓存和预取策略
4. 监控异步操作的性能指标
5. 定期进行性能测试和调优
