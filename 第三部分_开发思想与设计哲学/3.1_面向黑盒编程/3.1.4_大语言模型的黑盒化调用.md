# 3.1.4 大语言模型的黑盒化调用

## 学习目标
了解如何封装LLM调用接口，实现多模型厂商的统一接入和无缝切换，屏蔽底层模型差异。

## LLM黑盒化调用的必要性

在AGI应用开发中，大语言模型（LLM）是核心能力提供者，但面临着多个挑战：

1. **多厂商差异**：OpenAI、Claude、ChatGLM等不同厂商的API接口格式各异
2. **参数格式不统一**：不同模型的输入输出参数结构和命名规则不同  
3. **功能特性差异**：某些模型支持流式输出，某些支持工具调用，功能特性不完全一致
4. **版本兼容性**：模型API经常更新，需要处理版本兼容问题
5. **成本和性能权衡**：不同场景下需要灵活切换不同成本和性能特征的模型

LLM的黑盒化调用通过统一接口抽象，将这些差异性封装在接口实现层，使上层AGI应用代码与具体的模型厂商解耦，实现模型的可替换性和系统的灵活性。

## 统一LLM接口设计

### 核心接口抽象

设计统一的LLM调用接口，屏蔽底层实现差异：

```java
/**
 * LLM统一调用接口
 */
public interface LLMInterface {
    
    /**
     * 基础文本生成
     * @param context 执行上下文
     * @param messages 消息列表  
     * @param systemMsgs 系统消息
     * @param stream 是否流式输出
     * @param temperature 温度参数
     * @return 异步结果
     */
    CompletableFuture<String> ask(
        AgentContext context,
        List<Message> messages, 
        List<Message> systemMsgs,
        boolean stream, 
        Double temperature
    );
    
    /**
     * 带工具调用的文本生成
     * @param context 执行上下文
     * @param messages 消息列表
     * @param systemMsgs 系统消息  
     * @param tools 可用工具列表
     * @param toolChoice 工具选择策略
     * @param stream 是否流式输出
     * @param temperature 温度参数
     * @return 异步结果
     */
    CompletableFuture<String> askWithTools(
        AgentContext context,
        List<Message> messages,
        List<Message> systemMsgs, 
        List<BaseTool> tools,
        ToolChoice toolChoice,
        boolean stream,
        Double temperature
    );
    
    /**
     * 获取模型配置信息
     */
    LLMSettings getSettings();
    
    /**
     * 获取token计数器
     */
    TokenCounter getTokenCounter();
}
```

### LLM实现类设计

基于统一接口实现具体的LLM调用类：

```java
/**
 * LLM统一调用实现类
 */
@Slf4j
@Data
public class LLM implements LLMInterface {
    
    private static final Map<String, LLM> instances = new ConcurrentHashMap<>();
    
    // 模型配置参数
    private final String model;
    private final String llmErp; 
    private final int maxTokens;
    private final double temperature;
    private final String apiKey;
    private final String baseUrl;
    private final String interfaceUrl;
    private final String functionCallType;
    private final TokenCounter tokenCounter;
    private final ObjectMapper objectMapper;
    private final Map<String, Object> extParams;
    
    // 统计信息
    private int totalInputTokens;
    private Integer maxInputTokens;
    
    /**
     * 构造方法 - 根据模型名称和ERP初始化LLM实例
     */
    public LLM(String modelName, String llmErp) {
        this.llmErp = llmErp;
        
        // 从配置中心加载模型配置
        LLMSettings config = Config.getLLMConfig(modelName);
        this.model = config.getModel();
        this.maxTokens = config.getMaxTokens();
        this.temperature = config.getTemperature();
        this.apiKey = config.getApiKey();
        this.baseUrl = config.getBaseUrl();
        this.interfaceUrl = StringUtils.isNotEmpty(config.getInterfaceUrl()) 
            ? config.getInterfaceUrl() : "/v1/chat/completions";
        this.functionCallType = config.getFunctionCallType();
        this.maxInputTokens = config.getMaxInputTokens();
        this.extParams = config.getExtParams();
        
        // 初始化组件
        this.totalInputTokens = 0;
        this.tokenCounter = new TokenCounter();
        this.objectMapper = new ObjectMapper();
    }
    
    /**
     * 单例模式获取LLM实例
     */
    public static LLM getInstance(String modelName, String llmErp) {
        String key = modelName + "_" + (llmErp != null ? llmErp : "default");
        return instances.computeIfAbsent(key, k -> new LLM(modelName, llmErp));
    }
    
    @Override
    public CompletableFuture<String> ask(
            AgentContext context,
            List<Message> messages,
            List<Message> systemMsgs,
            boolean stream,
            Double temperature) {
        
        try {
            // 格式化消息
            List<Map<String, Object>> formattedMessages;
            if (systemMsgs != null && !systemMsgs.isEmpty()) {
                List<Map<String, Object>> formattedSystemMsgs = formatMessages(systemMsgs, false);
                formattedMessages = new ArrayList<>(formattedSystemMsgs);
                formattedMessages.addAll(formatMessages(messages, isClaudeModel()));
            } else {
                formattedMessages = formatMessages(messages, isClaudeModel());
            }
            
            // 构建请求参数
            Map<String, Object> params = buildRequestParams(formattedMessages, temperature);
            
            log.info("{} call llm ask request {}", context.getRequestId(), 
                     JSONObject.toJSONString(params));
            
            // 根据是否流式调用不同方法
            if (!stream) {
                params.put("stream", false);
                return callOpenAI(params).thenApply(response -> parseResponse(context, response));
            } else {
                params.put("stream", true);
                return callOpenAIStream(params);
            }
            
        } catch (Exception e) {
            log.error("{} Unexpected error in ask: {}", context.getRequestId(), e.getMessage(), e);
            CompletableFuture<String> future = new CompletableFuture<>();
            future.completeExceptionally(e);
            return future;
        }
    }
    
    @Override
    public CompletableFuture<String> askWithTools(
            AgentContext context,
            List<Message> messages,
            List<Message> systemMsgs,
            List<BaseTool> tools,
            ToolChoice toolChoice,
            boolean stream,
            Double temperature) {
        
        try {
            // 转换工具格式
            List<Map<String, Object>> formattedTools = formatTools(tools);
            
            // 构建带工具的请求参数
            Map<String, Object> params = buildRequestParamsWithTools(
                messages, systemMsgs, formattedTools, toolChoice, temperature);
                
            log.info("{} call llm request with tools {}", context.getRequestId(), 
                     JSONObject.toJSONString(params));
                     
            if (!stream) {
                params.put("stream", false);
                return callOpenAI(params).thenApply(response -> parseToolResponse(context, response));
            } else {
                params.put("stream", true);
                return callOpenAIStream(params);
            }
            
        } catch (Exception e) {
            log.error("{} Error in askWithTools: {}", context.getRequestId(), e.getMessage(), e);
            CompletableFuture<String> future = new CompletableFuture<>();
            future.completeExceptionally(e);
            return future;
        }
    }
}
```

## 多厂商适配器设计

### 消息格式适配

不同厂商的消息格式存在差异，需要进行格式转换：

```java
/**
 * 格式化消息以适配不同厂商
 */
public static List<Map<String, Object>> formatMessages(List<Message> messages, boolean isClaudeModel) {
    List<Map<String, Object>> formattedMessages = new ArrayList<>();
    
    for (Message message : messages) {
        Map<String, Object> formattedMessage = new HashMap<>();
        
        // 角色映射
        String role = mapRole(message.getRole(), isClaudeModel);
        formattedMessage.put("role", role);
        
        // 内容格式化
        if (isClaudeModel) {
            // Claude格式
            formattedMessage.put("content", formatClaudeContent(message));
        } else {
            // OpenAI格式
            formattedMessage.put("content", formatOpenAIContent(message));
        }
        
        formattedMessages.add(formattedMessage);
    }
    
    return formattedMessages;
}

/**
 * 角色映射
 */
private static String mapRole(RoleType role, boolean isClaudeModel) {
    switch (role) {
        case USER:
            return "user";
        case ASSISTANT:
            return "assistant";
        case SYSTEM:
            return isClaudeModel ? "user" : "system";  // Claude不支持system角色
        case TOOL:
            return "tool";
        default:
            return "user";
    }
}
```

### 工具调用格式适配

不同模型的工具调用格式差异较大，需要统一转换：

```java
/**
 * 将OpenAI格式工具转换为Claude格式
 */
public List<Map<String, Object>> convertGptToolsToClaude(List<Map<String, Object>> gptTools) {
    List<Map<String, Object>> claudeTools = new ArrayList<>();
    
    for (Map<String, Object> gptTool : gptTools) {
        Map<String, Object> function = (Map<String, Object>) gptTool.get("function");
        
        Map<String, Object> claudeTool = new HashMap<>();
        claudeTool.put("name", function.get("name"));
        claudeTool.put("description", function.get("description"));
        claudeTool.put("input_schema", function.get("parameters"));
        
        claudeTools.add(claudeTool);
    }
    
    return claudeTools;
}

/**
 * 统一工具格式化
 */
private List<Map<String, Object>> formatTools(List<BaseTool> tools) {
    List<Map<String, Object>> formattedTools = new ArrayList<>();
    
    for (BaseTool tool : tools) {
        Map<String, Object> toolDef = new HashMap<>();
        toolDef.put("type", "function");
        
        Map<String, Object> function = new HashMap<>();
        function.put("name", tool.getName());
        function.put("description", tool.getDescription());
        function.put("parameters", tool.toParams());
        
        toolDef.put("function", function);
        formattedTools.add(toolDef);
    }
    
    // 根据模型类型进行格式转换
    if (isClaudeModel()) {
        return convertGptToolsToClaude(formattedTools);
    }
    
    return formattedTools;
}
```

## 配置驱动的模型管理

### LLM配置类设计

```java
/**
 * LLM配置设置类
 */
@Data
@Builder
@NoArgsConstructor  
@AllArgsConstructor
public class LLMSettings {
    private String model;              // 模型名称
    private int maxTokens;             // 最大token数
    private double temperature;        // 温度参数
    private String apiType;            // API类型
    private String apiKey;             // API密钥
    private String apiVersion;         // API版本
    private String baseUrl;            // 基础URL
    private String interfaceUrl;       // 接口URL
    private String functionCallType;   // 函数调用类型
    private int maxInputTokens;        // 最大输入token
    private Map<String, Object> extParams;  // 扩展参数
}
```

### 配置管理器

```java
/**
 * LLM配置管理器
 */
public class Config {
    private static final Map<String, LLMSettings> modelConfigs = new ConcurrentHashMap<>();
    
    static {
        initializeDefaultConfigs();
    }
    
    /**
     * 初始化默认配置
     */
    private static void initializeDefaultConfigs() {
        // OpenAI GPT-4配置
        modelConfigs.put("gpt-4", LLMSettings.builder()
            .model("gpt-4-0125-preview")
            .maxTokens(4096)
            .temperature(0.7)
            .apiType("openai")
            .baseUrl("https://api.openai.com")
            .interfaceUrl("/v1/chat/completions")
            .functionCallType("tool_call")
            .maxInputTokens(32000)
            .build());
            
        // Claude配置
        modelConfigs.put("claude", LLMSettings.builder()
            .model("claude-3-sonnet-20240229")
            .maxTokens(4096)
            .temperature(0.7) 
            .apiType("anthropic")
            .baseUrl("https://api.anthropic.com")
            .interfaceUrl("/v1/messages")
            .functionCallType("tool_use")
            .maxInputTokens(200000)
            .build());
            
        // 本地模型配置
        modelConfigs.put("local-llm", LLMSettings.builder()
            .model("qwen2.5:7b")
            .maxTokens(2048)
            .temperature(0.8)
            .apiType("ollama")
            .baseUrl("http://localhost:11434")
            .interfaceUrl("/api/chat")
            .functionCallType("json_mode")
            .maxInputTokens(8000)
            .build());
    }
    
    /**
     * 获取LLM配置
     */
    public static LLMSettings getLLMConfig(String modelName) {
        LLMSettings config = modelConfigs.get(modelName);
        if (config == null) {
            throw new IllegalArgumentException("Unknown model: " + modelName);
        }
        return config;
    }
    
    /**
     * 注册新的模型配置
     */
    public static void registerModel(String modelName, LLMSettings settings) {
        modelConfigs.put(modelName, settings);
    }
}
```

## HTTP调用层抽象

### 统一HTTP调用接口

```java
/**
 * 统一的HTTP调用方法
 */
private CompletableFuture<String> callOpenAI(Map<String, Object> params) {
    CompletableFuture<String> future = new CompletableFuture<>();
    
    try {
        OkHttpClient client = new OkHttpClient.Builder()
            .connectTimeout(60, TimeUnit.SECONDS)
            .readTimeout(300, TimeUnit.SECONDS)
            .writeTimeout(300, TimeUnit.SECONDS)
            .callTimeout(300, TimeUnit.SECONDS)
            .build();
        
        String url = baseUrl + interfaceUrl;
        
        RequestBody body = RequestBody.create(
            MediaType.parse("application/json"),
            JSONObject.toJSONString(params)
        );
        
        Request.Builder requestBuilder = new Request.Builder()
            .url(url)
            .post(body);
            
        // 根据不同API类型设置请求头
        setAuthHeaders(requestBuilder);
        
        Request request = requestBuilder.build();
        
        client.newCall(request).enqueue(new Callback() {
            @Override
            public void onFailure(Call call, IOException e) {
                future.completeExceptionally(e);
            }
            
            @Override
            public void onResponse(Call call, Response response) throws IOException {
                if (response.isSuccessful()) {
                    future.complete(response.body().string());
                } else {
                    future.completeExceptionally(
                        new IOException("HTTP " + response.code() + ": " + response.message())
                    );
                }
                response.close();
            }
        });
        
    } catch (Exception e) {
        future.completeExceptionally(e);
    }
    
    return future;
}

/**
 * 设置不同厂商的认证头
 */
private void setAuthHeaders(Request.Builder requestBuilder) {
    if ("openai".equals(getApiType())) {
        requestBuilder.addHeader("Authorization", "Bearer " + apiKey);
        requestBuilder.addHeader("Content-Type", "application/json");
    } else if ("anthropic".equals(getApiType())) {
        requestBuilder.addHeader("x-api-key", apiKey);
        requestBuilder.addHeader("anthropic-version", "2023-06-01");
        requestBuilder.addHeader("Content-Type", "application/json");
    } else if ("ollama".equals(getApiType())) {
        requestBuilder.addHeader("Content-Type", "application/json");
    }
}
```

### 流式调用处理

```java
/**
 * 流式调用实现
 */
private CompletableFuture<String> callOpenAIStream(Map<String, Object> params) {
    CompletableFuture<String> future = new CompletableFuture<>();
    StringBuilder responseBuilder = new StringBuilder();
    
    try {
        OkHttpClient client = createHttpClient();
        String url = baseUrl + interfaceUrl;
        
        RequestBody body = RequestBody.create(
            MediaType.parse("application/json"),
            JSONObject.toJSONString(params)
        );
        
        Request.Builder requestBuilder = new Request.Builder()
            .url(url)
            .post(body);
            
        setAuthHeaders(requestBuilder);
        Request request = requestBuilder.build();
        
        client.newCall(request).enqueue(new Callback() {
            @Override
            public void onResponse(Call call, Response response) throws IOException {
                if (!response.isSuccessful()) {
                    future.completeExceptionally(
                        new IOException("HTTP " + response.code() + ": " + response.message())
                    );
                    return;
                }
                
                try (BufferedReader reader = new BufferedReader(
                        new InputStreamReader(response.body().byteStream(), StandardCharsets.UTF_8))) {
                    
                    String line;
                    while ((line = reader.readLine()) != null) {
                        if (line.startsWith("data: ")) {
                            String data = line.substring(6);
                            if (!"[DONE]".equals(data)) {
                                processStreamData(data, responseBuilder);
                            }
                        }
                    }
                    
                    future.complete(responseBuilder.toString());
                } catch (Exception e) {
                    future.completeExceptionally(e);
                }
            }
            
            @Override
            public void onFailure(Call call, IOException e) {
                future.completeExceptionally(e);
            }
        });
        
    } catch (Exception e) {
        future.completeExceptionally(e);
    }
    
    return future;
}
```

## 模型切换与负载均衡

### 动态模型路由

```java
/**
 * 模型路由器 - 根据策略选择合适的模型
 */
@Component
public class ModelRouter {
    
    private final List<String> availableModels;
    private final AtomicInteger roundRobinCounter = new AtomicInteger(0);
    
    public ModelRouter() {
        this.availableModels = Arrays.asList("gpt-4", "claude", "local-llm");
    }
    
    /**
     * 根据负载均衡策略选择模型
     */
    public String selectModel(String strategy) {
        switch (strategy.toLowerCase()) {
            case "round_robin":
                return selectByRoundRobin();
            case "cost_optimized":
                return selectByCost();
            case "performance_optimized":
                return selectByPerformance();
            default:
                return availableModels.get(0);
        }
    }
    
    private String selectByRoundRobin() {
        int index = roundRobinCounter.getAndIncrement() % availableModels.size();
        return availableModels.get(index);
    }
    
    private String selectByCost() {
        // 选择成本最低的模型
        return "local-llm";
    }
    
    private String selectByPerformance() {
        // 选择性能最好的模型
        return "gpt-4";
    }
}
```

### 故障转移机制

```java
/**
 * LLM调用故障转移装饰器
 */
public class FailoverLLM implements LLMInterface {
    private final List<LLM> llmInstances;
    private final int maxRetries;
    
    public FailoverLLM(List<LLM> llmInstances, int maxRetries) {
        this.llmInstances = llmInstances;
        this.maxRetries = maxRetries;
    }
    
    @Override
    public CompletableFuture<String> ask(AgentContext context, List<Message> messages, 
                                        List<Message> systemMsgs, boolean stream, Double temperature) {
        
        return attemptCall(context, messages, systemMsgs, stream, temperature, 0);
    }
    
    private CompletableFuture<String> attemptCall(AgentContext context, List<Message> messages,
                                                 List<Message> systemMsgs, boolean stream, 
                                                 Double temperature, int attemptIndex) {
        
        if (attemptIndex >= llmInstances.size()) {
            CompletableFuture<String> future = new CompletableFuture<>();
            future.completeExceptionally(new RuntimeException("All LLM instances failed"));
            return future;
        }
        
        LLM currentLLM = llmInstances.get(attemptIndex);
        
        return currentLLM.ask(context, messages, systemMsgs, stream, temperature)
            .handle((result, throwable) -> {
                if (throwable != null) {
                    log.warn("LLM instance {} failed, trying next: {}", 
                             currentLLM.getModel(), throwable.getMessage());
                    return attemptCall(context, messages, systemMsgs, stream, temperature, attemptIndex + 1)
                        .join();
                }
                return result;
            });
    }
}
```

## 实践建议

### 1. 接口设计原则
- 保持接口简洁统一
- 支持异步调用
- 提供完整的参数控制
- 建立清晰的异常处理机制

### 2. 性能优化
- 实现连接池管理
- 支持请求缓存
- 实现超时控制
- 监控调用性能

### 3. 安全性考虑
- API密钥安全存储
- 请求内容过滤
- 访问权限控制
- 调用审计日志

### 4. 可观测性
- 调用链路追踪
- 性能指标监控
- 错误率统计
- 成本分析

## 小结

LLM的黑盒化调用通过统一接口抽象、适配器模式、配置驱动等技术手段，实现了多模型厂商的统一接入和无缝切换。这种设计模式不仅屏蔽了底层模型差异，还提供了负载均衡、故障转移等高级功能，为AGI应用的模型管理提供了灵活可靠的基础架构。

在实际应用中，LLM黑盒化需要考虑性能、安全性、成本控制等多个方面。通过合理的架构设计和完善的监控机制，可以构建出高效、稳定、经济的LLM调用系统，为AGI应用提供强有力的模型服务支撑。
