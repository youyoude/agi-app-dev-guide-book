# 3.1.4 大语言模型的黑盒化调用

## 学习目标
了解如何封装LLM调用接口，实现多模型厂商的统一接入和无缝切换，屏蔽底层模型差异。

## LLM黑盒化调用的必要性

在AI应用开发中，大语言模型（LLM）是核心能力提供者，但面临着多个挑战：

1. **多厂商差异**：OpenAI、Claude、ChatGLM等不同厂商的API接口格式各异
2. **参数格式不统一**：不同模型的输入输出参数结构和命名规则不同  
3. **功能特性差异**：某些模型支持流式输出，某些支持工具调用，功能特性不完全一致
4. **版本兼容性**：模型API经常更新，需要处理版本兼容问题
5. **成本和性能权衡**：不同场景下需要灵活切换不同成本和性能特征的模型

LLM的黑盒化调用通过统一接口抽象，将这些差异性封装在接口实现层，使上层AI应用代码与具体的模型厂商解耦，实现模型的可替换性和系统的灵活性。

## 统一LLM接口设计

### 核心接口抽象

设计统一的LLM调用接口，屏蔽底层实现差异：

```java
/**
 * LLM统一调用接口
 */
public interface LLMInterface {
    
    /**
     * 基础文本生成
     * @param context 执行上下文
     * @param messages 消息列表  
     * @param systemMsgs 系统消息
     * @param stream 是否流式输出
     * @param temperature 温度参数
     * @return 异步结果
     */
    CompletableFuture<String> ask(
        AgentContext context,
        List<Message> messages, 
        List<Message> systemMsgs,
        boolean stream, 
        Double temperature
    );
    
    /**
     * 带工具调用的文本生成
     * @param context 执行上下文
     * @param messages 消息列表
     * @param systemMsgs 系统消息  
     * @param tools 可用工具列表
     * @param toolChoice 工具选择策略
     * @param stream 是否流式输出
     * @param temperature 温度参数
     * @return 异步结果
     */
    CompletableFuture<String> askWithTools(
        AgentContext context,
        List<Message> messages,
        List<Message> systemMsgs, 
        List<BaseTool> tools,
        ToolChoice toolChoice,
        boolean stream,
        Double temperature
    );
    
    /**
     * 获取模型配置信息
     */
    LLMSettings getSettings();
    
    /**
     * 获取token计数器
     */
    TokenCounter getTokenCounter();
}
```

### LLM实现类设计

基于统一接口实现具体的LLM调用类，采用单例模式和配置驱动的设计：

```java
/**
 * LLM统一调用实现类
 */
@Slf4j
@Data
public class LLM implements LLMInterface {
    
    private static final Map<String, LLM> instances = new ConcurrentHashMap<>();
    
    // 核心配置参数
    private final String model;
    private final String provider;
    private final LLMSettings settings;
    private final TokenCounter tokenCounter;
    
    /**
     * 构造方法 - 基于配置初始化LLM实例
     */
    public LLM(String modelName, String provider) {
        this.provider = provider;
        this.settings = Config.getLLMConfig(modelName);
        this.model = settings.getModel();
        this.tokenCounter = new TokenCounter();
    }
    
    /**
     * 单例模式获取LLM实例
     */
    public static LLM getInstance(String modelName, String provider) {
        String key = modelName + "_" + (provider != null ? provider : "default");
        return instances.computeIfAbsent(key, k -> new LLM(modelName, provider));
    }
    
    @Override
    public CompletableFuture<String> ask(AgentContext context, List<Message> messages, 
                                        List<Message> systemMsgs, boolean stream, Double temperature) {
        try {
            // 消息格式化和参数构建
            List<Map<String, Object>> formattedMessages = formatMessages(messages, systemMsgs);
            Map<String, Object> params = buildRequestParams(formattedMessages, temperature, stream);
            
            log.info("{} LLM request: model={}, stream={}", context.getRequestId(), model, stream);
            
            // 根据流式标志选择调用方式
            return stream ? callLLMStream(params) : callLLM(params);
            
        } catch (Exception e) {
            log.error("{} LLM call failed: {}", context.getRequestId(), e.getMessage());
            return CompletableFuture.failedFuture(e);
        }
    }
    
    @Override
    public CompletableFuture<String> askWithTools(AgentContext context, List<Message> messages,
                                                 List<Message> systemMsgs, List<BaseTool> tools,
                                                 ToolChoice toolChoice, boolean stream, Double temperature) {
        try {
            // 工具格式转换和参数构建
            List<Map<String, Object>> formattedTools = formatTools(tools);
            Map<String, Object> params = buildRequestParamsWithTools(
                messages, systemMsgs, formattedTools, toolChoice, temperature, stream);
                
            log.info("{} LLM request with {} tools", context.getRequestId(), tools.size());
            
            return stream ? callLLMStream(params) : callLLM(params);
            
        } catch (Exception e) {
            log.error("{} LLM tool call failed: {}", context.getRequestId(), e.getMessage());
            return CompletableFuture.failedFuture(e);
        }
    }
    
    // 私有方法：格式化消息、构建参数、执行调用等
    // （具体实现细节移至实现文档或代码库）
}
```

### 关键设计要点

1. **配置驱动**：通过配置文件管理不同模型的参数
2. **单例管理**：避免重复创建实例，提升性能
3. **统一异常处理**：标准化错误处理和日志记录
4. **流式支持**：同时支持同步和流式调用模式

## 多厂商适配器设计

### 消息格式适配

不同厂商的消息格式存在差异，需要进行格式转换：

```java
/**
 * 格式化消息以适配不同厂商
 */
public static List<Map<String, Object>> formatMessages(List<Message> messages, boolean isClaudeModel) {
    List<Map<String, Object>> formattedMessages = new ArrayList<>();
    
    for (Message message : messages) {
        Map<String, Object> formattedMessage = new HashMap<>();
        
        // 角色映射
        String role = mapRole(message.getRole(), isClaudeModel);
        formattedMessage.put("role", role);
        
        // 内容格式化
        if (isClaudeModel) {
            // Claude格式
            formattedMessage.put("content", formatClaudeContent(message));
        } else {
            // OpenAI格式
            formattedMessage.put("content", formatOpenAIContent(message));
        }
        
        formattedMessages.add(formattedMessage);
    }
    
    return formattedMessages;
}

/**
 * 角色映射
 */
private static String mapRole(RoleType role, boolean isClaudeModel) {
    switch (role) {
        case USER:
            return "user";
        case ASSISTANT:
            return "assistant";
        case SYSTEM:
            return isClaudeModel ? "user" : "system";  // Claude不支持system角色
        case TOOL:
            return "tool";
        default:
            return "user";
    }
}
```

### 工具调用格式适配

不同模型的工具调用格式差异较大，需要统一转换：

```java
/**
 * 将OpenAI格式工具转换为Claude格式
 */
public List<Map<String, Object>> convertGptToolsToClaude(List<Map<String, Object>> gptTools) {
    List<Map<String, Object>> claudeTools = new ArrayList<>();
    
    for (Map<String, Object> gptTool : gptTools) {
        Map<String, Object> function = (Map<String, Object>) gptTool.get("function");
        
        Map<String, Object> claudeTool = new HashMap<>();
        claudeTool.put("name", function.get("name"));
        claudeTool.put("description", function.get("description"));
        claudeTool.put("input_schema", function.get("parameters"));
        
        claudeTools.add(claudeTool);
    }
    
    return claudeTools;
}

/**
 * 统一工具格式化
 */
private List<Map<String, Object>> formatTools(List<BaseTool> tools) {
    List<Map<String, Object>> formattedTools = new ArrayList<>();
    
    for (BaseTool tool : tools) {
        Map<String, Object> toolDef = new HashMap<>();
        toolDef.put("type", "function");
        
        Map<String, Object> function = new HashMap<>();
        function.put("name", tool.getName());
        function.put("description", tool.getDescription());
        function.put("parameters", tool.toParams());
        
        toolDef.put("function", function);
        formattedTools.add(toolDef);
    }
    
    // 根据模型类型进行格式转换
    if (isClaudeModel()) {
        return convertGptToolsToClaude(formattedTools);
    }
    
    return formattedTools;
}
```

## 配置驱动的模型管理

### LLM配置类设计

```java
/**
 * LLM配置设置类
 */
@Data
@Builder
@NoArgsConstructor  
@AllArgsConstructor
public class LLMSettings {
    private String model;              // 模型名称
    private int maxTokens;             // 最大token数
    private double temperature;        // 温度参数
    private String apiType;            // API类型
    private String apiKey;             // API密钥
    private String apiVersion;         // API版本
    private String baseUrl;            // 基础URL
    private String interfaceUrl;       // 接口URL
    private String functionCallType;   // 函数调用类型
    private int maxInputTokens;        // 最大输入token
    private Map<String, Object> extParams;  // 扩展参数
}
```

### 配置管理器

```java
/**
 * LLM配置管理器
 */
public class Config {
    private static final Map<String, LLMSettings> modelConfigs = new ConcurrentHashMap<>();
    
    static {
        initializeDefaultConfigs();
    }
    
    /**
     * 初始化默认配置
     */
    private static void initializeDefaultConfigs() {
        // OpenAI GPT-4配置
        modelConfigs.put("gpt-4", LLMSettings.builder()
            .model("gpt-4-0125-preview")
            .maxTokens(4096)
            .temperature(0.7)
            .apiType("openai")
            .baseUrl("https://api.openai.com")
            .interfaceUrl("/v1/chat/completions")
            .functionCallType("tool_call")
            .maxInputTokens(32000)
            .build());
            
        // Claude配置
        modelConfigs.put("claude", LLMSettings.builder()
            .model("claude-3-sonnet-20240229")
            .maxTokens(4096)
            .temperature(0.7) 
            .apiType("anthropic")
            .baseUrl("https://api.anthropic.com")
            .interfaceUrl("/v1/messages")
            .functionCallType("tool_use")
            .maxInputTokens(200000)
            .build());
            
        // 本地模型配置
        modelConfigs.put("local-llm", LLMSettings.builder()
            .model("qwen2.5:7b")
            .maxTokens(2048)
            .temperature(0.8)
            .apiType("ollama")
            .baseUrl("http://localhost:11434")
            .interfaceUrl("/api/chat")
            .functionCallType("json_mode")
            .maxInputTokens(8000)
            .build());
    }
    
    /**
     * 获取LLM配置
     */
    public static LLMSettings getLLMConfig(String modelName) {
        LLMSettings config = modelConfigs.get(modelName);
        if (config == null) {
            throw new IllegalArgumentException("Unknown model: " + modelName);
        }
        return config;
    }
    
    /**
     * 注册新的模型配置
     */
    public static void registerModel(String modelName, LLMSettings settings) {
        modelConfigs.put(modelName, settings);
    }
}
```

## HTTP调用层抽象

### 统一HTTP调用架构

HTTP调用层负责将标准化的请求参数转换为具体厂商的API调用，主要包含以下组件：

```java
/**
 * HTTP调用抽象层
 */
public class LLMHttpClient {
    
    private final OkHttpClient httpClient;
    private final LLMSettings settings;
    
    /**
     * 同步调用
     */
    public CompletableFuture<String> call(Map<String, Object> params) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                Request request = buildRequest(params);
                try (Response response = httpClient.newCall(request).execute()) {
                    return handleResponse(response);
                }
            } catch (Exception e) {
                throw new RuntimeException("LLM call failed", e);
            }
        });
    }
    
    /**
     * 流式调用
     */
    public CompletableFuture<String> callStream(Map<String, Object> params, 
                                               StreamHandler streamHandler) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                Request request = buildStreamRequest(params);
                return executeStreamRequest(request, streamHandler);
            } catch (Exception e) {
                throw new RuntimeException("Stream call failed", e);
            }
        });
    }
    
    private Request buildRequest(Map<String, Object> params) {
        String url = settings.getBaseUrl() + settings.getInterfaceUrl();
        RequestBody body = RequestBody.create(
            MediaType.parse("application/json"),
            JSON.toJSONString(params)
        );
        
        return new Request.Builder()
            .url(url)
            .post(body)
            .headers(buildHeaders())
            .build();
    }
    
    private Headers buildHeaders() {
        Headers.Builder builder = new Headers.Builder();
        
        // 根据不同厂商设置认证头
        switch (settings.getApiType()) {
            case "openai":
                builder.add("Authorization", "Bearer " + settings.getApiKey());
                break;
            case "anthropic":
                builder.add("x-api-key", settings.getApiKey());
                builder.add("anthropic-version", "2023-06-01");
                break;
            case "ollama":
                // 本地模型无需认证
                break;
        }
        
        builder.add("Content-Type", "application/json");
        return builder.build();
    }
}
```

### 关键特性

1. **连接池管理**：复用HTTP连接，提升性能
2. **超时控制**：设置合理的连接和读取超时
3. **错误处理**：统一的异常处理和重试机制
4. **流式支持**：支持Server-Sent Events流式响应

## 模型切换与负载均衡

### 动态模型路由

```java
/**
 * 模型路由器 - 根据策略选择合适的模型
 */
@Component
public class ModelRouter {
    
    private final List<String> availableModels;
    private final AtomicInteger roundRobinCounter = new AtomicInteger(0);
    
    public ModelRouter() {
        this.availableModels = Arrays.asList("gpt-4", "claude", "local-llm");
    }
    
    /**
     * 根据负载均衡策略选择模型
     */
    public String selectModel(String strategy) {
        switch (strategy.toLowerCase()) {
            case "round_robin":
                return selectByRoundRobin();
            case "cost_optimized":
                return selectByCost();
            case "performance_optimized":
                return selectByPerformance();
            default:
                return availableModels.get(0);
        }
    }
    
    private String selectByRoundRobin() {
        int index = roundRobinCounter.getAndIncrement() % availableModels.size();
        return availableModels.get(index);
    }
    
    private String selectByCost() {
        // 选择成本最低的模型
        return "local-llm";
    }
    
    private String selectByPerformance() {
        // 选择性能最好的模型
        return "gpt-4";
    }
}
```

### 故障转移机制

```java
/**
 * LLM调用故障转移装饰器
 */
public class FailoverLLM implements LLMInterface {
    private final List<LLM> llmInstances;
    private final int maxRetries;
    
    public FailoverLLM(List<LLM> llmInstances, int maxRetries) {
        this.llmInstances = llmInstances;
        this.maxRetries = maxRetries;
    }
    
    @Override
    public CompletableFuture<String> ask(AgentContext context, List<Message> messages, 
                                        List<Message> systemMsgs, boolean stream, Double temperature) {
        
        return attemptCall(context, messages, systemMsgs, stream, temperature, 0);
    }
    
    private CompletableFuture<String> attemptCall(AgentContext context, List<Message> messages,
                                                 List<Message> systemMsgs, boolean stream, 
                                                 Double temperature, int attemptIndex) {
        
        if (attemptIndex >= llmInstances.size()) {
            CompletableFuture<String> future = new CompletableFuture<>();
            future.completeExceptionally(new RuntimeException("All LLM instances failed"));
            return future;
        }
        
        LLM currentLLM = llmInstances.get(attemptIndex);
        
        return currentLLM.ask(context, messages, systemMsgs, stream, temperature)
            .handle((result, throwable) -> {
                if (throwable != null) {
                    log.warn("LLM instance {} failed, trying next: {}", 
                             currentLLM.getModel(), throwable.getMessage());
                    return attemptCall(context, messages, systemMsgs, stream, temperature, attemptIndex + 1)
                        .join();
                }
                return result;
            });
    }
}
```

## 实践建议

### 1. 接口设计原则
- 保持接口简洁统一
- 支持异步调用
- 提供完整的参数控制
- 建立清晰的异常处理机制

### 2. 性能优化
- 实现连接池管理
- 支持请求缓存
- 实现超时控制
- 监控调用性能

### 3. 安全性考虑
- API密钥安全存储
- 请求内容过滤
- 访问权限控制
- 调用审计日志

### 4. 可观测性
- 调用链路追踪
- 性能指标监控
- 错误率统计
- 成本分析

## 小结

LLM的黑盒化调用通过统一接口抽象、适配器模式、配置驱动等技术手段，实现了多模型厂商的统一接入和无缝切换。这种设计模式不仅屏蔽了底层模型差异，还提供了负载均衡、故障转移等高级功能，为AI应用的模型管理提供了灵活可靠的基础架构。

在实际应用中，LLM黑盒化需要考虑性能、安全性、成本控制等多个方面。通过合理的架构设计和完善的监控机制，可以构建出高效、稳定、经济的LLM调用系统，为AI应用提供强有力的模型服务支撑。
