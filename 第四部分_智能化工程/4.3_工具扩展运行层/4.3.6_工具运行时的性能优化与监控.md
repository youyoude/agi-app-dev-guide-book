# 4.3.6 å·¥å…·è¿è¡Œæ—¶çš„æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

> "æ€§èƒ½ä¼˜åŒ–æ˜¯å·¥ç¨‹è‰ºæœ¯çš„ä½“ç°ï¼Œè€Œç›‘æ§æ˜¯æ€§èƒ½ä¼˜åŒ–çš„çœ¼ç›ã€‚åªæœ‰å»ºç«‹äº†å®Œå–„çš„è§‚æµ‹ä½“ç³»ï¼Œæ‰èƒ½å®ç°ç²¾å‡†çš„æ€§èƒ½è°ƒä¼˜ã€‚"

## ğŸ¯ æœ¬èŠ‚å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬èŠ‚å­¦ä¹ åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š
- âœ… å»ºç«‹å·¥å…·æ‰§è¡Œçš„å…¨é¢æ€§èƒ½ç›‘æ§ä½“ç³»
- âœ… å®ç°æ™ºèƒ½åŒ–çš„æ€§èƒ½åˆ†æå’Œç“¶é¢ˆè¯†åˆ«
- âœ… æŒæ¡å·¥å…·æ‰§è¡Œçš„å¤šå±‚æ¬¡ä¼˜åŒ–ç­–ç•¥
- âœ… æ„å»ºè‡ªé€‚åº”çš„æ€§èƒ½è°ƒä¼˜æœºåˆ¶

## æ€§èƒ½ç›‘æ§ä½“ç³»è®¾è®¡

### å¤šç»´åº¦æ€§èƒ½æŒ‡æ ‡ä½“ç³»

å·¥å…·è¿è¡Œæ—¶çš„æ€§èƒ½ç›‘æ§éœ€è¦è¦†ç›–å¤šä¸ªç»´åº¦ï¼Œå½¢æˆç«‹ä½“åŒ–çš„è§‚æµ‹ä½“ç³»ï¼š

```mermaid
graph TB
    subgraph "æ€§èƒ½ç›‘æ§ä½“ç³»"
        subgraph "ç³»ç»Ÿå±‚ç›‘æ§"
            SYS_CPU[CPUä½¿ç”¨ç‡]
            SYS_MEM[å†…å­˜ä½¿ç”¨é‡]
            SYS_IO[ç£ç›˜I/O]
            SYS_NET[ç½‘ç»œå¸¦å®½]
        end
        
        subgraph "åº”ç”¨å±‚ç›‘æ§"
            APP_THROUGHPUT[å·¥å…·ååé‡]
            APP_LATENCY[å“åº”å»¶è¿Ÿ]
            APP_ERROR[é”™è¯¯ç‡]
            APP_CONCURRENCY[å¹¶å‘åº¦]
        end
        
        subgraph "ä¸šåŠ¡å±‚ç›‘æ§"
            BIZ_SUCCESS[ä»»åŠ¡æˆåŠŸç‡]
            BIZ_QUALITY[ç»“æœè´¨é‡]
            BIZ_EFFICIENCY[æ‰§è¡Œæ•ˆç‡]
            BIZ_COST[èµ„æºæˆæœ¬]
        end
        
        subgraph "ç”¨æˆ·å±‚ç›‘æ§"
            USER_SATISFACTION[ç”¨æˆ·æ»¡æ„åº¦]
            USER_EXPERIENCE[ç”¨æˆ·ä½“éªŒ]
            USER_RETENTION[ç”¨æˆ·ç•™å­˜]
        end
    end
    
    SYS_CPU --> ANALYZER[æ€§èƒ½åˆ†æå¼•æ“]
    SYS_MEM --> ANALYZER
    APP_THROUGHPUT --> ANALYZER
    APP_LATENCY --> ANALYZER
    BIZ_SUCCESS --> ANALYZER
    USER_SATISFACTION --> ANALYZER
```

### å®æ—¶æ€§èƒ½ç›‘æ§å¼•æ“

```python
import asyncio
import time
import psutil
import threading
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from collections import deque, defaultdict
import numpy as np
from datetime import datetime, timedelta

@dataclass
class MetricPoint:
    """æŒ‡æ ‡æ•°æ®ç‚¹"""
    timestamp: float
    value: float
    tags: Dict[str, str] = field(default_factory=dict)

@dataclass
class PerformanceProfile:
    """æ€§èƒ½å‰–æ"""
    tool_name: str
    execution_time: float
    cpu_time: float
    memory_peak: float
    io_operations: int
    network_calls: int
    cache_hits: int
    cache_misses: int
    error_count: int
    
class RealTimeMetricsCollector:
    """å®æ—¶æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, collection_interval: float = 1.0):
        self.collection_interval = collection_interval
        self.metrics_buffer = defaultdict(lambda: deque(maxlen=3600))  # 1å°æ—¶æ•°æ®
        self.collectors = {}
        self.running = False
        self.callbacks = []
        
    def register_collector(self, name: str, collector: Callable[[], float]):
        """æ³¨å†ŒæŒ‡æ ‡æ”¶é›†å™¨"""
        self.collectors[name] = collector
        
    def add_callback(self, callback: Callable[[str, MetricPoint], None]):
        """æ·»åŠ æŒ‡æ ‡å›è°ƒ"""
        self.callbacks.append(callback)
        
    async def start_collection(self):
        """å¯åŠ¨æŒ‡æ ‡æ”¶é›†"""
        self.running = True
        await asyncio.gather(
            self._collect_system_metrics(),
            self._collect_application_metrics(),
            self._process_metric_alerts()
        )
        
    async def stop_collection(self):
        """åœæ­¢æŒ‡æ ‡æ”¶é›†"""
        self.running = False
        
    async def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»Ÿçº§æŒ‡æ ‡"""
        
        while self.running:
            try:
                timestamp = time.time()
                
                # CPUæŒ‡æ ‡
                cpu_percent = psutil.cpu_percent(interval=None)
                self._add_metric("system.cpu.percent", timestamp, cpu_percent)
                
                # å†…å­˜æŒ‡æ ‡
                memory = psutil.virtual_memory()
                self._add_metric("system.memory.percent", timestamp, memory.percent)
                self._add_metric("system.memory.available", timestamp, memory.available / 1024 / 1024)  # MB
                
                # ç£ç›˜I/OæŒ‡æ ‡
                disk_io = psutil.disk_io_counters()
                if disk_io:
                    self._add_metric("system.disk.read_bytes", timestamp, disk_io.read_bytes)
                    self._add_metric("system.disk.write_bytes", timestamp, disk_io.write_bytes)
                
                # ç½‘ç»œI/OæŒ‡æ ‡
                network_io = psutil.net_io_counters()
                if network_io:
                    self._add_metric("system.network.bytes_sent", timestamp, network_io.bytes_sent)
                    self._add_metric("system.network.bytes_recv", timestamp, network_io.bytes_recv)
                
                await asyncio.sleep(self.collection_interval)
                
            except Exception as e:
                self.logger.error(f"System metrics collection error: {e}")
                await asyncio.sleep(self.collection_interval)
                
    async def _collect_application_metrics(self):
        """æ”¶é›†åº”ç”¨çº§æŒ‡æ ‡"""
        
        while self.running:
            try:
                timestamp = time.time()
                
                # æ‰§è¡Œè‡ªå®šä¹‰æ”¶é›†å™¨
                for name, collector in self.collectors.items():
                    try:
                        value = collector()
                        self._add_metric(f"application.{name}", timestamp, value)
                    except Exception as e:
                        self.logger.error(f"Collector {name} error: {e}")
                
                await asyncio.sleep(self.collection_interval)
                
            except Exception as e:
                self.logger.error(f"Application metrics collection error: {e}")
                await asyncio.sleep(self.collection_interval)
                
    def _add_metric(self, metric_name: str, timestamp: float, value: float, tags: Dict[str, str] = None):
        """æ·»åŠ æŒ‡æ ‡æ•°æ®"""
        
        metric_point = MetricPoint(
            timestamp=timestamp,
            value=value,
            tags=tags or {}
        )
        
        self.metrics_buffer[metric_name].append(metric_point)
        
        # è§¦å‘å›è°ƒ
        for callback in self.callbacks:
            try:
                callback(metric_name, metric_point)
            except Exception as e:
                self.logger.error(f"Metric callback error: {e}")
                
    def get_metric_values(self, metric_name: str, duration: int = 300) -> List[MetricPoint]:
        """è·å–æŒ‡å®šæ—¶é—´æ®µçš„æŒ‡æ ‡å€¼"""
        
        if metric_name not in self.metrics_buffer:
            return []
        
        current_time = time.time()
        start_time = current_time - duration
        
        return [
            point for point in self.metrics_buffer[metric_name]
            if point.timestamp >= start_time
        ]
        
    def calculate_metric_statistics(self, metric_name: str, duration: int = 300) -> Dict[str, float]:
        """è®¡ç®—æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯"""
        
        values = [point.value for point in self.get_metric_values(metric_name, duration)]
        
        if not values:
            return {}
        
        return {
            "min": min(values),
            "max": max(values),
            "mean": np.mean(values),
            "median": np.median(values),
            "std": np.std(values),
            "p95": np.percentile(values, 95),
            "p99": np.percentile(values, 99)
        }
```

### æ€§èƒ½å‰–æå™¨

```python
import cProfile
import pstats
import io
from contextlib import contextmanager
import resource
import tracemalloc
from typing import Generator, Dict, Any

class PerformanceProfiler:
    """æ€§èƒ½å‰–æå™¨"""
    
    def __init__(self):
        self.active_profiles = {}
        self.profile_history = []
        
    @contextmanager
    def profile_execution(self, tool_name: str, session_id: str = None) -> Generator[PerformanceProfile, None, None]:
        """æ€§èƒ½å‰–æä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        
        session_id = session_id or f"{tool_name}_{int(time.time())}"
        
        # å¼€å§‹å†…å­˜è·Ÿè¸ª
        tracemalloc.start()
        
        # åˆ›å»ºCPUå‰–æå™¨
        profiler = cProfile.Profile()
        profiler.enable()
        
        # è®°å½•å¼€å§‹æ—¶é—´å’Œèµ„æºä½¿ç”¨
        start_time = time.time()
        start_memory = self._get_memory_usage()
        start_rusage = resource.getrusage(resource.RUSAGE_SELF)
        
        profile = PerformanceProfile(
            tool_name=tool_name,
            execution_time=0.0,
            cpu_time=0.0,
            memory_peak=0.0,
            io_operations=0,
            network_calls=0,
            cache_hits=0,
            cache_misses=0,
            error_count=0
        )
        
        self.active_profiles[session_id] = profile
        
        try:
            yield profile
            
        except Exception as e:
            profile.error_count += 1
            raise
            
        finally:
            # åœæ­¢å‰–æ
            profiler.disable()
            
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            end_time = time.time()
            profile.execution_time = end_time - start_time
            
            # è®¡ç®—CPUæ—¶é—´
            end_rusage = resource.getrusage(resource.RUSAGE_SELF)
            profile.cpu_time = (
                (end_rusage.ru_utime - start_rusage.ru_utime) +
                (end_rusage.ru_stime - start_rusage.ru_stime)
            )
            
            # è·å–å†…å­˜ä½¿ç”¨å³°å€¼
            current_memory, peak_memory = tracemalloc.get_traced_memory()
            profile.memory_peak = peak_memory / 1024 / 1024  # MB
            tracemalloc.stop()
            
            # åˆ†æCPUå‰–æç»“æœ
            cpu_stats = self._analyze_cpu_profile(profiler)
            
            # è®°å½•å‰–æç»“æœ
            profile_result = {
                "session_id": session_id,
                "tool_name": tool_name,
                "profile": profile,
                "cpu_stats": cpu_stats,
                "timestamp": end_time
            }
            
            self.profile_history.append(profile_result)
            
            # æ¸…ç†æ´»è·ƒå‰–æ
            if session_id in self.active_profiles:
                del self.active_profiles[session_id]
                
    def _analyze_cpu_profile(self, profiler: cProfile.Profile) -> Dict[str, Any]:
        """åˆ†æCPUå‰–æç»“æœ"""
        
        # åˆ›å»ºç»Ÿè®¡å¯¹è±¡
        stats_buffer = io.StringIO()
        stats = pstats.Stats(profiler, stream=stats_buffer)
        stats.sort_stats('cumulative')
        
        # è·å–æœ€è€—æ—¶çš„å‡½æ•°
        top_functions = []
        for func_name, (call_count, total_time, cumulative_time, callers) in stats.stats.items():
            if cumulative_time > 0.001:  # åªå…³æ³¨è€—æ—¶è¶…è¿‡1msçš„å‡½æ•°
                top_functions.append({
                    "function": f"{func_name[0]}:{func_name[1]}({func_name[2]})",
                    "calls": call_count,
                    "total_time": total_time,
                    "cumulative_time": cumulative_time,
                    "per_call": cumulative_time / call_count if call_count > 0 else 0
                })
        
        # æŒ‰ç´¯è®¡æ—¶é—´æ’åº
        top_functions.sort(key=lambda x: x["cumulative_time"], reverse=True)
        
        return {
            "total_calls": sum(call_count for call_count, _, _, _ in stats.stats.values()),
            "top_functions": top_functions[:10],  # è¿”å›å‰10ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
            "profile_summary": stats_buffer.getvalue()
        }
        
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
        
    def get_tool_performance_summary(self, tool_name: str, hours: int = 24) -> Dict[str, Any]:
        """è·å–å·¥å…·æ€§èƒ½æ‘˜è¦"""
        
        current_time = time.time()
        start_time = current_time - (hours * 3600)
        
        # ç­›é€‰æŒ‡å®šæ—¶é—´æ®µçš„å‰–æç»“æœ
        relevant_profiles = [
            result for result in self.profile_history
            if (result["tool_name"] == tool_name and 
                result["timestamp"] >= start_time)
        ]
        
        if not relevant_profiles:
            return {"error": f"No performance data for tool {tool_name}"}
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        execution_times = [result["profile"].execution_time for result in relevant_profiles]
        memory_peaks = [result["profile"].memory_peak for result in relevant_profiles]
        cpu_times = [result["profile"].cpu_time for result in relevant_profiles]
        error_counts = [result["profile"].error_count for result in relevant_profiles]
        
        return {
            "tool_name": tool_name,
            "time_range_hours": hours,
            "total_executions": len(relevant_profiles),
            "execution_time": {
                "min": min(execution_times),
                "max": max(execution_times),
                "mean": np.mean(execution_times),
                "p95": np.percentile(execution_times, 95)
            },
            "memory_usage": {
                "min": min(memory_peaks),
                "max": max(memory_peaks),
                "mean": np.mean(memory_peaks),
                "p95": np.percentile(memory_peaks, 95)
            },
            "cpu_time": {
                "min": min(cpu_times),
                "max": max(cpu_times),
                "mean": np.mean(cpu_times),
                "total": sum(cpu_times)
            },
            "error_rate": sum(error_counts) / len(relevant_profiles),
            "recent_bottlenecks": self._identify_bottlenecks(relevant_profiles[-10:])
        }
        
    def _identify_bottlenecks(self, profiles: List[Dict]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        
        bottlenecks = []
        
        # åˆ†ææœ€è€—æ—¶çš„å‡½æ•°
        function_times = defaultdict(list)
        
        for profile_result in profiles:
            cpu_stats = profile_result.get("cpu_stats", {})
            top_functions = cpu_stats.get("top_functions", [])
            
            for func in top_functions:
                function_times[func["function"]].append(func["cumulative_time"])
        
        # æ‰¾å‡ºå¹³å‡è€—æ—¶æœ€é•¿çš„å‡½æ•°
        for function, times in function_times.items():
            avg_time = np.mean(times)
            if avg_time > 0.1:  # å¹³å‡è€—æ—¶è¶…è¿‡100ms
                bottlenecks.append({
                    "type": "cpu_bottleneck",
                    "function": function,
                    "average_time": avg_time,
                    "occurrences": len(times)
                })
        
        # æŒ‰å¹³å‡è€—æ—¶æ’åº
        bottlenecks.sort(key=lambda x: x["average_time"], reverse=True)
        
        return bottlenecks[:5]  # è¿”å›å‰5ä¸ªç“¶é¢ˆ
```

## æ™ºèƒ½æ€§èƒ½åˆ†æ

### å¼‚å¸¸æ£€æµ‹å’Œé¢„è­¦ç³»ç»Ÿ

```python
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from typing import List, Dict, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

class PerformanceAnomalyDetector:
    """æ€§èƒ½å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, contamination: float = 0.1):
        self.contamination = contamination
        self.models = {}
        self.scalers = {}
        self.baseline_metrics = {}
        self.alert_thresholds = {}
        
    def train_baseline(self, tool_name: str, historical_data: List[Dict[str, float]]):
        """è®­ç»ƒåŸºçº¿æ¨¡å‹"""
        
        if not historical_data:
            return
        
        # æå–ç‰¹å¾
        features = self._extract_features(historical_data)
        
        if len(features) < 10:  # æ•°æ®ç‚¹å¤ªå°‘
            return
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹
        model = IsolationForest(
            contamination=self.contamination,
            random_state=42,
            n_estimators=100
        )
        model.fit(features_scaled)
        
        # ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨
        self.models[tool_name] = model
        self.scalers[tool_name] = scaler
        
        # è®¡ç®—åŸºçº¿æŒ‡æ ‡
        self.baseline_metrics[tool_name] = self._calculate_baseline_metrics(historical_data)
        
        # è®¾ç½®å‘Šè­¦é˜ˆå€¼
        self.alert_thresholds[tool_name] = self._calculate_alert_thresholds(historical_data)
        
    def detect_anomaly(self, tool_name: str, current_metrics: Dict[str, float]) -> Dict[str, Any]:
        """æ£€æµ‹æ€§èƒ½å¼‚å¸¸"""
        
        if tool_name not in self.models:
            return {"anomaly": False, "reason": "No baseline model"}
        
        # æå–å½“å‰ç‰¹å¾
        features = self._extract_features([current_metrics])
        
        if not features:
            return {"anomaly": False, "reason": "Invalid metrics"}
        
        # æ ‡å‡†åŒ–
        scaler = self.scalers[tool_name]
        features_scaled = scaler.transform(features)
        
        # å¼‚å¸¸æ£€æµ‹
        model = self.models[tool_name]
        anomaly_score = model.decision_function(features_scaled)[0]
        is_anomaly = model.predict(features_scaled)[0] == -1
        
        # è¯¦ç»†åˆ†æ
        analysis = self._analyze_performance_deviation(tool_name, current_metrics)
        
        return {
            "anomaly": is_anomaly,
            "anomaly_score": float(anomaly_score),
            "analysis": analysis,
            "recommendations": self._generate_recommendations(tool_name, current_metrics, analysis)
        }
        
    def _extract_features(self, data: List[Dict[str, float]]) -> np.ndarray:
        """æå–ç‰¹å¾å‘é‡"""
        
        feature_keys = [
            "execution_time", "memory_peak", "cpu_time", 
            "error_count", "cache_hits", "cache_misses"
        ]
        
        features = []
        for metrics in data:
            feature_vector = []
            for key in feature_keys:
                value = metrics.get(key, 0.0)
                feature_vector.append(float(value))
            features.append(feature_vector)
        
        return np.array(features)
        
    def _calculate_baseline_metrics(self, historical_data: List[Dict[str, float]]) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—åŸºçº¿æŒ‡æ ‡"""
        
        baseline = {}
        
        for key in ["execution_time", "memory_peak", "cpu_time", "error_count"]:
            values = [data.get(key, 0.0) for data in historical_data]
            
            if values:
                baseline[key] = {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "p50": np.percentile(values, 50),
                    "p95": np.percentile(values, 95),
                    "p99": np.percentile(values, 99)
                }
        
        return baseline
        
    def _calculate_alert_thresholds(self, historical_data: List[Dict[str, float]]) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—å‘Šè­¦é˜ˆå€¼"""
        
        thresholds = {}
        
        for key in ["execution_time", "memory_peak", "cpu_time", "error_count"]:
            values = [data.get(key, 0.0) for data in historical_data]
            
            if values:
                mean = np.mean(values)
                std = np.std(values)
                
                thresholds[key] = {
                    "warning": mean + 2 * std,  # 2ä¸ªæ ‡å‡†å·®
                    "critical": mean + 3 * std  # 3ä¸ªæ ‡å‡†å·®
                }
        
        return thresholds
        
    def _analyze_performance_deviation(self, tool_name: str, current_metrics: Dict[str, float]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½åå·®"""
        
        baseline = self.baseline_metrics.get(tool_name, {})
        thresholds = self.alert_thresholds.get(tool_name, {})
        
        analysis = {
            "deviations": [],
            "severity": "normal"
        }
        
        for metric_name, current_value in current_metrics.items():
            if metric_name not in baseline:
                continue
            
            baseline_stats = baseline[metric_name]
            baseline_mean = baseline_stats["mean"]
            baseline_std = baseline_stats["std"]
            
            # è®¡ç®—Zåˆ†æ•°
            z_score = (current_value - baseline_mean) / baseline_std if baseline_std > 0 else 0
            
            # åˆ¤æ–­åå·®ç¨‹åº¦
            if abs(z_score) > 2:
                deviation_type = "increase" if z_score > 0 else "decrease"
                severity = "critical" if abs(z_score) > 3 else "warning"
                
                analysis["deviations"].append({
                    "metric": metric_name,
                    "current_value": current_value,
                    "baseline_mean": baseline_mean,
                    "z_score": z_score,
                    "deviation_type": deviation_type,
                    "severity": severity
                })
                
                # æ›´æ–°æ€»ä½“ä¸¥é‡ç¨‹åº¦
                if severity == "critical":
                    analysis["severity"] = "critical"
                elif analysis["severity"] != "critical" and severity == "warning":
                    analysis["severity"] = "warning"
        
        return analysis
        
    def _generate_recommendations(self, 
                                tool_name: str, 
                                current_metrics: Dict[str, float],
                                analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        
        recommendations = []
        
        for deviation in analysis.get("deviations", []):
            metric = deviation["metric"]
            deviation_type = deviation["deviation_type"]
            
            if metric == "execution_time" and deviation_type == "increase":
                recommendations.append(
                    "æ‰§è¡Œæ—¶é—´å¼‚å¸¸å¢é•¿ï¼Œå»ºè®®æ£€æŸ¥ç®—æ³•å¤æ‚åº¦æˆ–å¢åŠ è®¡ç®—èµ„æº"
                )
                
            elif metric == "memory_peak" and deviation_type == "increase":
                recommendations.append(
                    "å†…å­˜ä½¿ç”¨é‡å¼‚å¸¸å¢é«˜ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼æˆ–ä¼˜åŒ–æ•°æ®ç»“æ„"
                )
                
            elif metric == "cpu_time" and deviation_type == "increase":
                recommendations.append(
                    "CPUä½¿ç”¨æ—¶é—´å¼‚å¸¸å¢é•¿ï¼Œå»ºè®®ä¼˜åŒ–è®¡ç®—å¯†é›†å‹æ“ä½œæˆ–å¯ç”¨ç¼“å­˜"
                )
                
            elif metric == "error_count" and deviation_type == "increase":
                recommendations.append(
                    "é”™è¯¯ç‡å¼‚å¸¸å¢é«˜ï¼Œå»ºè®®æ£€æŸ¥è¾“å…¥æ•°æ®è´¨é‡æˆ–å¢å¼ºå¼‚å¸¸å¤„ç†"
                )
        
        if not recommendations:
            recommendations.append("æ€§èƒ½æŒ‡æ ‡æ­£å¸¸ï¼Œå»ºè®®ç»§ç»­ç›‘æ§")
        
        return recommendations
```

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### è‡ªé€‚åº”èµ„æºç®¡ç†å™¨

```python
class AdaptiveResourceManager:
    """è‡ªé€‚åº”èµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.resource_pools = {}
        self.allocation_history = []
        self.performance_feedback = {}
        self.optimization_policies = {}
        
    def register_resource_pool(self, pool_name: str, initial_size: int, max_size: int):
        """æ³¨å†Œèµ„æºæ± """
        
        self.resource_pools[pool_name] = {
            "current_size": initial_size,
            "max_size": max_size,
            "utilization_history": deque(maxlen=100),
            "performance_impact": deque(maxlen=50)
        }
        
    async def optimize_resource_allocation(self, tool_name: str, current_load: Dict[str, float]):
        """ä¼˜åŒ–èµ„æºåˆ†é…"""
        
        # åˆ†æå½“å‰è´Ÿè½½æ¨¡å¼
        load_pattern = self._analyze_load_pattern(current_load)
        
        # é¢„æµ‹èµ„æºéœ€æ±‚
        predicted_demand = await self._predict_resource_demand(tool_name, load_pattern)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_plan = self._generate_optimization_plan(
            tool_name, current_load, predicted_demand
        )
        
        # æ‰§è¡Œèµ„æºè°ƒæ•´
        adjustments = await self._execute_resource_adjustments(optimization_plan)
        
        return {
            "load_pattern": load_pattern,
            "predicted_demand": predicted_demand,
            "optimization_plan": optimization_plan,
            "adjustments": adjustments
        }
        
    def _analyze_load_pattern(self, current_load: Dict[str, float]) -> Dict[str, Any]:
        """åˆ†æè´Ÿè½½æ¨¡å¼"""
        
        # è®¡ç®—è´Ÿè½½ç‰¹å¾
        total_load = sum(current_load.values())
        load_distribution = {
            resource: load / total_load if total_load > 0 else 0
            for resource, load in current_load.items()
        }
        
        # è¯†åˆ«è´Ÿè½½ç±»å‹
        load_type = "balanced"
        dominant_resource = max(load_distribution, key=load_distribution.get)
        if load_distribution[dominant_resource] > 0.6:
            load_type = f"{dominant_resource}_intensive"
        
        return {
            "total_load": total_load,
            "distribution": load_distribution,
            "dominant_resource": dominant_resource,
            "load_type": load_type,
            "timestamp": time.time()
        }
        
    async def _predict_resource_demand(self, 
                                     tool_name: str, 
                                     load_pattern: Dict[str, Any]) -> Dict[str, float]:
        """é¢„æµ‹èµ„æºéœ€æ±‚"""
        
        # åŸºäºå†å²æ•°æ®çš„ç®€å•é¢„æµ‹æ¨¡å‹
        historical_demands = self._get_historical_demands(tool_name, 10)  # æœ€è¿‘10æ¬¡
        
        if not historical_demands:
            # æ²¡æœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨å½“å‰è´Ÿè½½ä½œä¸ºåŸºçº¿
            return {
                "cpu": load_pattern["distribution"].get("cpu", 0.5),
                "memory": load_pattern["distribution"].get("memory", 0.3),
                "network": load_pattern["distribution"].get("network", 0.2)
            }
        
        # è®¡ç®—åŠ æƒå¹³å‡é¢„æµ‹
        weights = np.exp(np.linspace(-1, 0, len(historical_demands)))  # æŒ‡æ•°åŠ æƒ
        weights /= weights.sum()
        
        predicted_demand = {}
        for resource in ["cpu", "memory", "network"]:
            resource_demands = [demand.get(resource, 0) for demand in historical_demands]
            predicted_demand[resource] = np.average(resource_demands, weights=weights)
        
        # è€ƒè™‘å½“å‰è´Ÿè½½æ¨¡å¼çš„å½±å“
        current_factor = 0.3  # å½“å‰è´Ÿè½½çš„æƒé‡
        for resource in predicted_demand:
            current_demand = load_pattern["distribution"].get(resource, 0)
            predicted_demand[resource] = (
                (1 - current_factor) * predicted_demand[resource] +
                current_factor * current_demand
            )
        
        return predicted_demand
        
    def _generate_optimization_plan(self, 
                                  tool_name: str,
                                  current_load: Dict[str, float],
                                  predicted_demand: Dict[str, float]) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–è®¡åˆ’"""
        
        optimizations = []
        
        # CPUä¼˜åŒ–
        if predicted_demand.get("cpu", 0) > 0.8:
            optimizations.append({
                "type": "cpu_scaling",
                "action": "increase_worker_threads",
                "target": min(predicted_demand["cpu"] * 1.2, 1.0),
                "priority": "high"
            })
        elif predicted_demand.get("cpu", 0) < 0.3:
            optimizations.append({
                "type": "cpu_scaling", 
                "action": "decrease_worker_threads",
                "target": max(predicted_demand["cpu"] * 1.1, 0.1),
                "priority": "medium"
            })
        
        # å†…å­˜ä¼˜åŒ–
        if predicted_demand.get("memory", 0) > 0.85:
            optimizations.append({
                "type": "memory_optimization",
                "action": "enable_memory_compression",
                "target": 0.8,
                "priority": "high"
            })
            optimizations.append({
                "type": "cache_optimization",
                "action": "increase_cache_size",
                "target": predicted_demand["memory"] * 0.8,
                "priority": "medium"
            })
        
        # ç½‘ç»œä¼˜åŒ–
        if predicted_demand.get("network", 0) > 0.7:
            optimizations.append({
                "type": "network_optimization",
                "action": "enable_connection_pooling",
                "target": 0.6,
                "priority": "medium"
            })
        
        return {
            "tool_name": tool_name,
            "optimizations": optimizations,
            "estimated_impact": self._estimate_optimization_impact(optimizations),
            "execution_order": self._prioritize_optimizations(optimizations)
        }
        
    async def _execute_resource_adjustments(self, optimization_plan: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œèµ„æºè°ƒæ•´"""
        
        adjustments = []
        
        for optimization in optimization_plan["execution_order"]:
            try:
                result = await self._apply_optimization(optimization)
                adjustments.append({
                    "optimization": optimization,
                    "result": result,
                    "success": True,
                    "timestamp": time.time()
                })
                
                # çŸ­æš‚ç­‰å¾…ä»¥è§‚å¯Ÿæ•ˆæœ
                await asyncio.sleep(1.0)
                
            except Exception as e:
                adjustments.append({
                    "optimization": optimization,
                    "error": str(e),
                    "success": False,
                    "timestamp": time.time()
                })
        
        return adjustments
        
    async def _apply_optimization(self, optimization: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨å…·ä½“çš„ä¼˜åŒ–æªæ–½"""
        
        optimization_type = optimization["type"]
        action = optimization["action"]
        
        if optimization_type == "cpu_scaling":
            if action == "increase_worker_threads":
                # å¢åŠ å·¥ä½œçº¿ç¨‹æ•°
                new_thread_count = await self._adjust_thread_pool_size(increase=True)
                return {"new_thread_count": new_thread_count}
                
            elif action == "decrease_worker_threads":
                # å‡å°‘å·¥ä½œçº¿ç¨‹æ•°
                new_thread_count = await self._adjust_thread_pool_size(increase=False)
                return {"new_thread_count": new_thread_count}
        
        elif optimization_type == "memory_optimization":
            if action == "enable_memory_compression":
                # å¯ç”¨å†…å­˜å‹ç¼©
                await self._enable_memory_compression()
                return {"memory_compression": "enabled"}
        
        elif optimization_type == "cache_optimization":
            if action == "increase_cache_size":
                # å¢åŠ ç¼“å­˜å¤§å°
                new_cache_size = await self._adjust_cache_size(increase=True)
                return {"new_cache_size": new_cache_size}
        
        elif optimization_type == "network_optimization":
            if action == "enable_connection_pooling":
                # å¯ç”¨è¿æ¥æ± 
                await self._enable_connection_pooling()
                return {"connection_pooling": "enabled"}
        
        return {"status": "optimization_applied"}
```

## æœ¬èŠ‚æ€»ç»“

æœ¬èŠ‚æ·±å…¥ä»‹ç»äº†å·¥å…·è¿è¡Œæ—¶çš„æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§ï¼š

### ğŸ¯ æ ¸å¿ƒç›‘æ§ä½“ç³»
1. **å¤šç»´åº¦æŒ‡æ ‡**ï¼šç³»ç»Ÿã€åº”ç”¨ã€ä¸šåŠ¡ã€ç”¨æˆ·å››ä¸ªå±‚é¢çš„å…¨æ–¹ä½ç›‘æ§
2. **å®æ—¶æ”¶é›†**ï¼šåŸºäºå¼‚æ­¥æ¶æ„çš„é«˜æ•ˆæŒ‡æ ‡æ”¶é›†æœºåˆ¶
3. **æ™ºèƒ½åˆ†æ**ï¼šåŸºäºæœºå™¨å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹å’Œæ€§èƒ½é¢„æµ‹
4. **è‡ªåŠ¨ä¼˜åŒ–**ï¼šè‡ªé€‚åº”çš„èµ„æºç®¡ç†å’Œæ€§èƒ½è°ƒä¼˜

### ğŸ”§ å…³é”®æŠ€æœ¯å®ç°
- åŸºäºæ—¶é—´åºåˆ—çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œå­˜å‚¨
- ä½¿ç”¨IsolationForestè¿›è¡Œæ€§èƒ½å¼‚å¸¸æ£€æµ‹
- åŸºäºå†å²æ•°æ®çš„èµ„æºéœ€æ±‚é¢„æµ‹ç®—æ³•
- å¤šç­–ç•¥çš„è‡ªåŠ¨åŒ–æ€§èƒ½ä¼˜åŒ–æœºåˆ¶

### ğŸš€ ä¼˜åŒ–ç­–ç•¥ä»·å€¼
- **é¢„é˜²æ€§**ï¼šé€šè¿‡é¢„æµ‹å‘ç°æ½œåœ¨æ€§èƒ½é—®é¢˜
- **è‡ªåŠ¨åŒ–**ï¼šå‡å°‘äººå·¥å¹²é¢„ï¼Œæé«˜è¿ç»´æ•ˆç‡
- **ç²¾å‡†æ€§**ï¼šåŸºäºæ•°æ®åˆ†æçš„ç²¾ç¡®ä¼˜åŒ–å»ºè®®
- **æŒç»­æ€§**ï¼šå»ºç«‹æŒç»­æ”¹è¿›çš„æ€§èƒ½ä¼˜åŒ–å¾ªç¯

---

**ä¸‹ä¸€æ­¥å­¦ä¹ **ï¼šæŒæ¡äº†æ€§èƒ½ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€æœ¯åï¼Œæˆ‘ä»¬å°†åœ¨4.3.7èŠ‚å­¦ä¹ ä¼ä¸šçº§å·¥å…·æ‰©å±•è¿è¡Œå±‚çš„å®‰å…¨ä¸æ²»ç†ï¼Œäº†è§£å¦‚ä½•åœ¨ä¼ä¸šç¯å¢ƒä¸­å®‰å…¨å¯é åœ°è¿è¡Œå·¥å…·ç³»ç»Ÿã€‚

> **ğŸ’¡ æ€§èƒ½ä¼˜åŒ–è¦è¯€**ï¼šæ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­è¿­ä»£çš„è¿‡ç¨‹ï¼Œå…³é”®åœ¨äºå»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»ï¼ŒåŸºäºæ•°æ®é©±åŠ¨å†³ç­–ï¼Œé¿å…è¿‡æ—©ä¼˜åŒ–ï¼Œä¸“æ³¨äºçœŸæ­£çš„æ€§èƒ½ç“¶é¢ˆã€‚
