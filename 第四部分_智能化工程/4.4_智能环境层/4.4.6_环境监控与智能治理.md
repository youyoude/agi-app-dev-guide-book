# 4.4.6 ç¯å¢ƒç›‘æ§ä¸æ™ºèƒ½æ²»ç†

> "ç›‘æ§æ˜¯è¿ç»´çš„çœ¼ç›ï¼Œæ²»ç†æ˜¯ç®¡ç†çš„å¤§è„‘ã€‚æ™ºèƒ½ç¯å¢ƒçš„ç›‘æ§ä¸æ²»ç†ï¼Œè®©ç³»ç»Ÿå…·å¤‡è‡ªæ„ŸçŸ¥ã€è‡ªè¯Šæ–­ã€è‡ªä¼˜åŒ–çš„èƒ½åŠ›ã€‚"

## ğŸ¯ æœ¬èŠ‚å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬èŠ‚å­¦ä¹ åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š
- âœ… å»ºç«‹å…¨æ–¹ä½çš„ç¯å¢ƒç›‘æ§ä½“ç³»
- âœ… å®ç°æ™ºèƒ½å‘Šè­¦å’Œå¼‚å¸¸æ£€æµ‹æœºåˆ¶
- âœ… æ„å»ºæ€§èƒ½åˆ†æå’Œä¼˜åŒ–ç³»ç»Ÿ
- âœ… å»ºç«‹å¯è§†åŒ–ç®¡ç†å’Œæ²»ç†å¹³å°

## å…¨æ–¹ä½ç›‘æ§ä½“ç³»

### å¤šç»´åº¦ç›‘æ§æ¶æ„

æ™ºèƒ½ç¯å¢ƒå±‚éœ€è¦å»ºç«‹è¦†ç›–åŸºç¡€è®¾æ–½ã€åº”ç”¨ã€ä¸šåŠ¡çš„å…¨æ–¹ä½ç›‘æ§ä½“ç³»ï¼š

```mermaid
graph TB
    subgraph "æ™ºèƒ½ç¯å¢ƒç›‘æ§æ¶æ„"
        subgraph "æ•°æ®é‡‡é›†å±‚"
            METRICS[ğŸ“Š æŒ‡æ ‡é‡‡é›†<br/>Prometheus]
            LOGS[ğŸ“ æ—¥å¿—é‡‡é›†<br/>Fluentd]
            TRACES[ğŸ” é“¾è·¯è¿½è¸ª<br/>Jaeger]
            EVENTS[ğŸ“¡ äº‹ä»¶é‡‡é›†<br/>EventBridge]
        end
        
        subgraph "æ•°æ®å­˜å‚¨å±‚"
            TSDB[â° æ—¶åºæ•°æ®åº“<br/>InfluxDB]
            LOGSTORE[ğŸ’¾ æ—¥å¿—å­˜å‚¨<br/>Elasticsearch]
            TRACESTORE[ğŸ—‚ï¸ è¿½è¸ªå­˜å‚¨<br/>Cassandra]
        end
        
        subgraph "æ•°æ®å¤„ç†å±‚"
            ANALYZER[ğŸ§  æ™ºèƒ½åˆ†æ<br/>Machine Learning]
            AGGREGATOR[ğŸ“ˆ æ•°æ®èšåˆ<br/>Stream Processing]
            CORRELATOR[ğŸ”— å…³è”åˆ†æ<br/>Complex Event Processing]
        end
        
        subgraph "å¯è§†åŒ–å±‚"
            DASHBOARD[ğŸ“º ä»ªè¡¨æ¿<br/>Grafana]
            ALERTS[ğŸš¨ å‘Šè­¦ç³»ç»Ÿ<br/>AlertManager]
            REPORTS[ğŸ“‹ æŠ¥å‘Šç³»ç»Ÿ<br/>Custom Reports]
        end
        
        subgraph "æ²»ç†å±‚"
            POLICIES[ğŸ“‹ ç­–ç•¥å¼•æ“<br/>Policy Engine]
            AUTOMATION[ğŸ”„ è‡ªåŠ¨åŒ–<br/>Automation Engine]
            GOVERNANCE[ğŸ‘‘ æ²»ç†ä¸­å¿ƒ<br/>Governance Portal]
        end
    end
    
    METRICS --> TSDB
    LOGS --> LOGSTORE
    TRACES --> TRACESTORE
    
    TSDB --> ANALYZER
    LOGSTORE --> AGGREGATOR
    TRACESTORE --> CORRELATOR
    
    ANALYZER --> DASHBOARD
    AGGREGATOR --> ALERTS
    CORRELATOR --> REPORTS
    
    DASHBOARD --> POLICIES
    ALERTS --> AUTOMATION
    REPORTS --> GOVERNANCE
```

### æ ¸å¿ƒç›‘æ§æŒ‡æ ‡ä½“ç³»

```python
"""
æ™ºèƒ½ç¯å¢ƒç›‘æ§æŒ‡æ ‡å®šä¹‰
file: monitoring/metrics-definitions.py
"""

from dataclasses import dataclass
from typing import Dict, List, Any, Optional
from enum import Enum
import time

class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹"""
    COUNTER = "counter"           # è®¡æ•°å™¨
    GAUGE = "gauge"              # è®¡é‡å™¨
    HISTOGRAM = "histogram"      # ç›´æ–¹å›¾
    SUMMARY = "summary"          # æ‘˜è¦

class MetricCategory(Enum):
    """æŒ‡æ ‡åˆ†ç±»"""
    INFRASTRUCTURE = "infrastructure"   # åŸºç¡€è®¾æ–½æŒ‡æ ‡
    APPLICATION = "application"        # åº”ç”¨æŒ‡æ ‡
    BUSINESS = "business"              # ä¸šåŠ¡æŒ‡æ ‡
    SECURITY = "security"              # å®‰å…¨æŒ‡æ ‡

@dataclass
class MetricDefinition:
    """æŒ‡æ ‡å®šä¹‰"""
    name: str
    type: MetricType
    category: MetricCategory
    description: str
    labels: List[str]
    unit: str
    collection_interval: int  # é‡‡é›†é—´éš”ï¼ˆç§’ï¼‰
    retention_period: int     # ä¿ç•™æœŸï¼ˆå¤©ï¼‰
    alert_rules: List[Dict[str, Any]]

class IntelligentEnvironmentMetrics:
    """æ™ºèƒ½ç¯å¢ƒæŒ‡æ ‡å®šä¹‰"""
    
    @staticmethod
    def get_infrastructure_metrics() -> List[MetricDefinition]:
        """åŸºç¡€è®¾æ–½æŒ‡æ ‡"""
        
        return [
            # CPUæŒ‡æ ‡
            MetricDefinition(
                name="node_cpu_utilization",
                type=MetricType.GAUGE,
                category=MetricCategory.INFRASTRUCTURE,
                description="èŠ‚ç‚¹CPUä½¿ç”¨ç‡",
                labels=["node_id", "cpu_core"],
                unit="percent",
                collection_interval=30,
                retention_period=90,
                alert_rules=[
                    {
                        "name": "HighCPUUsage",
                        "condition": "node_cpu_utilization > 80",
                        "duration": "5m",
                        "severity": "warning"
                    }
                ]
            ),
            
            # å†…å­˜æŒ‡æ ‡
            MetricDefinition(
                name="node_memory_utilization",
                type=MetricType.GAUGE,
                category=MetricCategory.INFRASTRUCTURE,
                description="èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡",
                labels=["node_id", "memory_type"],
                unit="percent", 
                collection_interval=30,
                retention_period=90,
                alert_rules=[
                    {
                        "name": "HighMemoryUsage",
                        "condition": "node_memory_utilization > 85",
                        "duration": "3m",
                        "severity": "critical"
                    }
                ]
            ),
            
            # GPUæŒ‡æ ‡
            MetricDefinition(
                name="gpu_utilization",
                type=MetricType.GAUGE,
                category=MetricCategory.INFRASTRUCTURE,
                description="GPUä½¿ç”¨ç‡",
                labels=["node_id", "gpu_id", "gpu_type"],
                unit="percent",
                collection_interval=10,
                retention_period=30,
                alert_rules=[
                    {
                        "name": "GPUOverheating",
                        "condition": "gpu_temperature > 80",
                        "duration": "1m",
                        "severity": "warning"
                    }
                ]
            ),
            
            # ç½‘ç»œæŒ‡æ ‡
            MetricDefinition(
                name="network_throughput",
                type=MetricType.COUNTER,
                category=MetricCategory.INFRASTRUCTURE,
                description="ç½‘ç»œååé‡",
                labels=["node_id", "interface", "direction"],
                unit="bytes_per_second",
                collection_interval=10,
                retention_period=30,
                alert_rules=[]
            ),
            
            # å­˜å‚¨æŒ‡æ ‡
            MetricDefinition(
                name="disk_utilization",
                type=MetricType.GAUGE,
                category=MetricCategory.INFRASTRUCTURE,
                description="ç£ç›˜ä½¿ç”¨ç‡",
                labels=["node_id", "mount_point", "disk_type"],
                unit="percent",
                collection_interval=60,
                retention_period=180,
                alert_rules=[
                    {
                        "name": "DiskSpaceLow",
                        "condition": "disk_utilization > 90",
                        "duration": "5m",
                        "severity": "critical"
                    }
                ]
            )
        ]
    
    @staticmethod
    def get_application_metrics() -> List[MetricDefinition]:
        """åº”ç”¨æŒ‡æ ‡"""
        
        return [
            # èµ„æºè°ƒåº¦å™¨æŒ‡æ ‡
            MetricDefinition(
                name="scheduler_request_latency",
                type=MetricType.HISTOGRAM,
                category=MetricCategory.APPLICATION,
                description="èµ„æºè°ƒåº¦è¯·æ±‚å»¶è¿Ÿ",
                labels=["scheduler_instance", "request_type"],
                unit="seconds",
                collection_interval=10,
                retention_period=30,
                alert_rules=[
                    {
                        "name": "HighSchedulerLatency",
                        "condition": "histogram_quantile(0.95, scheduler_request_latency) > 5",
                        "duration": "2m",
                        "severity": "warning"
                    }
                ]
            ),
            
            # ç¯å¢ƒç®¡ç†å™¨æŒ‡æ ‡  
            MetricDefinition(
                name="environment_creation_time",
                type=MetricType.HISTOGRAM,
                category=MetricCategory.APPLICATION,
                description="ç¯å¢ƒåˆ›å»ºæ—¶é—´",
                labels=["environment_type", "cluster_id"],
                unit="seconds",
                collection_interval=30,
                retention_period=60,
                alert_rules=[
                    {
                        "name": "SlowEnvironmentCreation",
                        "condition": "histogram_quantile(0.90, environment_creation_time) > 60",
                        "duration": "5m", 
                        "severity": "warning"
                    }
                ]
            ),
            
            # å®‰å…¨æ§åˆ¶å™¨æŒ‡æ ‡
            MetricDefinition(
                name="security_violations_total",
                type=MetricType.COUNTER,
                category=MetricCategory.SECURITY,
                description="å®‰å…¨è¿è§„æ€»æ•°",
                labels=["violation_type", "severity", "source_ip"],
                unit="count",
                collection_interval=60,
                retention_period=365,
                alert_rules=[
                    {
                        "name": "SecurityViolationSpike",
                        "condition": "rate(security_violations_total[5m]) > 10",
                        "duration": "1m",
                        "severity": "critical"
                    }
                ]
            ),
            
            # å·¥å…·æ‰§è¡ŒæŒ‡æ ‡
            MetricDefinition(
                name="tool_execution_success_rate",
                type=MetricType.GAUGE,
                category=MetricCategory.APPLICATION,
                description="å·¥å…·æ‰§è¡ŒæˆåŠŸç‡",
                labels=["tool_type", "environment_type"],
                unit="percent",
                collection_interval=60,
                retention_period=90,
                alert_rules=[
                    {
                        "name": "LowToolSuccessRate",
                        "condition": "tool_execution_success_rate < 95",
                        "duration": "10m",
                        "severity": "warning"
                    }
                ]
            )
        ]
    
    @staticmethod
    def get_business_metrics() -> List[MetricDefinition]:
        """ä¸šåŠ¡æŒ‡æ ‡"""
        
        return [
            # ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
            MetricDefinition(
                name="user_request_satisfaction",
                type=MetricType.GAUGE,
                category=MetricCategory.BUSINESS,
                description="ç”¨æˆ·è¯·æ±‚æ»¡æ„åº¦",
                labels=["user_tier", "request_complexity"],
                unit="score",
                collection_interval=300,
                retention_period=180,
                alert_rules=[
                    {
                        "name": "LowUserSatisfaction",
                        "condition": "user_request_satisfaction < 4.0",
                        "duration": "15m",
                        "severity": "warning"
                    }
                ]
            ),
            
            # èµ„æºåˆ©ç”¨æ•ˆç‡
            MetricDefinition(
                name="resource_efficiency_score",
                type=MetricType.GAUGE,
                category=MetricCategory.BUSINESS,
                description="èµ„æºåˆ©ç”¨æ•ˆç‡åˆ†æ•°",
                labels=["resource_type", "cluster_id"],
                unit="score",
                collection_interval=300,
                retention_period=365,
                alert_rules=[
                    {
                        "name": "LowResourceEfficiency",
                        "condition": "resource_efficiency_score < 70",
                        "duration": "30m",
                        "severity": "info"
                    }
                ]
            ),
            
            # æˆæœ¬æŒ‡æ ‡
            MetricDefinition(
                name="operational_cost_per_request",
                type=MetricType.GAUGE,
                category=MetricCategory.BUSINESS,
                description="æ¯è¯·æ±‚è¿è¥æˆæœ¬",
                labels=["cost_center", "environment"],
                unit="currency",
                collection_interval=3600,
                retention_period=365,
                alert_rules=[
                    {
                        "name": "HighOperationalCost",
                        "condition": "operational_cost_per_request > threshold",
                        "duration": "1h",
                        "severity": "info"
                    }
                ]
            )
        ]

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.prometheus_gateway = PrometheusGateway()
        self.metric_definitions = {}
        self.collection_tasks = {}
        
    async def register_metrics(self, metrics: List[MetricDefinition]):
        """æ³¨å†Œç›‘æ§æŒ‡æ ‡"""
        
        for metric in metrics:
            self.metric_definitions[metric.name] = metric
            
            # åˆ›å»ºPrometheusæŒ‡æ ‡å¯¹è±¡
            await self._create_prometheus_metric(metric)
            
            # å¯åŠ¨é‡‡é›†ä»»åŠ¡
            collection_task = asyncio.create_task(
                self._start_metric_collection(metric)
            )
            self.collection_tasks[metric.name] = collection_task
    
    async def _create_prometheus_metric(self, metric: MetricDefinition):
        """åˆ›å»ºPrometheusæŒ‡æ ‡"""
        
        if metric.type == MetricType.COUNTER:
            prometheus_metric = Counter(
                name=metric.name,
                documentation=metric.description,
                labelnames=metric.labels
            )
        elif metric.type == MetricType.GAUGE:
            prometheus_metric = Gauge(
                name=metric.name,
                documentation=metric.description,
                labelnames=metric.labels
            )
        elif metric.type == MetricType.HISTOGRAM:
            prometheus_metric = Histogram(
                name=metric.name,
                documentation=metric.description,
                labelnames=metric.labels
            )
        
        # æ³¨å†Œåˆ°Prometheus
        await self.prometheus_gateway.register_metric(prometheus_metric)
    
    async def _start_metric_collection(self, metric: MetricDefinition):
        """å¯åŠ¨æŒ‡æ ‡é‡‡é›†"""
        
        while True:
            try:
                # é‡‡é›†æŒ‡æ ‡æ•°æ®
                metric_data = await self._collect_metric_data(metric)
                
                # å‘é€åˆ°Prometheus
                await self.prometheus_gateway.push_metric(
                    metric.name, metric_data
                )
                
                # ç­‰å¾…ä¸‹æ¬¡é‡‡é›†
                await asyncio.sleep(metric.collection_interval)
                
            except Exception as e:
                logging.error(f"æŒ‡æ ‡é‡‡é›†å¤±è´¥ {metric.name}: {str(e)}")
                await asyncio.sleep(60)  # é”™è¯¯æ—¶ç­‰å¾…1åˆ†é’Ÿ
    
    async def _collect_metric_data(self, metric: MetricDefinition) -> Dict[str, Any]:
        """é‡‡é›†æŒ‡æ ‡æ•°æ®"""
        
        if metric.category == MetricCategory.INFRASTRUCTURE:
            return await self._collect_infrastructure_metric(metric)
        elif metric.category == MetricCategory.APPLICATION:
            return await self._collect_application_metric(metric)
        elif metric.category == MetricCategory.BUSINESS:
            return await self._collect_business_metric(metric)
        elif metric.category == MetricCategory.SECURITY:
            return await self._collect_security_metric(metric)
        
        return {}
```

## æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ

### å¤šç»´åº¦å‘Šè­¦è§„åˆ™å¼•æ“

```python
"""
æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ
file: monitoring/intelligent-alerting.py
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import json
import re

class AlertSeverity(Enum):
    """å‘Šè­¦ä¸¥é‡ç¨‹åº¦"""
    INFO = 1
    WARNING = 2
    ERROR = 3
    CRITICAL = 4
    EMERGENCY = 5

class AlertState(Enum):
    """å‘Šè­¦çŠ¶æ€"""
    FIRING = "firing"
    PENDING = "pending"
    RESOLVED = "resolved"
    SILENCED = "silenced"

@dataclass
class AlertRule:
    """å‘Šè­¦è§„åˆ™"""
    rule_id: str
    name: str
    description: str
    condition: str
    severity: AlertSeverity
    duration: str
    labels: Dict[str, str]
    annotations: Dict[str, str]
    enabled: bool = True

@dataclass
class Alert:
    """å‘Šè­¦å®ä¾‹"""
    alert_id: str
    rule_id: str
    name: str
    severity: AlertSeverity
    state: AlertState
    message: str
    labels: Dict[str, str]
    started_at: datetime
    ended_at: Optional[datetime] = None
    acknowledged_by: Optional[str] = None
    acknowledged_at: Optional[datetime] = None

class IntelligentAlertingEngine:
    """æ™ºèƒ½å‘Šè­¦å¼•æ“"""
    
    def __init__(self):
        self.alert_rules = {}
        self.active_alerts = {}
        self.alert_history = []
        self.notification_channels = {}
        self.ml_predictor = AlertPredictor()
        self.noise_reducer = AlertNoiseReducer()
        
    async def register_alert_rule(self, rule: AlertRule):
        """æ³¨å†Œå‘Šè­¦è§„åˆ™"""
        
        # éªŒè¯è§„åˆ™è¯­æ³•
        if not await self._validate_rule_syntax(rule.condition):
            raise ValueError(f"å‘Šè­¦è§„åˆ™è¯­æ³•é”™è¯¯: {rule.condition}")
        
        self.alert_rules[rule.rule_id] = rule
        logging.info(f"æ³¨å†Œå‘Šè­¦è§„åˆ™: {rule.name}")
        
        # å¯åŠ¨è§„åˆ™è¯„ä¼°ä»»åŠ¡
        asyncio.create_task(self._evaluate_rule_continuously(rule))
    
    async def _validate_rule_syntax(self, condition: str) -> bool:
        """éªŒè¯å‘Šè­¦è§„åˆ™è¯­æ³•"""
        
        # æ£€æŸ¥PromQLè¯­æ³•
        try:
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨çœŸæ­£çš„PromQLè§£æå™¨
            # ç®€åŒ–ç‰ˆæœ¬ï¼Œæ£€æŸ¥åŸºæœ¬è¯­æ³•
            if not condition.strip():
                return False
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¯”è¾ƒæ“ä½œç¬¦
            comparison_operators = ['>', '<', '>=', '<=', '==', '!=']
            has_comparison = any(op in condition for op in comparison_operators)
            
            if not has_comparison:
                return False
            
            return True
            
        except Exception:
            return False
    
    async def _evaluate_rule_continuously(self, rule: AlertRule):
        """æŒç»­è¯„ä¼°å‘Šè­¦è§„åˆ™"""
        
        while rule.enabled:
            try:
                # è§£ææŒç»­æ—¶é—´
                duration_seconds = self._parse_duration(rule.duration)
                
                # è¯„ä¼°è§„åˆ™æ¡ä»¶
                evaluation_result = await self._evaluate_rule_condition(
                    rule.condition, duration_seconds
                )
                
                if evaluation_result["triggered"]:
                    # åˆ›å»ºæˆ–æ›´æ–°å‘Šè­¦
                    await self._handle_alert_triggered(rule, evaluation_result)
                else:
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è§£å†³å‘Šè­¦
                    await self._handle_alert_resolved(rule.rule_id)
                
                # ç­‰å¾…ä¸‹æ¬¡è¯„ä¼°
                await asyncio.sleep(30)  # æ¯30ç§’è¯„ä¼°ä¸€æ¬¡
                
            except Exception as e:
                logging.error(f"å‘Šè­¦è§„åˆ™è¯„ä¼°å¤±è´¥ {rule.name}: {str(e)}")
                await asyncio.sleep(60)
    
    def _parse_duration(self, duration_str: str) -> int:
        """è§£ææŒç»­æ—¶é—´å­—ç¬¦ä¸²"""
        
        duration_map = {
            's': 1,
            'm': 60,
            'h': 3600,
            'd': 86400
        }
        
        # è§£æå¦‚ "5m", "1h", "30s" ç­‰æ ¼å¼
        match = re.match(r'(\d+)([smhd])', duration_str)
        if match:
            value, unit = match.groups()
            return int(value) * duration_map[unit]
        
        return 300  # é»˜è®¤5åˆ†é’Ÿ
    
    async def _evaluate_rule_condition(self, 
                                     condition: str, 
                                     duration_seconds: int) -> Dict[str, Any]:
        """è¯„ä¼°è§„åˆ™æ¡ä»¶"""
        
        # æŸ¥è¯¢Prometheusè·å–æŒ‡æ ‡æ•°æ®
        query_result = await self._query_prometheus(condition)
        
        if not query_result["success"]:
            return {"triggered": False, "error": query_result["error"]}
        
        # æ£€æŸ¥æ¡ä»¶æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´å†…æŒç»­æ»¡è¶³
        values = query_result["data"]
        
        if not values:
            return {"triggered": False}
        
        # ç®€åŒ–é€»è¾‘ï¼šå¦‚æœæœ‰å€¼ä¸”æ»¡è¶³æ¡ä»¶ï¼Œåˆ™è®¤ä¸ºè§¦å‘
        triggered = len(values) > 0
        
        return {
            "triggered": triggered,
            "values": values,
            "evaluated_at": datetime.now(),
            "duration_satisfied": duration_seconds
        }
    
    async def _handle_alert_triggered(self, 
                                    rule: AlertRule,
                                    evaluation_result: Dict[str, Any]):
        """å¤„ç†å‘Šè­¦è§¦å‘"""
        
        alert_id = f"{rule.rule_id}_{datetime.now().timestamp()}"
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„æ´»è·ƒå‘Šè­¦
        existing_alert = self._find_active_alert(rule.rule_id)
        
        if existing_alert:
            # æ›´æ–°ç°æœ‰å‘Šè­¦
            existing_alert.message = f"æŒç»­è§¦å‘: {rule.description}"
            logging.info(f"å‘Šè­¦æŒç»­è§¦å‘: {rule.name}")
            return
        
        # åˆ›å»ºæ–°å‘Šè­¦
        alert = Alert(
            alert_id=alert_id,
            rule_id=rule.rule_id,
            name=rule.name,
            severity=rule.severity,
            state=AlertState.FIRING,
            message=rule.description,
            labels=rule.labels,
            started_at=datetime.now()
        )
        
        # æ™ºèƒ½é™å™ªå¤„ç†
        if await self.noise_reducer.should_suppress_alert(alert):
            alert.state = AlertState.SILENCED
            logging.info(f"å‘Šè­¦è¢«æ™ºèƒ½é™å™ªæŠ‘åˆ¶: {rule.name}")
        else:
            self.active_alerts[alert_id] = alert
            
            # å‘é€é€šçŸ¥
            await self._send_alert_notification(alert)
            
            logging.warning(f"å‘Šè­¦è§¦å‘: {rule.name}")
    
    async def _handle_alert_resolved(self, rule_id: str):
        """å¤„ç†å‘Šè­¦è§£å†³"""
        
        # æŸ¥æ‰¾ç›¸å…³çš„æ´»è·ƒå‘Šè­¦
        resolved_alerts = []
        
        for alert_id, alert in list(self.active_alerts.items()):
            if alert.rule_id == rule_id:
                alert.state = AlertState.RESOLVED
                alert.ended_at = datetime.now()
                
                resolved_alerts.append(alert)
                del self.active_alerts[alert_id]
        
        # å‘é€è§£å†³é€šçŸ¥
        for alert in resolved_alerts:
            await self._send_alert_resolution_notification(alert)
            
            # ç§»åŠ¨åˆ°å†å²è®°å½•
            self.alert_history.append(alert)
            
            logging.info(f"å‘Šè­¦å·²è§£å†³: {alert.name}")
    
    def _find_active_alert(self, rule_id: str) -> Optional[Alert]:
        """æŸ¥æ‰¾æ´»è·ƒå‘Šè­¦"""
        
        for alert in self.active_alerts.values():
            if alert.rule_id == rule_id:
                return alert
        
        return None
    
    async def _send_alert_notification(self, alert: Alert):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        
        # æ ¹æ®å‘Šè­¦ä¸¥é‡ç¨‹åº¦é€‰æ‹©é€šçŸ¥æ¸ é“
        notification_channels = await self._select_notification_channels(alert.severity)
        
        notification_message = {
            "alert_id": alert.alert_id,
            "name": alert.name,
            "severity": alert.severity.name,
            "message": alert.message,
            "labels": alert.labels,
            "started_at": alert.started_at.isoformat(),
            "dashboard_url": f"https://grafana.example.com/alerts/{alert.alert_id}"
        }
        
        # å‘é€åˆ°å„ä¸ªé€šçŸ¥æ¸ é“
        for channel_name, channel in notification_channels.items():
            try:
                await channel.send_notification(notification_message)
            except Exception as e:
                logging.error(f"å‘é€å‘Šè­¦é€šçŸ¥å¤±è´¥ {channel_name}: {str(e)}")
    
    async def _select_notification_channels(self, 
                                          severity: AlertSeverity) -> Dict[str, Any]:
        """é€‰æ‹©é€šçŸ¥æ¸ é“"""
        
        channels = {}
        
        # æ ¹æ®ä¸¥é‡ç¨‹åº¦é€‰æ‹©æ¸ é“
        if severity in [AlertSeverity.CRITICAL, AlertSeverity.EMERGENCY]:
            channels.update({
                "slack": self.notification_channels.get("slack"),
                "pagerduty": self.notification_channels.get("pagerduty"),
                "email": self.notification_channels.get("email"),
                "sms": self.notification_channels.get("sms")
            })
        elif severity == AlertSeverity.ERROR:
            channels.update({
                "slack": self.notification_channels.get("slack"),
                "email": self.notification_channels.get("email")
            })
        elif severity == AlertSeverity.WARNING:
            channels.update({
                "slack": self.notification_channels.get("slack")
            })
        
        # è¿‡æ»¤Noneå€¼
        return {k: v for k, v in channels.items() if v is not None}

class AlertPredictor:
    """å‘Šè­¦é¢„æµ‹å™¨"""
    
    def __init__(self):
        self.historical_data = []
        self.prediction_model = None
        
    async def predict_upcoming_alerts(self, 
                                    time_horizon: int = 3600) -> List[Dict[str, Any]]:
        """é¢„æµ‹å³å°†å‘ç”Ÿçš„å‘Šè­¦"""
        
        # åŸºäºå†å²æ¨¡å¼é¢„æµ‹
        predictions = []
        
        # åˆ†æå†å²å‘Šè­¦æ¨¡å¼
        patterns = await self._analyze_alert_patterns()
        
        for pattern in patterns:
            # é¢„æµ‹åŸºäºæ¨¡å¼çš„å¯èƒ½å‘Šè­¦
            if pattern["likelihood"] > 0.7:
                prediction = {
                    "predicted_alert": pattern["alert_type"],
                    "likelihood": pattern["likelihood"],
                    "estimated_time": datetime.now() + timedelta(seconds=pattern["time_offset"]),
                    "suggested_actions": pattern["preventive_actions"]
                }
                predictions.append(prediction)
        
        return predictions
    
    async def _analyze_alert_patterns(self) -> List[Dict[str, Any]]:
        """åˆ†æå‘Šè­¦æ¨¡å¼"""
        
        # ç®€åŒ–çš„æ¨¡å¼è¯†åˆ«é€»è¾‘
        patterns = []
        
        # åˆ†ææ—¶é—´æ¨¡å¼
        time_patterns = self._analyze_time_patterns()
        
        # åˆ†æä¾èµ–å…³ç³»æ¨¡å¼
        dependency_patterns = self._analyze_dependency_patterns()
        
        # åˆ†æèµ„æºè€—å°½æ¨¡å¼
        resource_patterns = self._analyze_resource_patterns()
        
        patterns.extend(time_patterns)
        patterns.extend(dependency_patterns)
        patterns.extend(resource_patterns)
        
        return patterns
    
    def _analyze_time_patterns(self) -> List[Dict[str, Any]]:
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        
        # ä¾‹å¦‚ï¼šæ¯å¤©ç‰¹å®šæ—¶é—´çš„å‘Šè­¦æ¨¡å¼
        return [
            {
                "alert_type": "high_cpu_usage",
                "pattern_type": "daily_peak",
                "likelihood": 0.8,
                "time_offset": 3600,  # 1å°æ—¶å
                "preventive_actions": ["é¢„æ‰©å®¹", "è´Ÿè½½å‡è¡¡è°ƒæ•´"]
            }
        ]

class AlertNoiseReducer:
    """å‘Šè­¦é™å™ªå™¨"""
    
    def __init__(self):
        self.suppression_rules = []
        self.alert_correlations = {}
        
    async def should_suppress_alert(self, alert: Alert) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æŠ‘åˆ¶å‘Šè­¦"""
        
        # æ£€æŸ¥æŠ‘åˆ¶è§„åˆ™
        for rule in self.suppression_rules:
            if await self._matches_suppression_rule(alert, rule):
                return True
        
        # æ£€æŸ¥å‘Šè­¦å…³è”æ€§
        if await self._is_duplicate_alert(alert):
            return True
        
        # æ£€æŸ¥å‘Šè­¦é¢‘ç‡
        if await self._is_alert_flooding(alert):
            return True
        
        return False
    
    async def _matches_suppression_rule(self, 
                                      alert: Alert, 
                                      rule: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ¹é…æŠ‘åˆ¶è§„åˆ™"""
        
        # æ£€æŸ¥æ ‡ç­¾åŒ¹é…
        required_labels = rule.get("labels", {})
        for key, value in required_labels.items():
            if alert.labels.get(key) != value:
                return False
        
        # æ£€æŸ¥æ—¶é—´çª—å£
        time_window = rule.get("time_window", {})
        if time_window:
            current_time = datetime.now()
            start_time = datetime.strptime(time_window["start"], "%H:%M")
            end_time = datetime.strptime(time_window["end"], "%H:%M")
            
            if not (start_time.time() <= current_time.time() <= end_time.time()):
                return False
        
        return True

## æ€§èƒ½åˆ†æä¸ä¼˜åŒ–

### æ™ºèƒ½æ€§èƒ½åˆ†æç³»ç»Ÿ

```python
"""
æ™ºèƒ½æ€§èƒ½åˆ†æç³»ç»Ÿ
file: monitoring/performance-analyzer.py
"""

import asyncio
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import logging

class PerformanceMetricType(Enum):
    """æ€§èƒ½æŒ‡æ ‡ç±»å‹"""
    LATENCY = "latency"
    THROUGHPUT = "throughput"
    ERROR_RATE = "error_rate"
    RESOURCE_UTILIZATION = "resource_utilization"
    SATURATION = "saturation"

class AnalysisType(Enum):
    """åˆ†æç±»å‹"""
    TREND_ANALYSIS = "trend_analysis"
    ANOMALY_DETECTION = "anomaly_detection"
    BOTTLENECK_IDENTIFICATION = "bottleneck_identification"
    CAPACITY_PLANNING = "capacity_planning"
    ROOT_CAUSE_ANALYSIS = "root_cause_analysis"

@dataclass
class PerformanceBaseline:
    """æ€§èƒ½åŸºçº¿"""
    metric_name: str
    baseline_value: float
    acceptable_range: Tuple[float, float]
    measurement_window: timedelta
    confidence_level: float
    last_updated: datetime

@dataclass
class PerformanceAnomaly:
    """æ€§èƒ½å¼‚å¸¸"""
    anomaly_id: str
    metric_name: str
    detected_at: datetime
    severity: float
    deviation_score: float
    affected_components: List[str]
    probable_causes: List[str]
    recommended_actions: List[str]

class IntelligentPerformanceAnalyzer:
    """æ™ºèƒ½æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.baselines = {}
        self.anomaly_detector = AnomalyDetector()
        self.trend_analyzer = TrendAnalyzer()
        self.bottleneck_detector = BottleneckDetector()
        self.capacity_planner = CapacityPlanner()
        self.root_cause_analyzer = RootCauseAnalyzer()
        
    async def establish_performance_baselines(self):
        """å»ºç«‹æ€§èƒ½åŸºçº¿"""
        
        logging.info("å¼€å§‹å»ºç«‹æ€§èƒ½åŸºçº¿")
        
        # æ”¶é›†å†å²æ•°æ®
        historical_data = await self._collect_historical_performance_data()
        
        # ä¸ºæ¯ä¸ªæŒ‡æ ‡å»ºç«‹åŸºçº¿
        for metric_name, data in historical_data.items():
            baseline = await self._calculate_baseline(metric_name, data)
            self.baselines[metric_name] = baseline
            
            logging.info(f"å»ºç«‹æ€§èƒ½åŸºçº¿: {metric_name} = {baseline.baseline_value}")
        
        logging.info("æ€§èƒ½åŸºçº¿å»ºç«‹å®Œæˆ")
    
    async def _calculate_baseline(self, 
                                metric_name: str,
                                historical_data: List[float]) -> PerformanceBaseline:
        """è®¡ç®—æ€§èƒ½åŸºçº¿"""
        
        if not historical_data:
            raise ValueError(f"ç¼ºå°‘å†å²æ•°æ®: {metric_name}")
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        data_array = np.array(historical_data)
        
        # ç§»é™¤å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
        q1, q3 = np.percentile(data_array, [25, 75])
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        cleaned_data = data_array[(data_array >= lower_bound) & (data_array <= upper_bound)]
        
        # è®¡ç®—åŸºçº¿å€¼ï¼ˆä½¿ç”¨ä¸­ä½æ•°æ›´ç¨³å®šï¼‰
        baseline_value = np.median(cleaned_data)
        
        # è®¡ç®—å¯æ¥å—èŒƒå›´ï¼ˆåŸºäºæ ‡å‡†å·®ï¼‰
        std_dev = np.std(cleaned_data)
        acceptable_range = (
            baseline_value - 2 * std_dev,
            baseline_value + 2 * std_dev
        )
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence_level = len(cleaned_data) / len(data_array)
        
        return PerformanceBaseline(
            metric_name=metric_name,
            baseline_value=float(baseline_value),
            acceptable_range=acceptable_range,
            measurement_window=timedelta(days=7),
            confidence_level=float(confidence_level),
            last_updated=datetime.now()
        )
    
    async def analyze_performance_trends(self, 
                                       time_range: timedelta = timedelta(hours=24)) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        
        analysis_results = {}
        
        # è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ€§èƒ½æ•°æ®
        end_time = datetime.now()
        start_time = end_time - time_range
        
        performance_data = await self._query_performance_data(start_time, end_time)
        
        for metric_name, data_points in performance_data.items():
            trend_analysis = await self.trend_analyzer.analyze_trend(
                metric_name, data_points
            )
            analysis_results[metric_name] = trend_analysis
        
        # ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
        trend_report = await self._generate_trend_report(analysis_results)
        
        return {
            "analysis_results": analysis_results,
            "trend_report": trend_report,
            "analyzed_at": datetime.now(),
            "time_range": str(time_range)
        }
    
    async def detect_performance_anomalies(self) -> List[PerformanceAnomaly]:
        """æ£€æµ‹æ€§èƒ½å¼‚å¸¸"""
        
        detected_anomalies = []
        
        # è·å–å®æ—¶æ€§èƒ½æ•°æ®
        current_metrics = await self._get_current_performance_metrics()
        
        for metric_name, current_value in current_metrics.items():
            if metric_name not in self.baselines:
                continue
            
            baseline = self.baselines[metric_name]
            
            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºå¯æ¥å—èŒƒå›´
            if not (baseline.acceptable_range[0] <= current_value <= baseline.acceptable_range[1]):
                # è®¡ç®—åå·®åˆ†æ•°
                deviation_score = await self._calculate_deviation_score(
                    current_value, baseline
                )
                
                # åˆ†æå¯èƒ½åŸå› 
                probable_causes = await self._analyze_probable_causes(
                    metric_name, current_value, baseline
                )
                
                # ç”Ÿæˆæ¨èæ“ä½œ
                recommended_actions = await self._generate_recommended_actions(
                    metric_name, deviation_score, probable_causes
                )
                
                anomaly = PerformanceAnomaly(
                    anomaly_id=f"perf_anomaly_{metric_name}_{int(datetime.now().timestamp())}",
                    metric_name=metric_name,
                    detected_at=datetime.now(),
                    severity=min(deviation_score / 3.0, 1.0),  # æ ‡å‡†åŒ–åˆ°0-1
                    deviation_score=deviation_score,
                    affected_components=await self._identify_affected_components(metric_name),
                    probable_causes=probable_causes,
                    recommended_actions=recommended_actions
                )
                
                detected_anomalies.append(anomaly)
                
                logging.warning(f"æ£€æµ‹åˆ°æ€§èƒ½å¼‚å¸¸: {metric_name} = {current_value} (åŸºçº¿: {baseline.baseline_value})")
        
        return detected_anomalies
    
    async def identify_bottlenecks(self) -> Dict[str, Any]:
        """è¯†åˆ«ç³»ç»Ÿç“¶é¢ˆ"""
        
        bottleneck_analysis = await self.bottleneck_detector.analyze_system_bottlenecks()
        
        return {
            "identified_bottlenecks": bottleneck_analysis["bottlenecks"],
            "impact_assessment": bottleneck_analysis["impact"],
            "optimization_recommendations": bottleneck_analysis["recommendations"],
            "analyzed_at": datetime.now()
        }
    
    async def perform_capacity_planning(self, 
                                      forecast_horizon: timedelta = timedelta(days=30)) -> Dict[str, Any]:
        """æ‰§è¡Œå®¹é‡è§„åˆ’"""
        
        capacity_analysis = await self.capacity_planner.analyze_capacity_requirements(
            forecast_horizon
        )
        
        return {
            "current_capacity": capacity_analysis["current"],
            "projected_demand": capacity_analysis["projected"],
            "capacity_gaps": capacity_analysis["gaps"],
            "scaling_recommendations": capacity_analysis["scaling_recommendations"],
            "forecast_horizon": str(forecast_horizon),
            "analyzed_at": datetime.now()
        }

class BottleneckDetector:
    """ç“¶é¢ˆæ£€æµ‹å™¨"""
    
    def __init__(self):
        self.dependency_graph = SystemDependencyGraph()
        
    async def analyze_system_bottlenecks(self) -> Dict[str, Any]:
        """åˆ†æç³»ç»Ÿç“¶é¢ˆ"""
        
        # è·å–ç³»ç»Ÿç»„ä»¶æ€§èƒ½æ•°æ®
        component_metrics = await self._collect_component_metrics()
        
        # è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
        bottlenecks = []
        
        for component_name, metrics in component_metrics.items():
            bottleneck_score = await self._calculate_bottleneck_score(
                component_name, metrics
            )
            
            if bottleneck_score > 0.7:  # ç“¶é¢ˆé˜ˆå€¼
                bottleneck_info = {
                    "component": component_name,
                    "bottleneck_score": bottleneck_score,
                    "limiting_factors": await self._identify_limiting_factors(
                        component_name, metrics
                    ),
                    "downstream_impact": await self._assess_downstream_impact(
                        component_name
                    )
                }
                bottlenecks.append(bottleneck_info)
        
        # è¯„ä¼°ç“¶é¢ˆå½±å“
        impact_assessment = await self._assess_bottleneck_impact(bottlenecks)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_recommendations = await self._generate_optimization_recommendations(
            bottlenecks
        )
        
        return {
            "bottlenecks": bottlenecks,
            "impact": impact_assessment,
            "recommendations": optimization_recommendations
        }
    
    async def _calculate_bottleneck_score(self, 
                                        component_name: str,
                                        metrics: Dict[str, float]) -> float:
        """è®¡ç®—ç“¶é¢ˆåˆ†æ•°"""
        
        # åŸºäºå¤šä¸ªæŒ‡æ ‡è®¡ç®—ç»¼åˆç“¶é¢ˆåˆ†æ•°
        factors = {
            "cpu_utilization": metrics.get("cpu_utilization", 0) / 100,
            "memory_utilization": metrics.get("memory_utilization", 0) / 100,
            "disk_io_wait": min(metrics.get("disk_io_wait", 0) / 50, 1.0),
            "network_utilization": metrics.get("network_utilization", 0) / 100,
            "queue_depth": min(metrics.get("queue_depth", 0) / 10, 1.0)
        }
        
        # æƒé‡é…ç½®
        weights = {
            "cpu_utilization": 0.25,
            "memory_utilization": 0.25,
            "disk_io_wait": 0.2,
            "network_utilization": 0.15,
            "queue_depth": 0.15
        }
        
        # è®¡ç®—åŠ æƒåˆ†æ•°
        bottleneck_score = sum(
            factors[factor] * weights[factor]
            for factor in factors
            if factor in weights
        )
        
        return min(bottleneck_score, 1.0)

class CapacityPlanner:
    """å®¹é‡è§„åˆ’å™¨"""
    
    def __init__(self):
        self.forecasting_models = {}
        self.growth_patterns = {}
        
    async def analyze_capacity_requirements(self, 
                                          forecast_horizon: timedelta) -> Dict[str, Any]:
        """åˆ†æå®¹é‡éœ€æ±‚"""
        
        # æ”¶é›†å†å²èµ„æºä½¿ç”¨æ•°æ®
        historical_usage = await self._collect_historical_usage_data()
        
        # åˆ†æå¢é•¿æ¨¡å¼
        growth_patterns = await self._analyze_growth_patterns(historical_usage)
        
        # é¢„æµ‹æœªæ¥éœ€æ±‚
        demand_forecast = await self._forecast_demand(
            historical_usage, growth_patterns, forecast_horizon
        )
        
        # è¯„ä¼°å½“å‰å®¹é‡
        current_capacity = await self._assess_current_capacity()
        
        # è¯†åˆ«å®¹é‡ç¼ºå£
        capacity_gaps = await self._identify_capacity_gaps(
            current_capacity, demand_forecast
        )
        
        # ç”Ÿæˆæ‰©å®¹å»ºè®®
        scaling_recommendations = await self._generate_scaling_recommendations(
            capacity_gaps
        )
        
        return {
            "current": current_capacity,
            "projected": demand_forecast,
            "gaps": capacity_gaps,
            "scaling_recommendations": scaling_recommendations
        }
    
    async def _forecast_demand(self,
                             historical_data: Dict[str, List[float]],
                             growth_patterns: Dict[str, Any],
                             horizon: timedelta) -> Dict[str, Any]:
        """é¢„æµ‹éœ€æ±‚"""
        
        forecasts = {}
        
        for resource_type, usage_history in historical_data.items():
            if len(usage_history) < 10:  # éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
                continue
            
            # ä½¿ç”¨æ—¶é—´åºåˆ—é¢„æµ‹
            forecast_values = await self._time_series_forecast(
                usage_history, horizon
            )
            
            forecasts[resource_type] = {
                "predicted_values": forecast_values,
                "confidence_interval": await self._calculate_confidence_interval(
                    forecast_values
                ),
                "peak_demand": max(forecast_values),
                "average_demand": np.mean(forecast_values)
            }
        
        return forecasts
```

## å¯è§†åŒ–ç®¡ç†å¹³å°

### Grafanaä»ªè¡¨æ¿é…ç½®

```json
{
  "dashboard": {
    "title": "æ™ºèƒ½ç¯å¢ƒå±‚ç›‘æ§ä»ªè¡¨æ¿",
    "tags": ["intelligent-environment", "agi", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "title": "ç³»ç»Ÿæ¦‚è§ˆ",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"intelligent-environment\"}",
            "legendFormat": "åœ¨çº¿æœåŠ¡æ•°"
          },
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "è¯·æ±‚é€Ÿç‡"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95%å»¶è¿Ÿ"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "unit": "short"
          }
        }
      },
      {
        "title": "èµ„æºè°ƒåº¦å™¨æ€§èƒ½",
        "type": "graph",
        "targets": [
          {
            "expr": "scheduler_request_latency_seconds",
            "legendFormat": "è°ƒåº¦å»¶è¿Ÿ - {{instance}}"
          },
          {
            "expr": "scheduler_queue_length",
            "legendFormat": "é˜Ÿåˆ—é•¿åº¦ - {{instance}}"
          },
          {
            "expr": "rate(scheduler_requests_total[5m])",
            "legendFormat": "è°ƒåº¦è¯·æ±‚é€Ÿç‡ - {{instance}}"
          }
        ],
        "xAxes": [
          {
            "type": "time"
          }
        ],
        "yAxes": [
          {
            "unit": "seconds",
            "min": 0
          }
        ]
      },
      {
        "title": "ç¯å¢ƒç®¡ç†å™¨çŠ¶æ€",
        "type": "table",
        "targets": [
          {
            "expr": "environment_manager_environments_total",
            "format": "table"
          }
        ],
        "transformations": [
          {
            "id": "organize",
            "options": {
              "excludeByName": {},
              "indexByName": {},
              "renameByName": {
                "environment_type": "ç¯å¢ƒç±»å‹",
                "status": "çŠ¶æ€",
                "count": "æ•°é‡"
              }
            }
          }
        ]
      },
      {
        "title": "å®‰å…¨äº‹ä»¶ç›‘æ§",
        "type": "logs",
        "targets": [
          {
            "expr": "{job=\"security-controller\"} |= \"violation\"",
            "refId": "A"
          }
        ],
        "options": {
          "showTime": true,
          "showLabels": false,
          "showCommonLabels": false,
          "wrapLogMessage": false,
          "prettifyLogMessage": false,
          "enableLogDetails": true
        }
      },
      {
        "title": "èµ„æºåˆ©ç”¨ç‡çƒ­åŠ›å›¾",
        "type": "heatmap",
        "targets": [
          {
            "expr": "node_cpu_utilization",
            "legendFormat": "{{node_id}}"
          }
        ],
        "heatmap": {
          "xAxis": {
            "show": true
          },
          "yAxis": {
            "show": true,
            "logBase": 1,
            "min": 0,
            "max": 100
          },
          "yBucketBound": "auto"
        }
      },
      {
        "title": "å‘Šè­¦ç»Ÿè®¡",
        "type": "piechart",
        "targets": [
          {
            "expr": "sum by (severity) (ALERTS)",
            "legendFormat": "{{severity}}"
          }
        ],
        "options": {
          "reduceOptions": {
            "values": false,
            "calcs": ["lastNotNull"],
            "fields": ""
          },
          "pieType": "pie",
          "tooltip": {
            "mode": "single"
          },
          "legend": {
            "displayMode": "table",
            "placement": "right"
          }
        }
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "5s"
  }
}
```

### æ™ºèƒ½è¿ç»´å†³ç­–æ”¯æŒç³»ç»Ÿ

```python
"""
æ™ºèƒ½è¿ç»´å†³ç­–æ”¯æŒç³»ç»Ÿ
file: monitoring/intelligent-ops.py
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import json

class DecisionType(Enum):
    """å†³ç­–ç±»å‹"""
    SCALING = "scaling"
    LOAD_BALANCING = "load_balancing"
    FAILOVER = "failover"
    RESOURCE_OPTIMIZATION = "resource_optimization"
    MAINTENANCE_SCHEDULING = "maintenance_scheduling"

class DecisionConfidence(Enum):
    """å†³ç­–ç½®ä¿¡åº¦"""
    LOW = 0.3
    MEDIUM = 0.6
    HIGH = 0.8
    VERY_HIGH = 0.9

@dataclass
class OperationalDecision:
    """è¿ç»´å†³ç­–"""
    decision_id: str
    decision_type: DecisionType
    confidence: DecisionConfidence
    description: str
    recommended_actions: List[str]
    expected_impact: Dict[str, Any]
    execution_priority: int
    execution_window: Optional[Dict[str, datetime]]
    prerequisites: List[str]
    rollback_plan: List[str]

class IntelligentOpsDecisionEngine:
    """æ™ºèƒ½è¿ç»´å†³ç­–å¼•æ“"""
    
    def __init__(self):
        self.decision_models = {}
        self.historical_decisions = []
        self.system_state_analyzer = SystemStateAnalyzer()
        self.impact_predictor = ImpactPredictor()
        
    async def analyze_and_recommend(self) -> List[OperationalDecision]:
        """åˆ†æç³»ç»ŸçŠ¶æ€å¹¶æ¨èè¿ç»´å†³ç­–"""
        
        # åˆ†æå½“å‰ç³»ç»ŸçŠ¶æ€
        system_state = await self.system_state_analyzer.analyze_current_state()
        
        # è¯†åˆ«éœ€è¦å†³ç­–çš„åœºæ™¯
        decision_scenarios = await self._identify_decision_scenarios(system_state)
        
        # ç”Ÿæˆè¿ç»´å»ºè®®
        recommendations = []
        for scenario in decision_scenarios:
            decision = await self._generate_decision_recommendation(scenario)
            if decision:
                recommendations.append(decision)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        recommendations.sort(key=lambda x: x.execution_priority, reverse=True)
        
        return recommendations
    
    async def _identify_decision_scenarios(self, 
                                         system_state: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«éœ€è¦å†³ç­–çš„åœºæ™¯"""
        
        scenarios = []
        
        # æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
        resource_usage = system_state.get("resource_usage", {})
        for resource_type, usage in resource_usage.items():
            if usage.get("utilization", 0) > 80:
                scenarios.append({
                    "type": "high_resource_usage",
                    "resource_type": resource_type,
                    "current_usage": usage,
                    "trend": usage.get("trend", "stable")
                })
        
        # æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
        performance_metrics = system_state.get("performance", {})
        for metric_name, metric_value in performance_metrics.items():
            baseline = await self._get_performance_baseline(metric_name)
            if baseline and abs(metric_value - baseline) > baseline * 0.2:
                scenarios.append({
                    "type": "performance_deviation",
                    "metric_name": metric_name,
                    "current_value": metric_value,
                    "baseline": baseline,
                    "deviation_ratio": abs(metric_value - baseline) / baseline
                })
        
        # æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
        service_health = system_state.get("service_health", {})
        for service_name, health_info in service_health.items():
            if health_info.get("status") != "healthy":
                scenarios.append({
                    "type": "service_health_issue",
                    "service_name": service_name,
                    "health_status": health_info.get("status"),
                    "error_details": health_info.get("errors", [])
                })
        
        return scenarios
    
    async def _generate_decision_recommendation(self, 
                                              scenario: Dict[str, Any]) -> Optional[OperationalDecision]:
        """ç”Ÿæˆå†³ç­–å»ºè®®"""
        
        scenario_type = scenario["type"]
        
        if scenario_type == "high_resource_usage":
            return await self._recommend_scaling_decision(scenario)
        elif scenario_type == "performance_deviation":
            return await self._recommend_optimization_decision(scenario)
        elif scenario_type == "service_health_issue":
            return await self._recommend_recovery_decision(scenario)
        
        return None
    
    async def _recommend_scaling_decision(self, 
                                        scenario: Dict[str, Any]) -> OperationalDecision:
        """æ¨èæ‰©å®¹å†³ç­–"""
        
        resource_type = scenario["resource_type"]
        current_usage = scenario["current_usage"]
        
        # åˆ†ææ‰©å®¹éœ€æ±‚
        scaling_analysis = await self._analyze_scaling_requirements(
            resource_type, current_usage
        )
        
        # è®¡ç®—å†³ç­–ç½®ä¿¡åº¦
        confidence = await self._calculate_scaling_confidence(scaling_analysis)
        
        # é¢„æµ‹æ‰©å®¹å½±å“
        expected_impact = await self.impact_predictor.predict_scaling_impact(
            resource_type, scaling_analysis["recommended_scale"]
        )
        
        decision = OperationalDecision(
            decision_id=f"scaling_{resource_type}_{int(datetime.now().timestamp())}",
            decision_type=DecisionType.SCALING,
            confidence=confidence,
            description=f"å»ºè®®å¯¹{resource_type}è¿›è¡Œæ‰©å®¹ï¼Œå½“å‰ä½¿ç”¨ç‡{current_usage['utilization']}%",
            recommended_actions=[
                f"å°†{resource_type}æ‰©å®¹è‡³{scaling_analysis['recommended_scale']}",
                "ç›‘æ§æ‰©å®¹åæ€§èƒ½å˜åŒ–",
                "éªŒè¯æ‰©å®¹æ•ˆæœ"
            ],
            expected_impact=expected_impact,
            execution_priority=self._calculate_priority(scenario, confidence),
            execution_window=self._determine_execution_window(resource_type),
            prerequisites=[
                "ç¡®è®¤èµ„æºé…é¢å……è¶³",
                "æ£€æŸ¥æ‰©å®¹å½±å“èŒƒå›´"
            ],
            rollback_plan=[
                "ç›‘æ§æ‰©å®¹æ•ˆæœ",
                "å¦‚æ•ˆæœä¸ä½³ï¼Œå›æ»šåˆ°åŸå§‹è§„æ¨¡",
                "åˆ†ææ‰©å®¹å¤±è´¥åŸå› "
            ]
        )
        
        return decision
    
    async def _analyze_scaling_requirements(self,
                                          resource_type: str,
                                          current_usage: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ‰©å®¹éœ€æ±‚"""
        
        current_scale = current_usage.get("current_instances", 1)
        utilization = current_usage.get("utilization", 0)
        trend = current_usage.get("trend", "stable")
        
        # åŸºäºä½¿ç”¨ç‡å’Œè¶‹åŠ¿è®¡ç®—å»ºè®®è§„æ¨¡
        if utilization > 90:
            scale_factor = 2.0  # é«˜ä½¿ç”¨ç‡æ—¶åŠ å€
        elif utilization > 80:
            scale_factor = 1.5  # ä¸­ç­‰ä½¿ç”¨ç‡æ—¶å¢åŠ 50%
        else:
            scale_factor = 1.2  # ä½ä½¿ç”¨ç‡æ—¶å¢åŠ 20%
        
        # è€ƒè™‘è¶‹åŠ¿å½±å“
        if trend == "increasing":
            scale_factor *= 1.2
        elif trend == "decreasing":
            scale_factor *= 0.9
        
        recommended_scale = max(int(current_scale * scale_factor), current_scale + 1)
        
        return {
            "current_scale": current_scale,
            "recommended_scale": recommended_scale,
            "scale_factor": scale_factor,
            "reasoning": f"åŸºäº{utilization}%ä½¿ç”¨ç‡å’Œ{trend}è¶‹åŠ¿"
        }

class SystemStateAnalyzer:
    """ç³»ç»ŸçŠ¶æ€åˆ†æå™¨"""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        
    async def analyze_current_state(self) -> Dict[str, Any]:
        """åˆ†æå½“å‰ç³»ç»ŸçŠ¶æ€"""
        
        # æ”¶é›†å„ç±»æŒ‡æ ‡
        infrastructure_metrics = await self.metrics_collector.collect_infrastructure_metrics()
        application_metrics = await self.metrics_collector.collect_application_metrics()
        service_health = await self.metrics_collector.collect_service_health()
        
        # åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ
        resource_usage = await self._analyze_resource_usage(infrastructure_metrics)
        
        # åˆ†ææ€§èƒ½è¡¨ç°
        performance_analysis = await self._analyze_performance(application_metrics)
        
        # åˆ†ææœåŠ¡å¥åº·çŠ¶æ€
        health_analysis = await self._analyze_service_health(service_health)
        
        return {
            "timestamp": datetime.now(),
            "resource_usage": resource_usage,
            "performance": performance_analysis,
            "service_health": health_analysis,
            "overall_health_score": await self._calculate_overall_health_score(
                resource_usage, performance_analysis, health_analysis
            )
        }
    
    async def _analyze_resource_usage(self, 
                                    metrics: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ"""
        
        usage_analysis = {}
        
        # CPUä½¿ç”¨åˆ†æ
        cpu_metrics = metrics.get("cpu", {})
        if cpu_metrics:
            usage_analysis["cpu"] = {
                "utilization": cpu_metrics.get("utilization", 0),
                "trend": await self._analyze_trend(cpu_metrics.get("history", [])),
                "current_instances": cpu_metrics.get("instances", 1),
                "load_average": cpu_metrics.get("load_average", 0)
            }
        
        # å†…å­˜ä½¿ç”¨åˆ†æ
        memory_metrics = metrics.get("memory", {})
        if memory_metrics:
            usage_analysis["memory"] = {
                "utilization": memory_metrics.get("utilization", 0),
                "trend": await self._analyze_trend(memory_metrics.get("history", [])),
                "available_gb": memory_metrics.get("available_gb", 0),
                "swap_usage": memory_metrics.get("swap_usage", 0)
            }
        
        # GPUä½¿ç”¨åˆ†æ
        gpu_metrics = metrics.get("gpu", {})
        if gpu_metrics:
            usage_analysis["gpu"] = {
                "utilization": gpu_metrics.get("utilization", 0),
                "memory_utilization": gpu_metrics.get("memory_utilization", 0),
                "temperature": gpu_metrics.get("temperature", 0),
                "power_usage": gpu_metrics.get("power_usage", 0)
            }
        
        return usage_analysis
    
    async def _analyze_trend(self, history: List[float]) -> str:
        """åˆ†æè¶‹åŠ¿"""
        
        if len(history) < 5:
            return "insufficient_data"
        
        # è®¡ç®—çº¿æ€§å›å½’æ–œç‡
        x = list(range(len(history)))
        y = history
        
        n = len(history)
        x_mean = sum(x) / n
        y_mean = sum(y) / n
        
        slope = sum((x[i] - x_mean) * (y[i] - y_mean) for i in range(n)) / \
                sum((x[i] - x_mean) ** 2 for i in range(n))
        
        if slope > 0.1:
            return "increasing"
        elif slope < -0.1:
            return "decreasing"
        else:
            return "stable"

## æœ¬èŠ‚æ€»ç»“

æœ¬èŠ‚æ·±å…¥ä»‹ç»äº†æ™ºèƒ½ç¯å¢ƒå±‚çš„ç›‘æ§ä¸æ²»ç†ä½“ç³»ï¼š

### ğŸ¯ æ ¸å¿ƒç›‘æ§èƒ½åŠ›

1. **å…¨æ–¹ä½ç›‘æ§ä½“ç³»**ï¼š
   - åŸºç¡€è®¾æ–½ã€åº”ç”¨ã€ä¸šåŠ¡ã€å®‰å…¨å››ä¸ªç»´åº¦
   - æŒ‡æ ‡ã€æ—¥å¿—ã€é“¾è·¯ã€äº‹ä»¶å››ç§æ•°æ®ç±»å‹
   - å®æ—¶ç›‘æ§å’Œå†å²åˆ†æç›¸ç»“åˆ

2. **æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ**ï¼š
   - å¤šç»´åº¦å‘Šè­¦è§„åˆ™å¼•æ“
   - æ™ºèƒ½é™å™ªå’Œå…³è”åˆ†æ
   - é¢„æµ‹æ€§å‘Šè­¦å’Œè‡ªåŠ¨åŒ–å“åº”

3. **æ€§èƒ½åˆ†æä¼˜åŒ–**ï¼š
   - åŸºçº¿å»ºç«‹å’Œè¶‹åŠ¿åˆ†æ
   - å¼‚å¸¸æ£€æµ‹å’Œç“¶é¢ˆè¯†åˆ«
   - å®¹é‡è§„åˆ’å’Œä¼˜åŒ–å»ºè®®

### ğŸ”§ å…³é”®æŠ€æœ¯å®ç°

- **æ™ºèƒ½æŒ‡æ ‡ä½“ç³»**ï¼šæ¶µç›–åŸºç¡€è®¾æ–½åˆ°ä¸šåŠ¡çš„å…¨æ ˆç›‘æ§æŒ‡æ ‡
- **æœºå™¨å­¦ä¹ åˆ†æ**ï¼šåŸºäºMLçš„å¼‚å¸¸æ£€æµ‹å’Œæ€§èƒ½é¢„æµ‹
- **å¯è§†åŒ–å¹³å°**ï¼šGrafanaä»ªè¡¨æ¿å’Œå®šåˆ¶åŒ–ç›‘æ§ç•Œé¢
- **å†³ç­–æ”¯æŒç³»ç»Ÿ**ï¼šåŸºäºæ•°æ®é©±åŠ¨çš„æ™ºèƒ½è¿ç»´å†³ç­–

### ğŸš€ æ™ºèƒ½åŒ–ç‰¹æ€§

- **è‡ªæ„ŸçŸ¥èƒ½åŠ›**ï¼šå®æ—¶æ„ŸçŸ¥ç³»ç»ŸçŠ¶æ€å’Œæ€§èƒ½å˜åŒ–
- **è‡ªè¯Šæ–­èƒ½åŠ›**ï¼šè‡ªåŠ¨è¯†åˆ«é—®é¢˜å’Œåˆ†ææ ¹æœ¬åŸå› 
- **è‡ªä¼˜åŒ–èƒ½åŠ›**ï¼šåŸºäºåˆ†æç»“æœè‡ªåŠ¨ä¼˜åŒ–ç³»ç»Ÿé…ç½®
- **é¢„æµ‹æ€§ç»´æŠ¤**ï¼šæå‰é¢„æµ‹æ•…éšœå’Œæ€§èƒ½é—®é¢˜

### ğŸ“Š æ²»ç†ä»·å€¼

- **æå‡è¿ç»´æ•ˆç‡**ï¼šå‡å°‘äººå·¥ç›‘æ§å’Œå¤„ç†æ—¶é—´
- **æé«˜ç³»ç»Ÿå¯é æ€§**ï¼šåŠæ—¶å‘ç°å’Œå¤„ç†é—®é¢˜
- **ä¼˜åŒ–èµ„æºåˆ©ç”¨**ï¼šåŸºäºæ•°æ®é©±åŠ¨çš„èµ„æºä¼˜åŒ–
- **æ”¯æŒå†³ç­–åˆ¶å®š**ï¼šä¸ºè¿ç»´å†³ç­–æä¾›ç§‘å­¦ä¾æ®

---

**ä¸‹ä¸€æ­¥å­¦ä¹ **ï¼šå®Œæˆäº†ç›‘æ§ä¸æ²»ç†çš„å­¦ä¹ åï¼Œæˆ‘ä»¬å°†å­¦ä¹ ä¼ä¸šçº§æ™ºèƒ½ç¯å¢ƒçš„éƒ¨ç½²ä¸è¿ç»´ï¼Œäº†è§£å¦‚ä½•åœ¨å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒä¸­å®æ–½æ™ºèƒ½ç¯å¢ƒå±‚ã€‚

> **ğŸ’¡ ç›‘æ§æ²»ç†è¦è¯€**ï¼šæœ‰æ•ˆçš„ç›‘æ§ä¸ä»…è¦å…¨é¢è¦†ç›–ï¼Œæ›´è¦æ™ºèƒ½åˆ†æã€‚é€šè¿‡æœºå™¨å­¦ä¹ å’Œæ•°æ®åˆ†æï¼Œå°†è¢«åŠ¨çš„ç›‘æ§è½¬å˜ä¸ºä¸»åŠ¨çš„æ´å¯Ÿï¼Œå°†å“åº”å¼è¿ç»´å‡çº§ä¸ºé¢„æµ‹æ€§ç»´æŠ¤ã€‚
