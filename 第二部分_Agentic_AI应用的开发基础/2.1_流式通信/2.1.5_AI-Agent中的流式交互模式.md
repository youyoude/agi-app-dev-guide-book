# 2.1.5 AI Agentä¸­çš„æµå¼äº¤äº’æ¨¡å¼

**å­¦ä¹ ç›®æ ‡ï¼š** ç†è§£AI Agentåœºæ™¯ä¸‹çš„ç‰¹æ®Šæµå¼äº¤äº’éœ€æ±‚ï¼ŒæŒæ¡æ™ºèƒ½åŒ–çš„æµå¼é€šä¿¡æ¨¡å¼

## å•æ™ºèƒ½ä½“æµå¼é€šä¿¡æœåŠ¡ç«¯å®ç°

åœ¨å­¦ä¹ å®Œæµå¼é€šä¿¡åŸºç¡€åï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªAgentic AIåº”ç”¨ä¸­çš„å•æ™ºèƒ½ä½“æ¡ˆä¾‹ï¼Œå­¦ä¹ å¦‚ä½•æ„å»ºå®Œæ•´çš„å…·æœ‰AI AgentæœåŠ¡èƒ½åŠ›çš„æµå¼é€šä¿¡æœåŠ¡ç«¯ã€‚
æœ¬æ–‡ä»¥Python AI Agentä½œä¸ºä¾‹å­ã€‚

## 1 Pythonç«¯æµå¼é€šä¿¡å®ç°
``` Python
# Please install OpenAI SDK first: `pip3 install openai`
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.environ.get('DEEPSEEK_API_KEY'),
    base_url="https://api.deepseek.com")

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"},
    ],
    stream=True
)

# æµå¼æ¶ˆè´¹å“åº”
for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```
è¿™æ®µä»£ç å±•ç¤ºäº†ä¸€ä¸ªæœ€ç®€å•çš„LLMé€šä¿¡ç¤ºä¾‹ï¼Œè¿™æ˜¯åˆ©ç”¨OpenAI SDKè°ƒç”¨LLMçš„æœ€å°å®ç°ã€‚è™½ç„¶ä»£ç ä¸­è®¾ç½®äº†`stream=True`å‚æ•°å¼€å¯äº†æµå¼å“åº”ï¼Œä½†è¿™ä¸ªåŸºç¡€Demoä½œä¸ºLLMçš„ç›´æ¥å®¢æˆ·ç«¯ï¼Œå¹¶ä¸å…·å¤‡æµå¼æ¶ˆæ¯è½¬å‘çš„èƒ½åŠ›ã€‚åœ¨å®é™…çš„AIåº”ç”¨å¼€å‘ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦åŸºäºæ­¤ç±»LLMå®¢æˆ·ç«¯æ„å»ºä¸€ä¸ªæµå¼æ¶ˆæ¯è½¬å‘æœåŠ¡ï¼Œä»¥å®ç°ä»LLMåˆ°å‰ç«¯ã€åç«¯çš„å®Œæ•´æµå¼é€šä¿¡é“¾è·¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€šè¿‡äº¬ä¸œçš„ä¸€ä¸ªå¼€æºé¡¹ç›®æ¥å­¦ä¹ å¦‚ä½•åœ¨æœåŠ¡ç«¯å®ç°è¿™æ ·çš„æµå¼è½¬å‘æœåŠ¡ã€‚
æœ¬æ–‡ä»¥äº¬ä¸œå¼€æºçš„JDGenieé¡¹ç›®ä¸­çš„genie-toolä¸ºä¾‹ï¼Œæ·±å…¥è§£æå¦‚ä½•åŸºäº**FastAPI + sse_starlette + litellm + SmoLAgents**æ„å»ºä¸€ä¸ªå®Œæ•´çš„å•æ™ºèƒ½ä½“æµå¼é€šä¿¡æœåŠ¡ç«¯ã€‚

## genie-toolæ ¸å¿ƒæŠ€æœ¯æ ˆåˆ†æ
### 1. FastAPIï¼šç°ä»£åŒ–çš„é«˜æ€§èƒ½ Python Web æ¡†æ¶
FastAPIä½œä¸ºæœåŠ¡ç«¯æ¡†æ¶ï¼Œæ ¸å¿ƒç‰¹æ€§ä¹‹ä¸€æ˜¯å¯¹å¼‚æ­¥ç¼–ç¨‹çš„åŸç”Ÿæ”¯æŒã€‚é€šè¿‡ä½¿ç”¨asyncå’Œawaitï¼ŒFastAPIèƒ½å¤Ÿé«˜æ•ˆå¤„ç† I/O å¯†é›†å‹ä»»åŠ¡ï¼Œå¦‚æ•°æ®åº“æŸ¥è¯¢ã€API è°ƒç”¨å’Œæ–‡ä»¶æ“ä½œã€‚


### 2. sse_starletteï¼šæä¾›æ ‡å‡†çš„Server-Sent Eventsåè®®å®ç°

sse_starlette æ˜¯ä¸€ä¸ªPythonåº“ï¼Œä¸º Starlette å’Œ FastAPI æ¡†æ¶æä¾›å¯¹ Server-Sent Eventsçš„æ”¯æŒã€‚å®ƒé€šè¿‡ EventSourceResponse ç±»å®ç°SSEåè®®ï¼Œå…è®¸æœåŠ¡å™¨å¼‚æ­¥å‘å®¢æˆ·ç«¯æ¨é€å®æ—¶æ•°æ®ï¼Œé€‚åˆæ„å»ºå®æ—¶Webåº”ç”¨ï¼Œå¦‚é€šçŸ¥ç³»ç»Ÿã€å®æ—¶ä»ªè¡¨ç›˜æˆ–æµå¼æ•°æ®æ›´æ–°ã€‚sse_starletteè½»é‡ä¸”ä¸ASGIæ¡†æ¶æ— ç¼é›†æˆï¼Œå¸¸ç”¨äºéœ€è¦é«˜æ•ˆå•å‘é€šä¿¡çš„åœºæ™¯ã€‚


**sse_starlette åº“çš„ä½œç”¨**
SSE æ”¯æŒï¼šä¸ºFastAPI æä¾›æ ‡å‡†åŒ–çš„ SSE å®ç°ï¼ŒåŸºäº HTML5 EventSource API
å®æ—¶æ•°æ®æ¨é€ï¼šæ”¯æŒæœåŠ¡å™¨åˆ°å®¢æˆ·ç«¯çš„å•å‘æ•°æ®æµï¼Œé€‚åˆå®æ—¶æ›´æ–°ã€‚
æ¶ˆæ¯æ ¼å¼åŒ–ï¼š è‡ªåŠ¨å°†Pythonå¯¹è±¡è½¬æ¢ä¸ºSSEæ ‡å‡†æ ¼å¼
é«˜æ€§èƒ½ï¼šåˆ©ç”¨ asyncio å’Œ anyio çš„å¼‚æ­¥ä»»åŠ¡ç»„ï¼ˆTaskGroupsï¼‰ï¼Œç¡®ä¿é«˜æ•ˆå¹¶å‘
æ˜“äºé›†æˆï¼šä¸FastAPIæ— ç¼å…¼å®¹ï¼Œæ”¯æŒè‡ªå®šä¹‰äº‹ä»¶å’Œå¤´éƒ¨
è¿æ¥ç®¡ç†ï¼š å¤„ç†HTTPè¿æ¥ã€å¿ƒè·³ã€é”™è¯¯æ¢å¤ç­‰


### 3. LiteLLMï¼šç»Ÿä¸€LLMå®¢æˆ·ç«¯æ¥å£

**ä¸åŒå‚å•†æ¥å£å·®å¼‚ç¤ºä¾‹ï¼š**

ä¸åŒLLMæä¾›å•†çš„APIæ¥å£åœ¨è¿”å›æ ¼å¼ã€å­—æ®µå‘½åã€æ•°æ®ç»“æ„ç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè¿™ç»™å¤šæ¨¡å‹é›†æˆå¸¦æ¥äº†æŒ‘æˆ˜ï¼š

1. **æµå¼å“åº”æ ¼å¼å·®å¼‚**
   - **OpenAI**: ä½¿ç”¨SSEæ ¼å¼ï¼Œæ•°æ®å­—æ®µä¸º`data: {"choices": [{"delta": {"content": "..."}}]}`
   - **Claude (Anthropic)**: ä½¿ç”¨ä¸åŒçš„äº‹ä»¶ç±»å‹ï¼Œå¦‚`content_block_delta`ã€`message_delta`ç­‰
   - **åƒé—®/æ–‡å¿ƒä¸€è¨€**: è¿”å›æ ¼å¼ä¸OpenAIç±»ä¼¼ä½†å­—æ®µåç§°å¯èƒ½ä¸åŒï¼Œå¦‚ä½¿ç”¨`result`æ›¿ä»£`choices`

2. **è®¤è¯æ–¹å¼å·®å¼‚**
   - **OpenAI**: ä½¿ç”¨`Authorization: Bearer <token>`å¤´éƒ¨è®¤è¯
   - **Claude**: ä½¿ç”¨`x-api-key`å¤´éƒ¨å­—æ®µ
   - **å›½å†…å‚å•†**: å¯èƒ½ä½¿ç”¨API Key + Secretç­¾åæœºåˆ¶

3. **å‚æ•°å‘½åå·®å¼‚**
   - **æ¸©åº¦å‚æ•°**: æœ‰çš„å«`temperature`ï¼Œæœ‰çš„å«`temp`
   - **æœ€å¤§tokenæ•°**: `max_tokens` vs `max_output_tokens` vs `max_new_tokens`
   - **æµå¼å¼€å…³**: `stream` vs `incremental_output`

4. **é”™è¯¯å“åº”æ ¼å¼å·®å¼‚**
   - **OpenAI**: `{"error": {"message": "...", "type": "...", "code": "..."}}`
   - **å…¶ä»–å‚å•†**: å¯èƒ½ä½¿ç”¨`error_msg`ã€`err_msg`æˆ–å®Œå…¨ä¸åŒçš„ç»“æ„

ä¸ºäº†è§£å†³ä¸Šè¿°å‚å•†å·®å¼‚é—®é¢˜ï¼Œå¼€æºç¤¾åŒºè¯ç”Ÿäº†`litellm`è¿™ç±»ç»Ÿä¸€æ¥å£å±‚å·¥å…·ã€‚

`litellm`æ˜¯ä¸€ä¸ªå¼ºå¤§çš„Pythonåº“ï¼Œå®ƒå±è”½äº†ä¸åŒLLMæä¾›å•†çš„APIå·®å¼‚ï¼Œè®©å¼€å‘è€…å¯ä»¥ä½¿ç”¨ç»Ÿä¸€çš„OpenAIæ ¼å¼è°ƒç”¨100+ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬OpenAIã€Claudeã€DeepSeekã€Geminiï¼Œä»¥åŠBedrockã€VertexAIã€Azureç­‰äº‘å¹³å°ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**

| åŠŸèƒ½åˆ†ç±» | è¯´æ˜ |
|---------|------|
| **è¾“å…¥é€‚é…** | è‡ªåŠ¨å°†è¯·æ±‚è½¬æ¢ä¸ºå„æä¾›å•†çš„completionã€embeddingã€image_generationç«¯ç‚¹æ ¼å¼ |
| **è¾“å‡ºæ ‡å‡†åŒ–** | å“åº”ç»Ÿä¸€é€šè¿‡`['choices'][0]['message']['content']`è·å–ï¼Œæ— è®ºåº•å±‚ä½¿ç”¨å“ªä¸ªæ¨¡å‹ |
| **å‚æ•°æ˜ å°„** | è‡ªåŠ¨å¤„ç†ä¸åŒæ¨¡å‹é—´çš„å‚æ•°å‘½åå·®å¼‚ï¼ˆå¦‚`max_tokens` vs `max_output_tokens`ï¼‰ |
| **é”™è¯¯ç»Ÿä¸€** | å°†å„å‚å•†çš„é”™è¯¯å“åº”æ ‡å‡†åŒ–ä¸ºç»Ÿä¸€æ ¼å¼ |
| **è·¯ç”±é™çº§** | æ”¯æŒè·¨éƒ¨ç½²ï¼ˆå¦‚Azure/OpenAIï¼‰çš„æ•…éšœè½¬ç§»å’Œé‡è¯•é€»è¾‘ |
| **é¢„ç®—é™æµ** | å¯æŒ‰é¡¹ç›®ã€API Keyã€æ¨¡å‹ç»´åº¦è®¾ç½®é¢„ç®—å’Œé€Ÿç‡é™åˆ¶ |

**æŠ€æœ¯ä¼˜åŠ¿ï¼š**
- **å¼€å‘æ•ˆç‡**ï¼šä¸€å¥—ä»£ç æ”¯æŒå¤šå‚å•†ï¼Œé¿å…é‡å¤é€‚é…å·¥ä½œ
- **åˆ‡æ¢çµæ´»**ï¼šé€šè¿‡é…ç½®å³å¯åˆ‡æ¢æ¨¡å‹ï¼Œæ— éœ€ä¿®æ”¹ä¸šåŠ¡ä»£ç 
- **è¿ç§»æˆæœ¬ä½**ï¼šå‚å•†è¿ç§»åªéœ€ä¿®æ”¹é…ç½®ï¼Œä»£ç æ— æ„ŸçŸ¥
- **ç»Ÿä¸€è¿ç»´**ï¼šä¾¿äºå®ç°ç»Ÿä¸€çš„ç›‘æ§ã€æ—¥å¿—å’Œæˆæœ¬ç®¡ç†


### 4. SmoLAgentsï¼šä¸“ä¸šçš„Agentæ¨ç†æ¡†æ¶

SmoLAgentsæ˜¯Hugging Faceäº2024å¹´åº•å¼€æºçš„è½»é‡çº§Agentæ¡†æ¶ï¼Œå…¶è®¾è®¡ç†å¿µæ˜¯"**ç®€æ´è€Œä¸ç®€å•**"â€”â€”ç”¨æœ€å°‘çš„ä»£ç å®ç°æœ€å®Œæ•´çš„æ™ºèƒ½ä½“èƒ½åŠ›ã€‚ç›¸æ¯”LangChainç­‰é‡é‡çº§æ¡†æ¶ï¼ŒSmoLAgentsçš„æ ¸å¿ƒä»£ç ä»…çº¦1000è¡Œï¼Œå´æä¾›äº†æ„å»ºç”Ÿäº§çº§Agentæ‰€éœ€çš„å…¨éƒ¨æ ¸å¿ƒåŠŸèƒ½ã€‚

**æ¡†æ¶å®šä½ä¸è®¾è®¡å“²å­¦ï¼š**
- **æç®€ä¸»ä¹‰**ï¼šéµå¾ª"å°‘å³æ˜¯å¤š"çš„è®¾è®¡åŸåˆ™ï¼Œé¿å…è¿‡åº¦æŠ½è±¡ï¼Œè®©å¼€å‘è€…èƒ½å¿«é€Ÿç†è§£å’ŒæŒæ§æ•´ä¸ªæ¡†æ¶
- **ä»£ç ä¼˜å…ˆï¼ˆCode-Firstï¼‰**ï¼šæ¨å´‡é€šè¿‡ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œæ¥è§£å†³é—®é¢˜ï¼Œè€Œéçº¯ç²¹çš„æç¤ºè¯å·¥ç¨‹
- **é€æ˜å¯è°ƒè¯•**ï¼šæ¯ä¸ªæ¨ç†æ­¥éª¤éƒ½æ¸…æ™°å¯è§ï¼Œä¾¿äºå¼€å‘è€…ç†è§£Agentçš„å†³ç­–è¿‡ç¨‹

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- **åŒæ¨¡å¼Agentæ¶æ„**ï¼š
  - `CodeAgent`ï¼šé€šè¿‡ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç æ¥å®Œæˆä»»åŠ¡ï¼Œé€‚åˆæ•°æ®å¤„ç†ã€è®¡ç®—ç­‰åœºæ™¯
  - `ToolCallingAgent`ï¼šé€šè¿‡JSONæ ¼å¼çš„å·¥å…·è°ƒç”¨å®Œæˆä»»åŠ¡ï¼Œå…¼å®¹OpenAI Function Callingè§„èŒƒ
- **ReActæ¨ç†æ¨¡å¼**ï¼šå®ç°"æ€è€ƒ(Thought)-è¡ŒåŠ¨(Action)-è§‚å¯Ÿ(Observation)"çš„å®Œæ•´æ¨ç†å¾ªç¯ï¼Œæ”¯æŒå¤šè½®è¿­ä»£ç›´è‡³ä»»åŠ¡å®Œæˆ
- **å†…ç½®å®‰å…¨æ²™ç®±**ï¼šä»£ç æ‰§è¡Œåœ¨éš”ç¦»ç¯å¢ƒä¸­è¿›è¡Œï¼Œæ”¯æŒæœ¬åœ°æ²™ç®±å’Œè¿œç¨‹E2Bæ‰§è¡Œå™¨
- **æµå¼è¾“å‡ºæ”¯æŒ**ï¼šåŸç”Ÿæ”¯æŒæµå¼è¾“å‡ºå’Œæ­¥éª¤çº§æ§åˆ¶ï¼Œå¯å®æ—¶è¿½è¸ªAgentçš„æ€è€ƒå’Œæ‰§è¡Œè¿‡ç¨‹
- **ä¸°å¯Œçš„å·¥å…·ç”Ÿæ€**ï¼šå†…ç½®ç½‘é¡µæœç´¢ã€Pythonè§£é‡Šå™¨ã€æ–‡ä»¶æ“ä½œç­‰å¸¸ç”¨å·¥å…·ï¼ŒåŒæ—¶æ”¯æŒå¿«é€Ÿè‡ªå®šä¹‰å·¥å…·

**åœ¨æœ¬æ¡ˆä¾‹ä¸­çš„ä½œç”¨ï¼š**
JDGenieé¡¹ç›®åŸºäºSmoLAgentsæ„å»ºäº†ä»£ç è§£é‡Šå™¨æ™ºèƒ½ä½“ï¼Œåˆ©ç”¨å…¶ReActæ¡†æ¶å®ç°äº†ï¼š
- è‡ªç„¶è¯­è¨€ä»»åŠ¡åˆ†æ
- ä»£ç ç”Ÿæˆä¸æ‰§è¡Œ
- ç»“æœéªŒè¯ä¸è¿­ä»£ä¼˜åŒ–
- å®Œæ•´çš„æµå¼äº¤äº’ä½“éªŒ



genie-toolæœåŠ¡æä¾›äº†å¤šä¸ªAPIæ¥å£ï¼Œå…¶ä¸­ `/code_interpreter` æ¥å£æ˜¯æ•´ä¸ªé¡¹ç›®çš„æ ¸å¿ƒAIèƒ½åŠ›å…¥å£ã€‚è¯¥æ¥å£å®ç°äº†**"è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ™ºèƒ½ä»£ç ç”Ÿæˆä¸æ‰§è¡Œ"**â€”â€”ç”¨æˆ·åªéœ€ç”¨è‡ªç„¶è¯­è¨€æè¿°æ•°æ®åˆ†æéœ€æ±‚ï¼ŒAIä¾¿èƒ½è‡ªåŠ¨ç”Ÿæˆå¹¶æ‰§è¡Œç›¸åº”çš„Pythonä»£ç ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä»¥è¿™ä¸ªæ¥å£ä¸ºä¾‹ï¼Œæ·±å…¥å‰–æå…¶å®Œæ•´çš„æµå¼æ¶æ„å®ç°ã€‚




##  å®Œæ•´genie-toolæ¥å£code_interpreteræ—¶åºå›¾

ä¸‹å›¾å±•ç¤ºäº† `/code_interpreter` æ¥å£ä»è¯·æ±‚å‘èµ·åˆ°æµå¼å“åº”è¿”å›çš„å®Œæ•´è°ƒç”¨é“¾è·¯ã€‚æ•´ä¸ªæµç¨‹æ¶‰åŠ5ä¸ªæ ¸å¿ƒå±‚çº§ï¼šJavaåç«¯å‘èµ·HTTPè¯·æ±‚ â†’ FastAPIè·¯ç”±å±‚æ¥æ”¶å¹¶å¤„ç† â†’ ä¸šåŠ¡é€»è¾‘å±‚åˆ›å»ºæ™ºèƒ½ä½“ â†’ CIAgentæ‰§è¡ŒReActæ¨ç† â†’ LiteLLMæ™ºèƒ½è·¯ç”±åˆ°å¯¹åº”å‚å•†APIã€‚æµå¼å“åº”åˆ™æ²¿ç›¸åæ–¹å‘é€å±‚ä¼ é€’ï¼Œæœ€ç»ˆä»¥SSEæ ¼å¼æ¨é€ç»™è°ƒç”¨æ–¹ã€‚

```mermaid
sequenceDiagram
    participant Java as Javaåç«¯
    participant API as FastAPIè·¯ç”±
    participant CodeInterpreter as code_interpreter_agent <br /> ä¸šåŠ¡é€»è¾‘
    participant CIAgent as CIAgent  <br />ï¼ˆæ™ºèƒ½ä½“ç±»ï¼‰
    participant LiteLLM as LiteLLMç»Ÿä¸€æ¥å£ <br />(completionå‡½æ•°+å†…ç½®è·¯ç”±)
    participant Provider as å‚å•†API <br />(OpenAI/Claudeç­‰)

    Java->>+API: POST /code_interpreter
    API->>+CodeInterpreter: code_interpreter_agent()
    CodeInterpreter->>+CIAgent: create_ci_agent() + agent.run()
    CIAgent->>+LiteLLM: model.generate_stream()
    
    Note over LiteLLM: LiteLLMå†…éƒ¨è·¯ç”±åˆ¤æ–­
    LiteLLM->>LiteLLM: if model in open_ai_chat_completion_models â†’ OpenAIåˆ†æ”¯
    LiteLLM->>LiteLLM: elif model in anthropic_models â†’ Claudeåˆ†æ”¯  
    LiteLLM->>LiteLLM: elif model in cohere_models â†’ Cohereåˆ†æ”¯
    LiteLLM->>LiteLLM: elif "replicate" in model â†’ Replicateåˆ†æ”¯
    LiteLLM->>+Provider: è°ƒç”¨å¯¹åº”å‚å•†API
    
    loop æµå¼å¤„ç†å¾ªç¯
        Provider-->>LiteLLM: å‚å•†åŸå§‹æµå¼å“åº” <br />(æ ¼å¼å„å¼‚)
        LiteLLM-->>LiteLLM: ç»Ÿä¸€è½¬æ¢ä¸ºOpenAIæ ¼å¼ <br />choices[0].delta.content
        LiteLLM-->>CIAgent: åŸå§‹æµå¼å“åº” <br />(ç»Ÿä¸€æ ¼å¼)
        CIAgent-->>CIAgent: è½¬æ¢ä¸ºChatMessageStreamDelta <br />å¹¶yield event
        CIAgent-->>CIAgent: _step_stream()èšåˆ+è§£æ+æ‰§è¡Œ
        CIAgent-->>CodeInterpreter: CodeOutput/ActionOutput <br />(smolagentsæ ‡å‡†)
        CodeInterpreter-->>CodeInterpreter: æ–‡ä»¶ä¸Šä¼ å¤„ç† <br />å¢å¼ºè¾“å‡ºå¯¹è±¡
        CodeInterpreter-->>API: CodeOutput/ActionOutput <br /> å¢å¼ºåçš„è¾“å‡ºå¯¹è±¡
        API-->>API: å®ç°å¤šç§æ¨é€æ¨¡å¼+æ ¼å¼åŒ–
        API-->>Java: ServerSentEvent
    end
```
## ğŸ“‹ æœåŠ¡åˆ†å±‚æ¶æ„é€Ÿè§ˆ

| å±‚çº§ | æ ¸å¿ƒæ¨¡å— | ä¸»è¦èŒè´£ | è¾“å…¥ç±»å‹ | è¾“å‡ºç±»å‹ |
|------|------|----------|----------|----------|
| **ç¬¬1å±‚** | FastAPI + sse_starlette  | HTTPè¯·æ±‚å¤„ç†ã€SSEå“åº” | CIRequest | ServerSentEvent |
| **ç¬¬2å±‚** | code_interpreter_agent.py | ä¸šåŠ¡é€»è¾‘å¤„ç†ã€ä»»åŠ¡åè°ƒã€æ–‡ä»¶å¤„ç† ã€åˆ›å»ºæ™ºèƒ½ä½“| ä»»åŠ¡å‚æ•° |  è‡ªå®šä¹‰çš„å¢å¼ºå¯¹è±¡CodeOutput (åŒ…å«ä»£ç æ–‡ä»¶ä¸‹è½½é“¾æ¥)/ActionOutput (åŒ…å«æ‰§è¡Œç»“æœ + æ‰€æœ‰ç”Ÿæˆæ–‡ä»¶çš„ä¸‹è½½é“¾æ¥) |
| **ç¬¬3å±‚** | CIAgent | æ™ºèƒ½ä½“è¿è¡Œã€ä»£ç ç”Ÿæˆã€ä»£ç æ‰§è¡Œ | ä»»åŠ¡å‚æ•°ã€æç¤ºè¯ | ä½¿ç”¨smolagentsæ ‡å‡†å¯¹è±¡ CodeOutput/ActionOutput |
| **ç¬¬4å±‚**  | LiteLLM| **ç»Ÿä¸€å…¥å£å‡½æ•°**ï¼šæ¥æ”¶æ ‡å‡†åŒ–å‚æ•°<br/>**æ™ºèƒ½è·¯ç”±**ï¼šæ ¹æ®modelå‚æ•°åˆ¤æ–­å‚å•†ï¼Œè°ƒç”¨å¯¹åº”å‚å•†çš„æ¥å£ï¼Œ **æ ¼å¼è½¬æ¢**ï¼šç»Ÿä¸€APIå“åº”æ ¼å¼<br/>â€¢ OpenAI: ç›´æ¥è¿”å›<br/>â€¢ Claude: `delta.text` â†’ `choices[0].delta.content`<br/>â€¢ å…¶ä»–å‚å•†æ ¼å¼æ ‡å‡†åŒ– | æ¶ˆæ¯æ•°ç»„ + æ¨¡å‹å‚æ•° | ç»Ÿä¸€æ ¼å¼çš„æµå¼JSON  |
| **ç¬¬5å±‚** | å‚å•†API | æä¾›åŸå§‹LLMèƒ½åŠ›ï¼šOpenAIã€Anthropicã€Cohereã€Replicateç­‰ |æ¶ˆæ¯æ•°ç»„ï¼ˆæç¤ºè¯ï¼‰ã€å‚å•†ç‰¹å®šå‚æ•°| æµå¼JSON ï¼ˆå„å‚å•†åŸå§‹æ ¼å¼ï¼‰|



## æœåŠ¡åˆ†å±‚è¯¦è§£

æœ¬èŠ‚é€šè¿‡ç®€åŒ–ä»£ç ç‰‡æ®µï¼Œæ·±å…¥åˆ†æ `code_interpreter` æ¥å£çš„5å±‚æµå¼æ¶æ„å®ç°æœºåˆ¶ï¼š

### ç¬¬1å±‚ï¼šFastAPIè·¯ç”±
```python
# genie-tool/api/tool.py
@router.post("/code_interpreter") 
async def post_code_interpreter(body: CIRequest):
    # æ–‡ä»¶è·¯å¾„é¢„å¤„ç†
    if body.file_names:
        # æ ‡å‡†åŒ–æ–‡ä»¶è·¯å¾„ä¸ºå®Œæ•´URL
        
    async def _stream():
        # æµå¼æ¨é€æ¨¡å¼æ§åˆ¶å˜é‡
        acc_content, acc_token, acc_time = "", 0, time.time()
        
        async for chunk in code_interpreter_agent(  # è°ƒç”¨ç¬¬2å±‚
            task=body.task,
            file_names=body.file_names, 
            request_id=body.request_id,
            stream=True  # æµå¼å¼€å…³
        ):
            # æ ¹æ®chunkç±»å‹åˆ†åˆ«å¤„ç†
            if isinstance(chunk, CodeOuput):  # ä»£ç ç”Ÿæˆ
                yield ServerSentEvent(data={code, fileInfo, isFinal=False})
            elif isinstance(chunk, ActionOutput):  # æ‰§è¡Œç»“æœ  
                yield ServerSentEvent(data={codeOutput, fileInfo, isFinal=True})
                yield ServerSentEvent(data="[DONE]")
            else:  # æ–‡æœ¬æµ
                # æ”¯æŒå¤šç§æ¨é€æ¨¡å¼ï¼šgeneral/token/time
                yield ServerSentEvent(data={requestId, data=chunk})
    
    return EventSourceResponse(_stream())
```

**ç¬¬1å±‚çš„æ ¸å¿ƒèŒè´£ï¼š**
- **HTTPè¯·æ±‚æ¥å…¥** - å¤„ç†POSTè¯·æ±‚å’Œå‚æ•°éªŒè¯
- **æ–‡ä»¶è·¯å¾„é¢„å¤„ç†** - æ ‡å‡†åŒ–æ–‡ä»¶URLæ ¼å¼
- **æµå¼å“åº”æ§åˆ¶** - ç®¡ç†SSEæµå¼æ¨é€çš„å¤šç§æ¨é€ç­–ç•¥
- **æ•°æ®ç±»å‹è½¬æ¢** - å°†ä¸šåŠ¡å¯¹è±¡è½¬æ¢ä¸ºSSEæ ¼å¼
- **ä¼šè¯ç®¡ç†** - é€šè¿‡requestIdè·Ÿè¸ªè¯·æ±‚çŠ¶æ€
- **æ¨é€æ¨¡å¼æ§åˆ¶** - æ”¯æŒgeneral/token/timeä¸‰ç§æ¨é€ç­–ç•¥

### ç¬¬2å±‚ï¼šcode_interpreter_agentå¤„ç†ä¸šåŠ¡é€»è¾‘
```python
# genie_tool/tool/code_interpreter.py
async def code_interpreter_agent(task, file_names, request_id, stream=True):
    # 1. æ–‡ä»¶ä¸‹è½½å’Œé¢„å¤„ç†
    files = await download_files_if_needed(file_names)
    
    # 2. åˆ›å»ºCIAgentæ™ºèƒ½ä½“
    agent = create_ci_agent(
        prompt_templates=ci_prompt_template,
        max_tokens=max_tokens,
        return_full_result=True,
        output_dir=output_dir,
    )
    
    # 3. æ„å»ºä»»åŠ¡æ¨¡æ¿
    template_task = Template(ci_prompt_template["task_template"]).render(
        files=files, task=task, output_dir=output_dir
    )
    
    # 4. æµå¼è¿è¡Œæ™ºèƒ½ä½“
    if stream:
        for step in agent.run(task=template_task, stream=True, max_steps=10):
            if isinstance(step, CodeOuput):  # ä»£ç ç”Ÿæˆæ­¥éª¤
                # æ–‡ä»¶ä¸Šä¼ å¤„ç†
                step.file_list = [await upload_file(...)]
                yield step
            elif isinstance(step, FinalAnswerStep):  # æœ€ç»ˆç»“æœ
                # æ”¶é›†æ‰€æœ‰ç”Ÿæˆæ–‡ä»¶å¹¶ä¸Šä¼ 
                yield ActionOutput(content=step.answer, file_list=all_files)
            else:  # æ–‡æœ¬æµç›´æ¥ä¼ é€’
                yield step
def create_ci_agent(...) -> CIAgent:
    # 1. åˆ›å»ºLLMæ¨¡å‹å®ä¾‹
    model = LiteLLMModel(
        max_tokens=max_tokens,
        model_id=os.getenv("CODE_INTEPRETER_MODEL", "gpt-4.1")  # æ¨¡å‹é…ç½®
    )
    
    # 2. æ„å»ºCIAgentæ™ºèƒ½ä½“
    return CIAgent(
        model=model,                              # LLMè°ƒç”¨å±‚
        prompt_templates=prompt_templates,        # æç¤ºè¯æ¨¡æ¿
        tools=[PythonInterpreterTool()],         # å·¥å…·é›†æˆ
        return_full_result=True,                 # å®Œæ•´ç»“æœå¼€å…³
        additional_authorized_imports=[...],     # é¢„æˆæƒåŒ…å¯¼å…¥
        output_dir=output_dir,                   # è¾“å‡ºç›®å½•
    )

```

**ç¬¬2å±‚çš„æ ¸å¿ƒèŒè´£ï¼š**
- **æ™ºèƒ½ä½“ç®¡ç†** - åˆ›å»ºå’Œé…ç½®CIAgentå®ä¾‹
- **ä»»åŠ¡åè°ƒ** - æ„å»ºä»»åŠ¡æ¨¡æ¿å’Œæç¤ºè¯å·¥ç¨‹
- **æ–‡ä»¶å¤„ç†** - ä¸‹è½½è¾“å…¥æ–‡ä»¶ã€ä¸Šä¼ ç”Ÿæˆæ–‡ä»¶
- **æ™ºèƒ½ä½“è¿è¡Œ** -  è§¦å‘æ™ºèƒ½ä½“è¿è¡Œ
- **æµå¼æ§åˆ¶** - ç®¡ç†æ™ºèƒ½ä½“çš„æµå¼è¿è¡Œæ¨¡å¼
- **æ•°æ®å¢å¼º** - å°†CIAgentè¾“å‡ºè½¬æ¢ä¸ºä¸šåŠ¡å¢å¼ºå¯¹è±¡
- **æ­¥éª¤åˆ†å‘** - æ ¹æ®æ­¥éª¤ç±»å‹è¿›è¡Œå·®å¼‚åŒ–å¤„ç†



### ç¬¬3å±‚ï¼šCIAgent æ™ºèƒ½ä½“ç±»

```python
# genie_tool/tool/ci_agent.py

# CIAgentç±»æ„é€ æ–¹æ³•
class CIAgent(CodeAgent):
    def __init__(
        self,
        tools: list[Tool],
        model: Model,
        prompt_templates: PromptTemplates | None = None,
        additional_authorized_imports: list[str] | None = None,
        planning_interval: int | None = None,
        executor_type: str | None = "local",
        output_dir: Optional[str] = None,
        *args, **kwargs,
    ):
        self.output_dir = output_dir  # è¾“å‡ºç›®å½•è®¾ç½®
        # è°ƒç”¨çˆ¶ç±»æ„é€ æ–¹æ³•ï¼Œç»§æ‰¿CodeAgentèƒ½åŠ›
        super().__init__(
            tools=tools,                    # å·¥å…·åˆ—è¡¨
            model=model,                    # LLMæ¨¡å‹
            prompt_templates=prompt_templates,  # æç¤ºè¯æ¨¡æ¿
            additional_authorized_imports=additional_authorized_imports,  # é¢„æˆæƒå¯¼å…¥
            planning_interval=planning_interval,  # è§„åˆ’é—´éš”
            executor_type=executor_type,    # æ‰§è¡Œå™¨ç±»å‹
            **kwargs,
        )
    
    # æŒ‰éœ€é‡å†™çˆ¶ç±»_step_streamæ–¹æ³•ï¼Œå®ç°åˆ†é˜¶æ®µæµå¼è¾“å‡ºã€**æ€§èƒ½ç›‘æ§**ã€è‡ªå®šä¹‰å®Œæˆåˆ¤æ–­
    @timer()
    def _step_stream(self, memory_step: ActionStep) -> Generator[
        ChatMessageStreamDelta | ToolCall | ToolOutput | ActionOutput | CodeOuput
    ]:
        """ReActæ¡†æ¶çš„æ ¸å¿ƒæ­¥éª¤ï¼šæ€è€ƒâ†’è¡ŒåŠ¨â†’è§‚å¯Ÿ"""
        # 1. æ„å»ºè®°å¿†æ¶ˆæ¯å¹¶è°ƒç”¨LLMæµå¼ç”Ÿæˆ
        memory_messages = self.write_memory_to_messages()
        output_stream = self.model.generate_stream(
            memory_messages,
            extra_headers={"x-ms-client-request-id": model_request_id}
        )
        
        # 2. æµå¼å¤„ç†å’Œæ¶ˆæ¯èšåˆ
        chat_message_stream_deltas = []
        for event in output_stream:
            chat_message_stream_deltas.append(event)
            yield event  # â†’ ChatMessageStreamDelta å®æ—¶æ–‡æœ¬æµ
            
        # 3. èšåˆå®Œæ•´æ¶ˆæ¯å¹¶è§£æä»£ç å—
        chat_message = agglomerate_stream_deltas(chat_message_stream_deltas)
        code_action = parse_code_blobs(chat_message.content)
        
        # 4. åˆ›å»ºå·¥å…·è°ƒç”¨å¹¶æ‰§è¡ŒPythonä»£ç 
        memory_step.tool_calls = [ToolCall(name="python_interpreter", ...)]
        _, execution_logs, _ = self.python_executor(code_action)
        
        # 5. è¿”å›ä»£ç ç”Ÿæˆç»“æœ
        yield CodeOuput(code=code_action, file_name=file_name)  # â†’ ä»£ç ç”Ÿæˆç»“æœ
        
        # 6. æœ€ç»ˆç­”æ¡ˆæ£€æŸ¥å’Œè¾“å‡º
        finalObj = FinalAnswerCheck(
            input_messages=self.input_messages,
            execution_logs=execution_logs,
            model=self.model, task=self.task, ...
        )
        finalFlag, exeLog = finalObj.check_is_final_answer()
        yield ActionOutput(output=exeLog, is_final_answer=finalFlag)  # â†’ æœ€ç»ˆæ‰§è¡Œç»“æœ
```

**ç¬¬3å±‚çš„æ ¸å¿ƒèŒè´£ï¼š**
- **æ™ºèƒ½ä½“å®šä¹‰** - åŸºäºsmolagentsæ¡†æ¶è‡ªå®šä¹‰ç»§æ‰¿CodeAgentçš„CIAgentç±»
- **ReActæ‰§è¡Œ** - å®ç°æ€è€ƒâ†’è¡ŒåŠ¨â†’è§‚å¯Ÿçš„æ¨ç†æ¡†æ¶
- **æµå¼å¤„ç†** - å°†LLMæµè½¬æ¢ä¸ºç»“æ„åŒ–å¯¹è±¡
- **æ™ºèƒ½ç¼–ç¨‹** - æ ¹æ®ä»»åŠ¡éœ€æ±‚ç”Ÿæˆç¬¦åˆé€»è¾‘çš„Pythonä»£ç 
- **ä»£ç æ‰§è¡Œ** - é›†æˆPythonè§£é‡Šå™¨è¿›è¡Œä»£ç è¿è¡Œ
- **å·¥å…·ç®¡ç†** - ç®¡ç†å„ç§å¤–éƒ¨å·¥å…·è°ƒç”¨

### ç¬¬4å±‚ï¼šLiteLLMç»Ÿä¸€è°ƒç”¨å±‚

#### 4.1 ä¸»å…¥å£å‡½æ•° - completion()
```python
# site-packages/litellm/main.py:80-294
def completion(
    model, messages,  # å¿…éœ€å‚æ•°
    functions=[], temperature=1, top_p=1, stream=False, max_tokens=float('inf'),
    api_key=None, azure=False, logger_fn=None, **kwargs
):
    # è·å–å¯é€‰å‚æ•°
    optional_params = get_optional_params(functions, temperature, stream, ...)
    
    # ğŸ”„ æ ¸å¿ƒè·¯ç”±é€»è¾‘å¼€å§‹
    if azure == True:
        # Azure OpenAI ä¸“ç”¨åˆ†æ”¯
        return azure_openai_call(model, messages, optional_params)
    elif model in litellm.open_ai_chat_completion_models:
        # OpenAI Chat æ¨¡å‹åˆ†æ”¯  
        return openai_chat_call(model, messages, optional_params)
    elif model in litellm.open_ai_text_completion_models:
        # OpenAI Text æ¨¡å‹åˆ†æ”¯
        return openai_text_call(model, messages, optional_params)
    elif "replicate" in model:
        # Replicate æ¨¡å‹åˆ†æ”¯
        return replicate_call(model, messages, optional_params)
    elif model in litellm.anthropic_models:
        # Anthropic Claude åˆ†æ”¯
        return anthropic_call(model, messages, optional_params)
    elif model in litellm.cohere_models:
        # Cohere æ¨¡å‹åˆ†æ”¯
        return cohere_call(model, messages, optional_params)
    else:
        raise ValueError(f"No valid completion model: {model}")
```

#### 4.2 æ¨¡å‹è·¯ç”±é…ç½® - å‚å•†åˆ¤æ–­ä¾æ®
```python
# site-packages/litellm/__init__.py:30-65

# OpenAI Chat æ¨¡å‹åˆ—è¡¨
open_ai_chat_completion_models = [
    "gpt-4", "gpt-4-0613", "gpt-4-32k", "gpt-4-32k-0613",
    "gpt-3.5-turbo", "gpt-3.5-turbo-16k", "gpt-3.5-turbo-0613"
]

# Anthropic æ¨¡å‹åˆ—è¡¨  
anthropic_models = ["claude-2", "claude-instant-1"]

# Cohere æ¨¡å‹åˆ—è¡¨
cohere_models = ["command-nightly", "command", "command-light"]

# Replicate æ¨¡å‹åˆ¤æ–­ï¼šä»»ä½•åŒ…å« "replicate/" çš„æ¨¡å‹å
replicate_models = ["replicate/"]  # å‰ç¼€åŒ¹é…
```

#### 4.3 å„å‚å•†APIè°ƒç”¨å®ç°è¯¦è§£

**ğŸ”¹ OpenAI è°ƒç”¨åˆ†æ”¯ï¼ˆç¬¬123-148è¡Œï¼‰**
```python
elif model in litellm.open_ai_chat_completion_models:
    # 1. APIé…ç½®
    openai.api_type = "openai"
    openai.api_base = "https://api.openai.com/v1"
    openai.api_key = api_key or os.environ.get("OPENAI_API_KEY")
    
    # 2. ç›´æ¥è°ƒç”¨OpenAIåŸç”Ÿæ¥å£
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        **optional_params  # stream=True, max_tokens, temperatureç­‰
    )
    return response  # ğŸ¯ åŸç”ŸOpenAIæ ¼å¼ï¼Œæ— éœ€è½¬æ¢
```

**ğŸ”¹ Anthropic Claude è°ƒç”¨åˆ†æ”¯ï¼ˆç¬¬212-255è¡Œï¼‰**
```python
elif model in litellm.anthropic_models:
    # 1. APIå¯†é’¥é…ç½®
    os.environ["ANTHROPIC_API_KEY"] = api_key or litellm.anthropic_key
    
    # 2. æ¶ˆæ¯æ ¼å¼è½¬æ¢ï¼šOpenAI â†’ Anthropic
    prompt = f"{HUMAN_PROMPT}"
    for message in messages:
        if message["role"] == "user":
            prompt += f"{HUMAN_PROMPT}{message['content']}"
        else:
            prompt += f"{AI_PROMPT}{message['content']}"
    prompt += f"{AI_PROMPT}"
    
    # 3. è°ƒç”¨AnthropicåŸç”Ÿæ¥å£
    anthropic = Anthropic()
    completion = anthropic.completions.create(
        model=model,
        prompt=prompt,  # ğŸ”„ å·²è½¬æ¢æ ¼å¼
        max_tokens_to_sample=max_tokens
    )
    
    # 4. å“åº”æ ¼å¼è½¬æ¢ï¼šAnthropic â†’ OpenAI
    new_response = {
        "choices": [{
            "finish_reason": "stop",
            "message": {
                "content": completion.completion,  # ğŸ”„ æå–å†…å®¹
                "role": "assistant"
            }
        }]
    }
    return new_response  # ğŸ¯ ç»Ÿä¸€OpenAIæ ¼å¼
```


#### 4.4 CIAgenté›†æˆè°ƒç”¨ç¤ºä¾‹
```python
# genie_tool/tool/code_interpreter.py:119-122
model = LiteLLMModel(
    max_tokens=max_tokens,
    model_id=os.getenv("CODE_INTEPRETER_MODEL", "gpt-4")  # ğŸ¯ æ¨¡å‹åè§¦å‘è·¯ç”±
)

# CIAgentå†…éƒ¨æµå¼è°ƒç”¨
output_stream = self.model.generate_stream(
    memory_messages,
    extra_headers={"x-ms-client-request-id": model_request_id}
)
```

**ç¬¬4å±‚çš„æ ¸å¿ƒèŒè´£ï¼ˆè¯¦ç»†åˆ†è§£ï¼‰ï¼š**
- **æ™ºèƒ½è·¯ç”±** - åŸºäºæ¨¡å‹åç§°è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„å‚å•†API
- **æ ¼å¼è½¬æ¢** - åŒå‘è½¬æ¢ï¼šè¾“å…¥æ ¼å¼é€‚é… + è¾“å‡ºæ ¼å¼ç»Ÿä¸€
- **è®¤è¯ç®¡ç†** - ç»Ÿä¸€å¤„ç†å„å‚å•†çš„APIå¯†é’¥å’Œè®¤è¯æ–¹å¼
- **æµå¼æ”¯æŒ** - ä¿æŒå„å‚å•†æµå¼å“åº”çš„å®æ—¶æ€§
- **å¼‚å¸¸ç»Ÿä¸€** - å°†ä¸åŒå‚å•†çš„å¼‚å¸¸æ˜ å°„ä¸ºç»Ÿä¸€çš„é”™è¯¯ç±»å‹
- **æ ‡å‡†åŒ–è¾“å‡º** - ç¡®ä¿æ‰€æœ‰å“åº”éƒ½ç¬¦åˆOpenAIæ ¼å¼ï¼š`choices[0].message.content`

### ç¬¬5å±‚ï¼šåŸå§‹LLM APIæœåŠ¡
```python
# OpenAI API / Claude API / DeepSeek API / å…¶ä»–å‚å•†API

# åŸå§‹APIè°ƒç”¨ç¤ºä¾‹
POST https://api.openai.com/v1/chat/completions
{
    "model": "gpt-4-turbo",
    "messages": [...],
    "stream": true,  # æµå¼å¼€å…³
    "max_tokens": 16000
}

# æµå¼å“åº”æ ¼å¼ï¼ˆå„å‚å•†ç»“æ„ä¸åŒï¼‰
data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","choices":[{"delta":{"content":"Hello"}}]}
data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","choices":[{"delta":{"content":" world"}}]}
data: [DONE]

# ä¸åŒå‚å•†çš„å“åº”å·®å¼‚
OpenAI: choices[0].delta.content
Claude: delta.text  
DeepSeek: choices[0].delta.content
```

**ç¬¬5å±‚çš„æ ¸å¿ƒèŒè´£ï¼š**
- **åŸå§‹AIèƒ½åŠ›** - æä¾›åŸºç¡€çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›
- **æµå¼å“åº”** - æ”¯æŒå®æ—¶æµå¼æ–‡æœ¬ç”Ÿæˆ
- **å¤šæ¨¡æ€æ”¯æŒ** - éƒ¨åˆ†æ¨¡å‹æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§è¾“å…¥
- **å‚æ•°æ§åˆ¶** - é€šè¿‡temperatureã€top_pç­‰å‚æ•°æ§åˆ¶ç”Ÿæˆè´¨é‡
- **ä»¤ç‰Œç®¡ç†** - ç®¡ç†è¾“å…¥è¾“å‡ºçš„tokenæ¶ˆè€—å’Œé™åˆ¶
- **å‚å•†å·®å¼‚** - ä¸åŒå‚å•†åœ¨APIæ ¼å¼ã€èƒ½åŠ›è¾¹ç•Œä¸Šå­˜åœ¨å·®å¼‚


## å°ç»“

æœ¬èŠ‚ä»¥äº¬ä¸œå¼€æºçš„genie-toolæœåŠ¡ä¸ºä¾‹ï¼Œæ·±å…¥å‰–æäº†å•æ™ºèƒ½ä½“æµå¼é€šä¿¡æœåŠ¡ç«¯çš„å®Œæ•´å®ç°æ¶æ„ã€‚æ ¸å¿ƒè¦ç‚¹å¦‚ä¸‹ï¼š

**æŠ€æœ¯æ ˆé€‰å‹**ï¼šé‡‡ç”¨ **FastAPI + sse_starlette + LiteLLM + SmoLAgents** çš„ç»„åˆï¼Œå„ç»„ä»¶å„å¸å…¶èŒâ€”â€”FastAPIæä¾›é«˜æ€§èƒ½å¼‚æ­¥WebæœåŠ¡ï¼Œsse_starletteå®ç°æ ‡å‡†SSEåè®®ï¼ŒLiteLLMç»Ÿä¸€å¤šå‚å•†LLMæ¥å£ï¼ŒSmoLAgentsæä¾›ReActæ¨ç†æ¡†æ¶ã€‚

**äº”å±‚æ¶æ„è®¾è®¡**ï¼š
- **ç¬¬1å±‚ï¼ˆFastAPIè·¯ç”±ï¼‰**ï¼šè´Ÿè´£HTTPè¯·æ±‚æ¥å…¥å’ŒSSEæµå¼å“åº”è¾“å‡ºï¼Œæ”¯æŒå¤šç§æ¨é€ç­–ç•¥
- **ç¬¬2å±‚ï¼ˆä¸šåŠ¡é€»è¾‘å±‚ï¼‰**ï¼šåè°ƒæ™ºèƒ½ä½“åˆ›å»ºã€ä»»åŠ¡æ¨¡æ¿æ„å»ºå’Œæ–‡ä»¶å¤„ç†
- **ç¬¬3å±‚ï¼ˆCIAgentæ™ºèƒ½ä½“ï¼‰**ï¼šæ‰§è¡ŒReActæ¨ç†å¾ªç¯ï¼Œå®Œæˆä»£ç ç”Ÿæˆä¸æ‰§è¡Œ
- **ç¬¬4å±‚ï¼ˆLiteLLMï¼‰**ï¼šæ™ºèƒ½è·¯ç”±åˆ°ä¸åŒå‚å•†APIï¼Œç»Ÿä¸€è¾“å…¥è¾“å‡ºæ ¼å¼
- **ç¬¬5å±‚ï¼ˆå‚å•†APIï¼‰**ï¼šæä¾›åŸå§‹çš„å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›

**æµå¼æ•°æ®æµè½¬**ï¼šè¯·æ±‚è‡ªä¸Šè€Œä¸‹é€å±‚è°ƒç”¨ï¼Œæµå¼å“åº”è‡ªä¸‹è€Œä¸Šé€å±‚ä¼ é€’ã€‚æ¯ä¸€å±‚å¯¹æ•°æ®è¿›è¡Œé€‚å½“çš„è½¬æ¢å’Œå¢å¼ºï¼Œæœ€ç»ˆä»¥SSEæ ¼å¼å®æ—¶æ¨é€ç»™è°ƒç”¨æ–¹ï¼Œå®ç°äº†ä»LLMåˆ°å‰ç«¯çš„å®Œæ•´æµå¼é€šä¿¡é“¾è·¯ã€‚

è¿™ç§åˆ†å±‚æ¶æ„çš„ä¼˜åŠ¿åœ¨äºï¼š**èŒè´£æ¸…æ™°ã€æ˜“äºæ‰©å±•ã€ä¾¿äºç»´æŠ¤**ã€‚å½“éœ€è¦åˆ‡æ¢LLMå‚å•†æ—¶åªéœ€ä¿®æ”¹é…ç½®ï¼Œå½“éœ€è¦å¢å¼ºä¸šåŠ¡èƒ½åŠ›æ—¶åªéœ€æ‰©å±•å¯¹åº”å±‚çº§ï¼Œä½“ç°äº†è‰¯å¥½çš„å·¥ç¨‹åŒ–è®¾è®¡æ€æƒ³ã€‚



## ğŸ¯ åç«¯å·¥å…·æœåŠ¡æ¶æ„åˆ†å±‚èŒè´£åˆ’åˆ†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          APIè·¯ç”±å±‚ (tool.py)        â”‚  â† HTTPè¯·æ±‚å¤„ç†ã€å‚æ•°éªŒè¯ã€å“åº”æ ¼å¼åŒ–
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     ä¸šåŠ¡é€»è¾‘å±‚ (code_interpreter.py) â”‚  â† æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ã€Agentç¼–æ’ã€æ–‡ä»¶å¤„ç†
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Agentå±‚ (ci_agent.py)           â”‚  â† æ™ºèƒ½æ¨ç†ã€ä»£ç æ‰§è¡Œã€æµå¼å¤„ç†
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      æ¨¡å‹å±‚ (LiteLLM + SmoLAgents)      â”‚  â† LLMè°ƒç”¨ã€æ¨ç†æ¡†æ¶
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```






å•æ™ºèƒ½ä½“åœ¨AIåº”ç”¨ä¸­çš„æµå¼æ•°æ®ä¼ è¾“çš„å®Œæ•´é“¾è·¯

## ğŸ”„ æµå¼æ•°æ®ä¼ è¾“çš„å®Œæ•´é“¾è·¯

### 1. æ•°æ®æµå‘åˆ†æ

"""
æ•°æ®æµå‘é“¾è·¯ï¼š

LLM API (OpenAI/Claude/DeepSeek)
    â†“ åŸå§‹æµå¼å“åº”
code_interpreter_agent() 
    â†“ å¤„ç†åçš„ CodeOutput/ActionOutput å¯¹è±¡
_stream() å¼‚æ­¥ç”Ÿæˆå™¨
    â†“ yield ServerSentEvent (ä½ é—®çš„è¿™ä¸€æ­¥ï¼)
EventSourceResponse (sse-starlette)
    â†“ HTTP SSE æµå¼å“åº”
Javaåç«¯ (æ¥æ”¶æ–¹)
    â†“ å†æ¬¡è½¬å‘
å‰ç«¯ç”¨æˆ·ç•Œé¢
"""
### 2. ServerSentEvent çš„æ•°æ®æ ¼å¼
```Python
             yield ServerSentEvent(
                    data=json.dumps(
                        {
                            "requestId": body.request_id,
                            "code": chunk.code,
                            "fileInfo": chunk.file_list,
                            "isFinal": False,
                        },
                        ensure_ascii=False,
                    )
                )

```


2.2 æµå¼å“åº”å¤„ç†æœºåˆ¶
```Python
  async with AsyncTimer(key=f"exec ask_llm"):
        if stream:
            async for chunk in response:
                if only_content:
                    if chunk.choices and chunk.choices[0] and chunk.choices[0].delta and chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                else:
                    yield chunk
        else:
            yield response.choices[0].message.content if only_content else response
```
å…³é”®ç‰¹æ€§ï¼š
å¼‚æ­¥è¿­ä»£å™¨æ¨¡å¼ - async for chunk in response
å¢é‡å†…å®¹æå– - ä»delta.contentè·å–æµå¼ç‰‡æ®µ
çµæ´»è¾“å‡ºæ¨¡å¼ - æ”¯æŒçº¯å†…å®¹æˆ–å®Œæ•´å“åº”å¯¹è±¡



 ä¸‰ç§æµå¼æ¨¡å¼ï¼šGeneral(å®æ—¶)ã€Token(ç´¯ç§¯)ã€Time(å®šæ—¶)ï¼Œé€‚åº”ä¸åŒåº”ç”¨åœºæ™¯

### 1.Generalæ¨¡å¼ï¼ˆå®æ—¶æ¨é€ï¼‰
ç‰¹ç‚¹ï¼š

- æ¯ä¸ªchunkç«‹å³æ¨é€åˆ°å‰ç«¯
- æœ€å¿«çš„ç”¨æˆ·åé¦ˆä½“éªŒ
- é€‚ç”¨äºä»£ç ç”Ÿæˆã€å®æ—¶å¯¹è¯åœºæ™¯

åº”ç”¨åœºæ™¯ï¼šÂ ä»£ç è§£é‡Šå™¨å·¥å…·ï¼Œç”¨æˆ·éœ€è¦å®æ—¶çœ‹åˆ°ä»£ç ç”Ÿæˆè¿‡ç¨‹
```Python
        acc_content += chunk
        acc_token += 1
        if body.stream_mode.mode == "general":
            yield ServerSentEvent(
                data=json.dumps(
                    {"requestId": body.request_id, "data": chunk, "isFinal": False},
                    ensure_ascii=False,
                )
            )
```


### 3.2 Tokenæ¨¡å¼ï¼ˆæ‰¹é‡ç´¯ç§¯æ¨é€ï¼‰
ç‰¹ç‚¹ï¼š
- ç´¯ç§¯Nä¸ªtokenåæ‰¹é‡æ¨é€
- å¹³è¡¡å“åº”é€Ÿåº¦å’Œç½‘ç»œå¼€é”€
- é€‚ç”¨äºé•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯
åº”ç”¨åœºæ™¯ï¼šÂ æ·±åº¦æœç´¢å·¥å…·ï¼Œéœ€è¦å¤„ç†å¤§é‡æœç´¢ç»“æœ
```Python
                elif body.stream_mode.mode == "token":
                    if acc_token >= body.stream_mode.token:
                        yield ServerSentEvent(
                            data=json.dumps(
                                {
                                    "requestId": body.request_id,
                                    "data": acc_content,
                                    "isFinal": False,
                                },
                                ensure_ascii=False,
                            )
                        )
                        acc_token = 0
                        acc_content = ""
```
### 3.3 Timeæ¨¡å¼ï¼ˆå®šæ—¶æ‰¹é‡æ¨é€ï¼‰
ç‰¹ç‚¹ï¼š
æŒ‰å›ºå®šæ—¶é—´é—´éš”æ¨é€
ç¨³å®šçš„æ¨é€èŠ‚å¥
é€‚ç”¨äºæŠ¥å‘Šç”Ÿæˆåœºæ™¯
åº”ç”¨åœºæ™¯ï¼š HTML/PPTæŠ¥å‘Šç”Ÿæˆï¼Œé¿å…é¢‘ç¹æ¨é€å½±å“æ¸²æŸ“æ€§èƒ½

```Python
                elif body.stream_mode.mode == "time":
                    if time.time() - acc_time > body.stream_mode.time:
                        yield ServerSentEvent(
                            data=json.dumps(
                                {
                                    "requestId": body.request_id,
                                    "data": acc_content,
                                    "isFinal": False,
                                },
                                ensure_ascii=False,
                            )
                        )
                        acc_time = time.time()
                        acc_content = ""
```






### å®é™…åº”ç”¨åœºæ™¯

- ä»£ç è§£é‡Šå™¨ï¼šå®æ—¶ç”Ÿæˆå’Œæ‰§è¡ŒPythonä»£ç ï¼Œæµå¼æ˜¾ç¤ºè¿‡ç¨‹

- æ·±åº¦æœç´¢ï¼šæ‰¹é‡å¤„ç†æœç´¢ç»“æœï¼ŒTokenæ¨¡å¼ä¼˜åŒ–ç½‘ç»œå¼€é”€

- æŠ¥å‘Šç”Ÿæˆï¼šå®šæ—¶æ¨é€HTML/PPTå†…å®¹ï¼Œé¿å…é¢‘ç¹UIæ›´æ–°


## Agentæ‰§è¡ŒçŠ¶æ€çš„æµå¼åé¦ˆ

### Agentç”Ÿå‘½å‘¨æœŸä¸çŠ¶æ€æµ

AI Agentçš„æ‰§è¡Œè¿‡ç¨‹é€šå¸¸åŒ…å«å¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½éœ€è¦å‘ç”¨æˆ·æä¾›å®æ—¶åé¦ˆï¼š

```java
public enum AgentState {
    INITIALIZING("åˆå§‹åŒ–ä¸­", "æ­£åœ¨å‡†å¤‡æ‰§è¡Œç¯å¢ƒ..."),
    PLANNING("åˆ¶å®šè®¡åˆ’", "æ­£åœ¨åˆ†æä»»åŠ¡å¹¶åˆ¶å®šæ‰§è¡Œè®¡åˆ’..."),
    TOOL_CALLING("è°ƒç”¨å·¥å…·", "æ­£åœ¨ä½¿ç”¨å·¥å…·æ‰§è¡Œå…·ä½“ä»»åŠ¡..."),
    THINKING("æ€è€ƒä¸­", "æ­£åœ¨åˆ†æä¸­é—´ç»“æœ..."),
    SUMMARIZING("æ€»ç»“ä¸­", "æ­£åœ¨æ•´ç†å’Œæ€»ç»“æ‰§è¡Œç»“æœ..."),
    COMPLETED("å·²å®Œæˆ", "ä»»åŠ¡æ‰§è¡Œå®Œæˆ"),
    ERROR("æ‰§è¡Œé”™è¯¯", "æ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯");

    private final String displayName;
    private final String description;

    AgentState(String displayName, String description) {
        this.displayName = displayName;
        this.description = description;
    }
}

@Component
public class AgentStatusStreamer {
    
    public void streamAgentStatus(Printer printer, AgentState state, String detail) {
        AgentStatusMessage message = AgentStatusMessage.builder()
            .state(state.name())
            .displayName(state.getDisplayName())
            .description(state.getDescription())
            .detail(detail)
            .timestamp(System.currentTimeMillis())
            .build();
            
        printer.send("agent_status", message);
    }

    public void streamThinking(Printer printer, String thought) {
        ThinkingMessage message = ThinkingMessage.builder()
            .content(thought)
            .timestamp(System.currentTimeMillis())
            .build();
            
        printer.send("agent_thinking", message);
    }

    public void streamPlanUpdate(Printer printer, List<Plan> plans, int currentStep) {
        PlanUpdateMessage message = PlanUpdateMessage.builder()
            .plans(plans)
            .currentStep(currentStep)
            .progress(calculateProgress(plans, currentStep))
            .timestamp(System.currentTimeMillis())
            .build();
            
        printer.send("plan_update", message);
    }

    private double calculateProgress(List<Plan> plans, int currentStep) {
        if (plans == null || plans.isEmpty()) {
            return 0.0;
        }
        return Math.min(100.0, (double) currentStep / plans.size() * 100);
    }
}
```

### åˆ†å±‚çŠ¶æ€åé¦ˆæœºåˆ¶

```java
public abstract class BaseAgent {
    protected Printer printer;
    protected AgentStatusStreamer statusStreamer;
    
    protected void executeWithStatusFeedback(String operation, Runnable task) {
        try {
            // å¼€å§‹æ‰§è¡Œåé¦ˆ
            statusStreamer.streamAgentStatus(printer, 
                AgentState.TOOL_CALLING, "å¼€å§‹æ‰§è¡Œ: " + operation);
            
            // æ‰§è¡Œå®é™…ä»»åŠ¡
            task.run();
            
            // å®Œæˆåé¦ˆ
            statusStreamer.streamAgentStatus(printer, 
                AgentState.COMPLETED, "å®Œæˆ: " + operation);
                
        } catch (Exception e) {
            // é”™è¯¯åé¦ˆ
            statusStreamer.streamAgentStatus(printer, 
                AgentState.ERROR, "æ‰§è¡Œå¤±è´¥: " + operation + " - " + e.getMessage());
            throw e;
        }
    }

    protected <T> T executeWithProgressFeedback(String operation, 
                                               Supplier<T> task, 
                                               ProgressCallback callback) {
        statusStreamer.streamAgentStatus(printer, AgentState.TOOL_CALLING, operation);
        
        return task.get(); // å®é™…æ‰§è¡Œä¸­ä¼šç»“åˆcallbackæä¾›è¿›åº¦æ›´æ–°
    }
}

@Component
public class PlanningAgent extends BaseAgent {
    
    @Override
    public void handle(AgentContext context, AgentRequest request) {
        printer = context.getPrinter();
        statusStreamer = new AgentStatusStreamer();
        
        try {
            // é˜¶æ®µ1ï¼šåˆå§‹åŒ–
            statusStreamer.streamAgentStatus(printer, AgentState.INITIALIZING, 
                "å‡†å¤‡åˆ†æä»»åŠ¡: " + request.getQuery());
            
            // é˜¶æ®µ2ï¼šåˆ¶å®šè®¡åˆ’
            statusStreamer.streamAgentStatus(printer, AgentState.PLANNING, 
                "æ­£åœ¨åˆ¶å®šæ‰§è¡Œè®¡åˆ’...");
            
            List<Plan> plans = executeWithProgressFeedback("ç”Ÿæˆæ‰§è¡Œè®¡åˆ’", 
                () -> generatePlan(request), 
                (progress, detail) -> {
                    // è¿›åº¦å›è°ƒ
                    printer.send("plan_progress", Map.of(
                        "progress", progress,
                        "detail", detail
                    ));
                });
            
            // æµå¼å‘é€å®Œæ•´è®¡åˆ’
            statusStreamer.streamPlanUpdate(printer, plans, 0);
            
            // é˜¶æ®µ3ï¼šæ‰§è¡Œè®¡åˆ’
            executePlansWithStreaming(context, plans);
            
        } catch (Exception e) {
            statusStreamer.streamAgentStatus(printer, AgentState.ERROR, e.getMessage());
            throw e;
        }
    }

    private void executePlansWithStreaming(AgentContext context, List<Plan> plans) {
        for (int i = 0; i < plans.size(); i++) {
            Plan plan = plans.get(i);
            
            // æ›´æ–°å½“å‰æ‰§è¡Œè®¡åˆ’
            statusStreamer.streamPlanUpdate(printer, plans, i + 1);
            
            // æ‰§è¡Œå•ä¸ªè®¡åˆ’æ­¥éª¤
            executeWithStatusFeedback("æ‰§è¡Œæ­¥éª¤: " + plan.getAction(), () -> {
                executePlan(context, plan);
                plan.setStatus(PlanStatus.COMPLETED);
            });
            
            // å‘é€æ­¥éª¤å®Œæˆçš„æµå¼åé¦ˆ
            printer.send("plan_step_completed", Map.of(
                "stepIndex", i,
                "plan", plan,
                "progress", (i + 1.0) / plans.size() * 100
            ));
        }
    }
}
```




## å°ç»“

AI-Agentä¸­çš„æµå¼äº¤äº’æ¨¡å¼å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **çŠ¶æ€æµå¼åé¦ˆ**ï¼šAgentæ‰§è¡Œçš„æ¯ä¸ªé˜¶æ®µéƒ½éœ€è¦å®æ—¶åé¦ˆç»™ç”¨æˆ·
2. **å·¥å…·è°ƒç”¨é€æ˜åŒ–**ï¼šå·¥å…·æ‰§è¡Œè¿‡ç¨‹å’Œç»“æœéœ€è¦å¢é‡ä¼ è¾“
3. **å¤šAgentåä½œå¯è§†åŒ–**ï¼šå¤æ‚çš„åä½œå…³ç³»éœ€è¦å®æ—¶å±•ç¤º
4. **Tokenä½¿ç”¨ç›‘æ§**ï¼šå®æ—¶ç›‘æ§å’Œæ§åˆ¶Tokenæ¶ˆè€—
5. **è¿›åº¦å¯è§†åŒ–**ï¼šæä¾›ç›´è§‚çš„æ‰§è¡Œè¿›åº¦å’ŒçŠ¶æ€å±•ç¤º

è¿™äº›æ¨¡å¼ä½¿ç”¨æˆ·èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£AI Agentçš„"æ€è€ƒ"å’Œæ‰§è¡Œè¿‡ç¨‹ï¼Œæä¾›äº†å‰æ‰€æœªæœ‰çš„é€æ˜åº¦å’Œäº¤äº’ä½“éªŒã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•ä¼˜åŒ–æµå¼é€šä¿¡çš„æ€§èƒ½å¹¶å»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»ã€‚

---

**æœ¬èŠ‚å…³é”®è¦ç‚¹ï¼š**
- Agentç”Ÿå‘½å‘¨æœŸçŠ¶æ€çš„æµå¼åé¦ˆæœºåˆ¶
- å·¥å…·è°ƒç”¨ç»“æœçš„å¢é‡ä¼ è¾“ç­–ç•¥
- å¤šAgentåä½œçš„å®æ—¶é€šä¿¡å’Œå¯è§†åŒ–
- æµå¼Tokenè®¡æ•°å’ŒåŠ¨æ€é™æµæ§åˆ¶
- Agentæ‰§è¡Œè¿›åº¦çš„å¯è§†åŒ–å±•ç¤ºæ–¹æ¡ˆ
