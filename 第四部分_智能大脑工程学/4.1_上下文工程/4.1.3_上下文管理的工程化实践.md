# 4.1.3 上下文管理的工程化实践

## 学习目标

- 掌握上下文管理的工程化实现方法
- 理解Token限制下的上下文优化策略
- 学会构建高效的上下文压缩与选择机制

## 1. 上下文管理的四大核心挑战

上下文管理的工程化实践需要解决四个核心挑战，这些挑战直接影响AI应用的性能、可靠性和用户体验。

### 1.1 容量挑战：Token窗口限制与扩展策略

**挑战描述**

即使是最先进的大语言模型，其上下文窗口也存在物理限制。GPT-4的32K Token限制、Claude的200K Token限制，以及一些长上下文模型的2M Token限制，都无法满足复杂企业应用中海量信息处理的需求。

**技术表现**：
- 长对话中的信息截断问题
- 多文档处理时的容量不足
- 复杂任务链中的上下文丢失

**扩展策略实现**：

在JoyAgent-JDGenie项目中，通过多种技术策略解决容量挑战：

```java
public class TokenWindowManager {
    private static final int MAX_TOKENS = 8000;  // 模型上下文窗口限制
    private static final int RESERVED_TOKENS = 1000;  // 预留响应空间
    private static final int SAFE_TOKENS = MAX_TOKENS - RESERVED_TOKENS;
    
    private TokenCounter tokenCounter;
    
    public String optimizeContextForTokenLimit(Memory memory, String systemPrompt) {
        int currentTokens = tokenCounter.countTokens(systemPrompt);
        List<Message> messages = memory.getMessages();
        
        // 1. 保留系统提示和最新用户消息
        List<Message> preservedMessages = new ArrayList<>();
        preservedMessages.addAll(getSystemMessages(messages));
        preservedMessages.add(getLatestUserMessage(messages));
        
        currentTokens += tokenCounter.countTokens(preservedMessages);
        
        // 2. 从最新开始，逐步添加历史消息
        for (int i = messages.size() - 1; i >= 0; i--) {
            Message msg = messages.get(i);
            int msgTokens = tokenCounter.countTokens(msg);
            
            if (currentTokens + msgTokens < SAFE_TOKENS) {
                preservedMessages.add(0, msg);
                currentTokens += msgTokens;
            } else {
                // 达到限制，对剩余消息进行压缩
                compressRemainingMessages(messages.subList(0, i + 1));
                break;
            }
        }
        
        return buildContextString(preservedMessages);
    }
    
    private void compressRemainingMessages(List<Message> oldMessages) {
        // 使用LLM对旧消息进行摘要压缩
        String summary = llm.generateSummary(oldMessages, 
            "Please summarize the key information from these messages in 200 tokens or less.");
        
        // 替换旧消息为压缩摘要
        memory.replaceOldMessagesWithSummary(summary, oldMessages);
    }
}
```

**分层Token分配策略**：

```java
public class LayeredTokenAllocation {
    
    public class TokenBudget {
        private final int systemPromptTokens;
        private final int workingMemoryTokens;
        private final int shortTermMemoryTokens;
        private final int longTermMemoryTokens;
        private final int responseReservedTokens;
        
        public TokenBudget(int totalTokens) {
            // 按重要性和使用频率分配Token预算
            this.responseReservedTokens = (int) (totalTokens * 0.3);  // 30%用于响应
            this.systemPromptTokens = (int) (totalTokens * 0.2);      // 20%用于系统提示
            this.workingMemoryTokens = (int) (totalTokens * 0.25);    // 25%用于工作记忆
            this.shortTermMemoryTokens = (int) (totalTokens * 0.15);  // 15%用于短期记忆
            this.longTermMemoryTokens = (int) (totalTokens * 0.1);    // 10%用于长期记忆
        }
    }
    
    public String allocateContextByBudget(AgentContext context, TokenBudget budget) {
        StringBuilder contextBuilder = new StringBuilder();
        
        // 1. 系统提示（最高优先级）
        String systemPrompt = truncateToTokenLimit(
            context.getSystemPrompt(), budget.getSystemPromptTokens());
        contextBuilder.append(systemPrompt);
        
        // 2. 工作记忆（当前任务相关）
        String workingMemoryContext = selectWorkingMemoryContent(
            context, budget.getWorkingMemoryTokens());
        contextBuilder.append(workingMemoryContext);
        
        // 3. 短期记忆（会话历史）
        String shortTermContext = selectShortTermMemoryContent(
            context, budget.getShortTermMemoryTokens());
        contextBuilder.append(shortTermContext);
        
        // 4. 长期记忆（相关知识）
        String longTermContext = retrieveRelevantLongTermMemory(
            context.getQuery(), budget.getLongTermMemoryTokens());
        contextBuilder.append(longTermContext);
        
        return contextBuilder.toString();
    }
}
```

### 1.2 选择挑战：信息相关性评估与筛选算法

**挑战描述**

在有限的Token预算内，如何准确识别和选择最相关的信息是上下文管理的核心挑战。错误的选择可能导致重要信息缺失或噪声信息干扰。

**相关性评估算法实现**：

```java
public class ContextRelevanceEvaluator {
    
    private final SemanticSimilarityCalculator similarityCalculator;
    private final KeywordExtractor keywordExtractor;
    private final ImportanceScorer importanceScorer;
    
    public double calculateRelevanceScore(Message message, String currentQuery, AgentContext context) {
        double relevanceScore = 0.0;
        
        // 1. 语义相似度（40%权重）
        double semanticSimilarity = similarityCalculator.calculateSimilarity(
            message.getContent(), currentQuery);
        relevanceScore += semanticSimilarity * 0.4;
        
        // 2. 关键词匹配度（25%权重）
        double keywordMatch = calculateKeywordMatch(message, currentQuery);
        relevanceScore += keywordMatch * 0.25;
        
        // 3. 时间衰减因子（20%权重）
        double timeDecay = calculateTimeDecay(message.getTimestamp());
        relevanceScore += timeDecay * 0.2;
        
        // 4. 用户反馈权重（15%权重）
        double userFeedback = getUserFeedbackWeight(message, context);
        relevanceScore += userFeedback * 0.15;
        
        return Math.min(relevanceScore, 1.0);
    }
    
    private double calculateKeywordMatch(Message message, String query) {
        Set<String> messageKeywords = keywordExtractor.extractKeywords(message.getContent());
        Set<String> queryKeywords = keywordExtractor.extractKeywords(query);
        
        // 计算Jaccard相似度
        Set<String> intersection = new HashSet<>(messageKeywords);
        intersection.retainAll(queryKeywords);
        
        Set<String> union = new HashSet<>(messageKeywords);
        union.addAll(queryKeywords);
        
        return union.isEmpty() ? 0.0 : (double) intersection.size() / union.size();
    }
    
    private double calculateTimeDecay(long timestamp) {
        long currentTime = System.currentTimeMillis();
        long timeDiff = currentTime - timestamp;
        
        // 指数衰减，半衰期为24小时
        double hours = timeDiff / (1000.0 * 60 * 60);
        return Math.exp(-0.693 * hours / 24.0);
    }
}
```

**智能筛选机制**：

```java
public class IntelligentContextSelector {
    
    public List<Message> selectOptimalContext(List<Message> allMessages, 
                                            String currentQuery, 
                                            int tokenBudget,
                                            AgentContext context) {
        
        // 1. 计算每条消息的相关性得分
        List<ScoredMessage> scoredMessages = allMessages.stream()
            .map(msg -> new ScoredMessage(msg, 
                evaluator.calculateRelevanceScore(msg, currentQuery, context)))
            .collect(Collectors.toList());
        
        // 2. 按相关性排序
        scoredMessages.sort(Comparator.comparing(ScoredMessage::getScore).reversed());
        
        // 3. 使用贪心算法选择最优组合
        return selectGreedyOptimal(scoredMessages, tokenBudget);
    }
    
    private List<Message> selectGreedyOptimal(List<ScoredMessage> scoredMessages, int tokenBudget) {
        List<Message> selectedMessages = new ArrayList<>();
        int usedTokens = 0;
        
        for (ScoredMessage scoredMsg : scoredMessages) {
            int msgTokens = tokenCounter.countTokens(scoredMsg.getMessage());
            
            // 贪心选择：相关性/Token比最优的消息
            double efficiency = scoredMsg.getScore() / msgTokens;
            
            if (usedTokens + msgTokens <= tokenBudget && efficiency > 0.1) {
                selectedMessages.add(scoredMsg.getMessage());
                usedTokens += msgTokens;
            }
        }
        
        return selectedMessages;
    }
}
```

### 1.3 压缩挑战：无损与有损压缩技术选择

**挑战描述**

当信息量超出容量限制时，需要通过压缩技术减少Token占用，同时尽可能保留关键信息。选择合适的压缩策略直接影响信息的完整性和可用性。

**分层压缩策略**：

```java
public class LayeredCompressionStrategy {
    
    public enum CompressionLevel {
        LOSSLESS,    // 无损压缩：重新格式化、去除冗余
        LOW_LOSS,    // 低损压缩：关键信息提取
        MEDIUM_LOSS, // 中损压缩：摘要生成
        HIGH_LOSS    // 高损压缩：极简摘要
    }
    
    public String compressContent(String content, CompressionLevel level, int targetTokens) {
        switch (level) {
            case LOSSLESS:
                return losslessCompress(content, targetTokens);
            case LOW_LOSS:
                return lowLossCompress(content, targetTokens);
            case MEDIUM_LOSS:
                return mediumLossCompress(content, targetTokens);
            case HIGH_LOSS:
                return highLossCompress(content, targetTokens);
            default:
                return content;
        }
    }
    
    private String losslessCompress(String content, int targetTokens) {
        // 1. 去除多余空格和换行
        String compressed = content.replaceAll("\\s+", " ").trim();
        
        // 2. 缩写常见短语
        compressed = compressed.replaceAll("\\b(you are|you're)\\b", "you're");
        compressed = compressed.replaceAll("\\b(I am|I'm)\\b", "I'm");
        
        // 3. 移除礼貌用语（在保持语义的前提下）
        compressed = removePolitenessPhrases(compressed);
        
        return compressed;
    }
    
    private String mediumLossCompress(String content, int targetTokens) {
        // 使用LLM生成摘要
        String prompt = String.format(
            "Please summarize the following content in approximately %d tokens, " +
            "preserving all key information and important details:\n\n%s",
            targetTokens, content
        );
        
        return llm.generateResponse(prompt, targetTokens);
    }
}
```

**自适应压缩实现**：

```java
public class AdaptiveCompressionManager {
    
    public String adaptiveCompress(List<Message> messages, int targetTokens) {
        int currentTokens = tokenCounter.countTokens(messages);
        
        if (currentTokens <= targetTokens) {
            return formatMessages(messages);
        }
        
        // 计算压缩率
        double compressionRatio = (double) targetTokens / currentTokens;
        
        if (compressionRatio > 0.8) {
            // 轻微压缩：只进行格式优化
            return lightCompress(messages);
        } else if (compressionRatio > 0.5) {
            // 中等压缩：关键信息提取
            return moderateCompress(messages);
        } else {
            // 重度压缩：摘要生成
            return heavyCompress(messages, targetTokens);
        }
    }
    
    private String heavyCompress(List<Message> messages, int targetTokens) {
        // 分类消息
        List<Message> userMessages = messages.stream()
            .filter(msg -> msg.getRole() == RoleType.USER)
            .collect(Collectors.toList());
        
        List<Message> assistantMessages = messages.stream()
            .filter(msg -> msg.getRole() == RoleType.ASSISTANT)
            .collect(Collectors.toList());
        
        // 分别压缩并合并
        String userSummary = generateSummary(userMessages, targetTokens / 2);
        String assistantSummary = generateSummary(assistantMessages, targetTokens / 2);
        
        return String.format("User interactions summary: %s\nAssistant responses summary: %s", 
                           userSummary, assistantSummary);
    }
}
```

### 1.4 隔离挑战：多租户环境下的上下文安全

**挑战描述**

在多租户的企业环境中，确保不同用户和会话之间的上下文隔离，防止信息泄露和交叉污染。

**租户隔离机制**：

```java
public class TenantContextIsolation {
    
    private final Map<String, TenantMemoryNamespace> tenantNamespaces = new ConcurrentHashMap<>();
    
    public class TenantMemoryNamespace {
        private final String tenantId;
        private final Map<String, SessionContext> sessions;
        private final EncryptionService encryptionService;
        
        public void storeContext(String sessionId, Context context) {
            // 1. 数据加密
            String encryptedData = encryptionService.encrypt(context.serialize());
            
            // 2. 添加租户标识
            SessionContext sessionContext = new SessionContext(tenantId, sessionId, encryptedData);
            
            // 3. 存储到隔离命名空间
            sessions.put(sessionId, sessionContext);
        }
        
        public Context retrieveContext(String sessionId, String requestingTenantId) {
            // 验证租户权限
            if (!tenantId.equals(requestingTenantId)) {
                throw new SecurityException("Cross-tenant context access denied");
            }
            
            SessionContext sessionContext = sessions.get(sessionId);
            if (sessionContext == null) {
                return null;
            }
            
            // 解密并返回
            String decryptedData = encryptionService.decrypt(sessionContext.getEncryptedData());
            return Context.deserialize(decryptedData);
        }
    }
    
    public void initializeTenant(String tenantId, SecurityConfig securityConfig) {
        EncryptionService encryptionService = new EncryptionService(securityConfig);
        TenantMemoryNamespace namespace = new TenantMemoryNamespace(tenantId, encryptionService);
        tenantNamespaces.put(tenantId, namespace);
    }
}
```

## 2. 上下文管理的技术解决方案

### 2.1 滑动窗口策略：FIFO、LRU、重要性加权

**FIFO滑动窗口实现**：

```java
public class FIFOSlidingWindow {
    private final Queue<Message> messageWindow;
    private final int maxSize;
    
    public FIFOSlidingWindow(int maxSize) {
        this.maxSize = maxSize;
        this.messageWindow = new LinkedList<>();
    }
    
    public void addMessage(Message message) {
        if (messageWindow.size() >= maxSize) {
            messageWindow.poll(); // 移除最旧的消息
        }
        messageWindow.offer(message);
    }
    
    public List<Message> getWindowContent() {
        return new ArrayList<>(messageWindow);
    }
}
```

**LRU滑动窗口实现**：

```java
public class LRUSlidingWindow {
    private final LinkedHashMap<String, Message> messageMap;
    private final int maxSize;
    
    public LRUSlidingWindow(int maxSize) {
        this.maxSize = maxSize;
        this.messageMap = new LinkedHashMap<String, Message>(maxSize, 0.75f, true) {
            @Override
            protected boolean removeEldestEntry(Map.Entry<String, Message> eldest) {
                return size() > maxSize;
            }
        };
    }
    
    public void addMessage(Message message) {
        messageMap.put(message.getId(), message);
    }
    
    public void accessMessage(String messageId) {
        // 访问消息会更新其在LRU中的位置
        Message message = messageMap.get(messageId);
        if (message != null) {
            messageMap.put(messageId, message); // 重新插入以更新位置
        }
    }
}
```

**重要性加权滑动窗口**：

```java
public class ImportanceWeightedSlidingWindow {
    private final TreeSet<WeightedMessage> messageSet;
    private final int maxSize;
    private final ContextRelevanceEvaluator evaluator;
    
    private class WeightedMessage implements Comparable<WeightedMessage> {
        private final Message message;
        private final double importance;
        private final long timestamp;
        
        @Override
        public int compareTo(WeightedMessage other) {
            // 按重要性排序，重要性相同时按时间排序
            int importanceComparison = Double.compare(other.importance, this.importance);
            if (importanceComparison != 0) {
                return importanceComparison;
            }
            return Long.compare(other.timestamp, this.timestamp);
        }
    }
    
    public void addMessage(Message message, String currentQuery, AgentContext context) {
        double importance = evaluator.calculateRelevanceScore(message, currentQuery, context);
        WeightedMessage weightedMessage = new WeightedMessage(message, importance);
        
        messageSet.add(weightedMessage);
        
        // 移除重要性最低的消息
        if (messageSet.size() > maxSize) {
            messageSet.pollLast();
        }
    }
}
```

### 2.2 分层缓存机制：多级缓存与数据一致性

**多级缓存架构**：

```java
public class HierarchicalCacheManager {
    
    private final Cache<String, Object> l1Cache; // 内存缓存
    private final Cache<String, Object> l2Cache; // Redis缓存
    private final PersistentStorage l3Storage;   // 持久化存储
    
    public HierarchicalCacheManager() {
        this.l1Cache = CacheBuilder.newBuilder()
            .maximumSize(1000)
            .expireAfterAccess(10, TimeUnit.MINUTES)
            .build();
            
        this.l2Cache = RedisCache.builder()
            .expireAfterWrite(1, TimeUnit.HOURS)
            .build();
            
        this.l3Storage = new DatabaseStorage();
    }
    
    public Object get(String key) {
        // L1缓存查找
        Object value = l1Cache.getIfPresent(key);
        if (value != null) {
            return value;
        }
        
        // L2缓存查找
        value = l2Cache.getIfPresent(key);
        if (value != null) {
            l1Cache.put(key, value); // 回写L1缓存
            return value;
        }
        
        // L3存储查找
        value = l3Storage.get(key);
        if (value != null) {
            l2Cache.put(key, value); // 回写L2缓存
            l1Cache.put(key, value); // 回写L1缓存
        }
        
        return value;
    }
    
    public void put(String key, Object value) {
        // 写入所有层级
        l1Cache.put(key, value);
        l2Cache.put(key, value);
        
        // 异步写入持久化存储
        CompletableFuture.runAsync(() -> l3Storage.put(key, value));
    }
}
```

**数据一致性保障**：

```java
public class CacheConsistencyManager {
    
    private final List<CacheInvalidationListener> listeners = new ArrayList<>();
    
    public void updateWithConsistency(String key, Object newValue) {
        try {
            // 1. 开始分布式锁
            DistributedLock lock = acquireLock(key);
            
            // 2. 更新数据
            updateValue(key, newValue);
            
            // 3. 通知所有缓存层失效
            invalidateAllCaches(key);
            
            // 4. 释放锁
            lock.release();
            
        } catch (Exception e) {
            // 回滚操作
            rollbackUpdate(key);
            throw new CacheConsistencyException("Failed to maintain cache consistency", e);
        }
    }
    
    private void invalidateAllCaches(String key) {
        listeners.forEach(listener -> {
            try {
                listener.onInvalidate(key);
            } catch (Exception e) {
                log.error("Cache invalidation failed for key: " + key, e);
            }
        });
    }
}
```

### 2.3 语义压缩技术：关键信息提取与摘要生成

**关键信息提取器**：

```java
public class KeyInformationExtractor {
    
    private final NamedEntityRecognizer ner;
    private final KeywordExtractor keywordExtractor;
    private final SentenceImportanceScorer scorer;
    
    public CompressedContext extractKeyInformation(String originalContent, int targetTokens) {
        // 1. 命名实体识别
        List<NamedEntity> entities = ner.extractEntities(originalContent);
        
        // 2. 关键词提取
        List<String> keywords = keywordExtractor.extractKeywords(originalContent, 10);
        
        // 3. 重要句子识别
        List<String> sentences = sentenceSplitter.split(originalContent);
        List<ScoredSentence> scoredSentences = sentences.stream()
            .map(sentence -> new ScoredSentence(sentence, scorer.scoreImportance(sentence)))
            .sorted(Comparator.comparing(ScoredSentence::getScore).reversed())
            .collect(Collectors.toList());
        
        // 4. 构建压缩上下文
        return buildCompressedContext(entities, keywords, scoredSentences, targetTokens);
    }
    
    private CompressedContext buildCompressedContext(List<NamedEntity> entities,
                                                   List<String> keywords,
                                                   List<ScoredSentence> sentences,
                                                   int targetTokens) {
        StringBuilder compressed = new StringBuilder();
        int usedTokens = 0;
        
        // 1. 添加关键实体
        String entitySummary = formatEntities(entities);
        compressed.append("Key entities: ").append(entitySummary).append("\n");
        usedTokens += tokenCounter.countTokens(entitySummary);
        
        // 2. 添加关键词
        String keywordSummary = String.join(", ", keywords);
        compressed.append("Keywords: ").append(keywordSummary).append("\n");
        usedTokens += tokenCounter.countTokens(keywordSummary);
        
        // 3. 添加重要句子
        for (ScoredSentence sentence : sentences) {
            int sentenceTokens = tokenCounter.countTokens(sentence.getText());
            if (usedTokens + sentenceTokens <= targetTokens) {
                compressed.append(sentence.getText()).append(" ");
                usedTokens += sentenceTokens;
            } else {
                break;
            }
        }
        
        return new CompressedContext(compressed.toString(), usedTokens, entities, keywords);
    }
}
```

## 3. 基于项目实践的上下文管理案例

### 3.1 JoyAgent-JDGenie中的上下文优化实践

**内存清理策略实现**：

项目中通过`Memory.clearToolContext()`方法实现工具上下文的智能清理：

```java
public void clearToolContext() {
    Iterator<Message> iterator = messages.iterator();
    while (iterator.hasNext()) {
        Message message = iterator.next();
        
        // 移除工具执行结果
        if (message.getRole() == RoleType.TOOL) {
            iterator.remove();
        }
        
        // 移除包含工具调用的助手消息
        if (message.getRole() == RoleType.ASSISTANT && 
            Objects.nonNull(message.getToolCalls()) && 
            !message.getToolCalls().isEmpty()) {
            iterator.remove();
        }
        
        // 移除特定的推理步骤
        if (Objects.nonNull(message.getContent()) && 
            message.getContent().startsWith("根据当前状态和可用工具，确定下一步行动")) {
            iterator.remove();
        }
    }
}
```

**动态提示词构建**：

```java
// 在ExecutorAgent中实现动态的提示词构建
public ExecutorAgent(AgentContext context) {
    StringBuilder toolPrompt = new StringBuilder();
    for (BaseTool tool : context.getToolCollection().getToolMap().values()) {
        toolPrompt.append(String.format("工具名：%s 工具描述：%s\n", 
                                       tool.getName(), tool.getDescription()));
    }
    
    // 动态替换上下文变量
    String systemPrompt = genieConfig.getExecutorSystemPromptMap()
        .getOrDefault(promptKey, ToolCallPrompt.SYSTEM_PROMPT)
        .replace("{{tools}}", toolPrompt.toString())
        .replace("{{query}}", context.getQuery())
        .replace("{{date}}", context.getDateInfo())
        .replace("{{sopPrompt}}", context.getSopPrompt());
    
    setSystemPrompt(systemPrompt);
}
```

### 3.2 多Agent协同中的上下文共享

**PlanSolve模式下的上下文管理**：

```java
// PlanSolveHandlerImpl中的上下文共享机制
public String handle(AgentContext agentContext, AgentRequest request) {
    PlanningAgent planning = new PlanningAgent(agentContext);
    ExecutorAgent executor = new ExecutorAgent(agentContext);
    
    // 多个从执行器共享主执行器的记忆
    List<ExecutorAgent> slaveExecutors = new ArrayList<>();
    for (String task : planningResults) {
        ExecutorAgent slaveExecutor = new ExecutorAgent(agentContext);
        slaveExecutor.setState(executor.getState());
        
        // 共享记忆状态
        slaveExecutor.getMemory().addMessages(executor.getMemory().getMessages());
        slaveExecutors.add(slaveExecutor);
    }
    
    // 执行完毕后同步记忆状态
    for (ExecutorAgent slaveExecutor : slaveExecutors) {
        for (int i = memoryIndex; i < slaveExecutor.getMemory().size(); i++) {
            executor.getMemory().addMessage(slaveExecutor.getMemory().get(i));
        }
        slaveExecutor.getMemory().clear();
        executor.setState(slaveExecutor.getState());
    }
}
```

### 3.3 深度搜索中的上下文管理

基于项目中的深度搜索工具，展示上下文的动态管理：

```python
class DeepSearchContextManager:
    def __init__(self):
        self.current_docs = []
        self.searched_queries = []
        self.context_window_limit = 8000  # Token限制
    
    async def manage_search_context(self, query: str, max_loop: int = 1):
        """动态管理搜索上下文"""
        current_loop = 1
        
        while current_loop <= max_loop:
            # 查询分解
            sub_queries = await query_decompose(query=query)
            
            # 去除已经检索过的query，避免重复
            sub_queries = [sub_query for sub_query in sub_queries
                          if sub_query not in self.searched_queries]
            
            # 并行搜索并去重
            searched_docs, docs_list = await self._search_queries_and_dedup(
                queries=sub_queries,
                request_id=request_id,
            )
            
            # 更新上下文，考虑Token限制
            self._update_context_with_limit(searched_docs, sub_queries)
            
            current_loop += 1
    
    def _update_context_with_limit(self, new_docs, new_queries):
        """考虑Token限制的上下文更新"""
        # 计算当前上下文的Token使用量
        current_tokens = self._calculate_context_tokens()
        
        # 为新文档预留空间
        available_tokens = self.context_window_limit - current_tokens
        
        if available_tokens > 1000:  # 有足够空间
            self.current_docs.extend(new_docs)
            self.searched_queries.extend(new_queries)
        else:
            # 空间不足，需要压缩旧文档
            self._compress_old_context()
            self.current_docs.extend(new_docs)
            self.searched_queries.extend(new_queries)
    
    def _compress_old_context(self):
        """压缩旧的上下文信息"""
        if len(self.current_docs) > 5:
            # 保留最新的5个文档，压缩其余文档
            old_docs = self.current_docs[:-5]
            recent_docs = self.current_docs[-5:]
            
            # 生成旧文档的摘要
            compressed_summary = self._generate_doc_summary(old_docs)
            
            # 用摘要替换旧文档
            self.current_docs = [compressed_summary] + recent_docs
```

## 技术实践要点

### 性能优化建议

1. **异步处理**：上下文压缩和检索操作使用异步处理，避免阻塞主流程
2. **批量操作**：合并相似的上下文操作，减少系统调用开销
3. **智能预取**：基于用户行为模式预取可能需要的上下文信息
4. **资源池化**：使用连接池和线程池管理上下文处理资源

### 监控与调试

1. **Token使用监控**：实时监控各层级的Token使用情况
2. **压缩效果评估**：定期评估压缩策略的效果和信息损失程度
3. **性能指标追踪**：监控上下文操作的延迟和吞吐量
4. **错误恢复机制**：建立上下文损坏时的恢复和重建机制

## 本节小结

上下文管理的工程化实践需要综合考虑容量、选择、压缩和隔离四大核心挑战。通过合理的技术策略选择和工程实现，可以在有限的资源约束下构建高效、可靠的上下文管理系统。

关键在于根据具体的应用场景和业务需求，灵活组合不同的技术方案，并建立完善的监控和优化机制。在实际项目中，需要不断迭代优化上下文管理策略，以适应业务发展和用户需求的变化。