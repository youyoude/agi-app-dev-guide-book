# 2.1.7 企业级部署与运维

**学习目标：** 掌握流式通信应用的企业级部署方案，学会运维管理的最佳实践

## 负载均衡器的流式通信支持配置

### Nginx流式代理配置

在企业级部署中，Nginx是最常用的反向代理服务器。针对流式通信（SSE），需要特别的配置来确保连接的稳定性和性能：

```nginx
# nginx.conf
http {
    # 全局配置
    upstream streaming_backend {
        # 使用IP哈希确保会话粘性
        ip_hash;
        server backend1.example.com:8080 max_fails=3 fail_timeout=30s;
        server backend2.example.com:8080 max_fails=3 fail_timeout=30s;
        server backend3.example.com:8080 max_fails=3 fail_timeout=30s;
        
        # 健康检查配置
        keepalive 32;
        keepalive_requests 1000;
        keepalive_timeout 60s;
    }

    # 流式通信专用配置
    server {
        listen 443 ssl http2;
        server_name api.example.com;
        
        # SSL配置
        ssl_certificate /path/to/certificate.crt;
        ssl_certificate_key /path/to/private.key;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384;
        
        # 流式通信路径配置
        location /web/api/v1/gpt/queryAgentStreamIncr {
            proxy_pass http://streaming_backend;
            
            # SSE关键配置
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # 禁用缓冲，确保实时传输
            proxy_buffering off;
            proxy_cache off;
            proxy_request_buffering off;
            
            # 超时配置
            proxy_connect_timeout 60s;
            proxy_send_timeout 3600s;      # 1小时
            proxy_read_timeout 3600s;      # 1小时
            
            # 保持连接
            proxy_set_header Connection "";
            
            # 添加CORS头（如果需要）
            add_header Access-Control-Allow-Origin *;
            add_header Access-Control-Allow-Methods 'GET, POST, OPTIONS';
            add_header Access-Control-Allow-Headers 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization';
            
            # 处理预检请求
            if ($request_method = 'OPTIONS') {
                add_header Access-Control-Allow-Origin *;
                add_header Access-Control-Max-Age 1728000;
                add_header Content-Type 'text/plain; charset=utf-8';
                add_header Content-Length 0;
                return 204;
            }
        }
        
        # 静态资源和API请求
        location / {
            proxy_pass http://streaming_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # 正常的代理缓冲设置
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
            proxy_busy_buffers_size 8k;
            
            proxy_connect_timeout 30s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
        
        # 健康检查端点
        location /health {
            access_log off;
            proxy_pass http://streaming_backend/actuator/health;
            proxy_connect_timeout 5s;
            proxy_send_timeout 5s;
            proxy_read_timeout 5s;
        }
    }
    
    # 监控和日志配置
    access_log /var/log/nginx/streaming_access.log combined buffer=32k flush=1m;
    error_log /var/log/nginx/streaming_error.log warn;
    
    # 性能优化
    worker_processes auto;
    worker_connections 4096;
    worker_rlimit_nofile 8192;
    
    # 连接池优化
    events {
        use epoll;
        multi_accept on;
    }
}
```

### HAProxy流式代理配置

HAProxy是另一个优秀的负载均衡器选择，特别适合处理长连接：

```haproxy
# haproxy.cfg
global
    daemon
    maxconn 4096
    log stdout local0
    
    # SSL配置
    ssl-default-bind-ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384
    ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets
    
defaults
    mode http
    timeout connect 10s
    timeout client 1h        # 客户端连接超时1小时
    timeout server 1h        # 服务器连接超时1小时
    timeout tunnel 1h        # 隧道超时1小时
    option httplog
    option dontlognull
    option log-health-checks
    
frontend streaming_frontend
    bind *:443 ssl crt /path/to/certificate.pem
    bind *:80
    redirect scheme https if !{ ssl_fc }
    
    # 根据路径分发
    acl is_streaming path_beg /web/api/v1/gpt/queryAgentStreamIncr
    acl is_health path /health
    
    use_backend streaming_backend if is_streaming
    use_backend health_backend if is_health
    default_backend default_backend
    
    # CORS处理
    http-response add-header Access-Control-Allow-Origin *
    http-response add-header Access-Control-Allow-Methods 'GET, POST, OPTIONS'
    http-response add-header Access-Control-Allow-Headers 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization'

backend streaming_backend
    balance source  # 使用源IP哈希，确保会话粘性
    
    # 服务器配置
    server backend1 backend1.example.com:8080 check inter 10s rise 3 fall 2 maxconn 500
    server backend2 backend2.example.com:8080 check inter 10s rise 3 fall 2 maxconn 500
    server backend3 backend3.example.com:8080 check inter 10s rise 3 fall 2 maxconn 500
    
    # 流式通信优化
    option http-server-close
    option prefer-last-server  # 优先使用最后一个服务器
    
    # 健康检查
    option httpchk GET /actuator/health HTTP/1.1\r\nHost:\ backend-health
    http-check expect status 200
    
backend health_backend
    balance roundrobin
    server backend1 backend1.example.com:8080 check
    server backend2 backend2.example.com:8080 check
    server backend3 backend3.example.com:8080 check

backend default_backend
    balance roundrobin
    server backend1 backend1.example.com:8080 check
    server backend2 backend2.example.com:8080 check
    server backend3 backend3.example.com:8080 check

# 统计页面
listen stats
    bind *:8404
    stats enable
    stats uri /stats
    stats refresh 30s
    stats admin if TRUE
```

### 会话粘性策略

流式通信需要特殊的会话粘性处理：

```java
@Component
public class SessionAffinityManager {
    private final RedisTemplate<String, String> redisTemplate;
    private final ConsistentHash<String> consistentHash;

    public SessionAffinityManager(RedisTemplate<String, String> redisTemplate) {
        this.redisTemplate = redisTemplate;
        this.consistentHash = new ConsistentHash<>(getBackendServers());
    }

    /**
     * 获取会话应该路由到的后端服务器
     */
    public String getBackendServer(String sessionId, String clientIP) {
        // 1. 首先检查Redis中是否已有会话绑定
        String existingServer = redisTemplate.opsForValue()
            .get("session:affinity:" + sessionId);
        
        if (existingServer != null && isServerHealthy(existingServer)) {
            return existingServer;
        }
        
        // 2. 使用一致性哈希选择服务器
        String selectedServer = consistentHash.get(clientIP);
        
        // 3. 将会话绑定到服务器
        redisTemplate.opsForValue().set(
            "session:affinity:" + sessionId, 
            selectedServer,
            Duration.ofHours(2) // 2小时过期
        );
        
        return selectedServer;
    }

    /**
     * 处理服务器故障转移
     */
    public String handleServerFailover(String sessionId, String failedServer) {
        // 1. 从可用服务器中选择新的服务器
        List<String> availableServers = getHealthyServers();
        availableServers.remove(failedServer);
        
        if (availableServers.isEmpty()) {
            throw new RuntimeException("没有可用的后端服务器");
        }
        
        // 2. 使用负载最低的服务器
        String newServer = selectLeastLoadedServer(availableServers);
        
        // 3. 更新会话绑定
        redisTemplate.opsForValue().set(
            "session:affinity:" + sessionId,
            newServer,
            Duration.ofHours(2)
        );
        
        // 4. 记录故障转移日志
        log.info("会话故障转移: sessionId={}, from={}, to={}", 
                sessionId, failedServer, newServer);
        
        return newServer;
    }

    private boolean isServerHealthy(String server) {
        // 实现服务器健康检查逻辑
        try {
            RestTemplate restTemplate = new RestTemplate();
            ResponseEntity<String> response = restTemplate.getForEntity(
                "http://" + server + "/actuator/health", 
                String.class
            );
            return response.getStatusCode().is2xxSuccessful();
        } catch (Exception e) {
            return false;
        }
    }

    private String selectLeastLoadedServer(List<String> servers) {
        // 实现负载检测逻辑，返回负载最低的服务器
        return servers.stream()
            .min(Comparator.comparing(this::getServerLoad))
            .orElse(servers.get(0));
    }

    private Double getServerLoad(String server) {
        // 从监控系统获取服务器负载
        try {
            String loadInfo = redisTemplate.opsForValue()
                .get("server:load:" + server);
            return loadInfo != null ? Double.parseDouble(loadInfo) : 100.0;
        } catch (Exception e) {
            return 100.0; // 默认高负载
        }
    }
}
```

## 网关层面的流式代理处理

### Spring Cloud Gateway配置

```yaml
# application.yml
spring:
  cloud:
    gateway:
      routes:
        # 流式通信路由
        - id: streaming-route
          uri: lb://streaming-service
          predicates:
            - Path=/web/api/v1/gpt/queryAgentStreamIncr
          filters:
            - name: CircuitBreaker
              args:
                name: streaming-circuit-breaker
                fallback-uri: forward:/streaming-fallback
            - name: Retry
              args:
                retries: 3
                statuses: BAD_GATEWAY,GATEWAY_TIMEOUT
                methods: POST
            - name: RequestRateLimiter
              args:
                redis-rate-limiter:
                  replenish-rate: 100  # 每秒补充令牌数
                  burst-capacity: 200  # 令牌桶容量
                key-resolver: "#{@ipKeyResolver}"
            - name: ModifyRequestBody
              args:
                content-type: application/json
        
        # 普通API路由
        - id: api-route
          uri: lb://streaming-service
          predicates:
            - Path=/web/api/**
            - "!Path=/web/api/v1/gpt/queryAgentStreamIncr"
          filters:
            - name: CircuitBreaker
              args:
                name: api-circuit-breaker
            - name: RequestRateLimiter
              args:
                redis-rate-limiter:
                  replenish-rate: 500
                  burst-capacity: 1000

  # 超时配置
  cloud:
    gateway:
      httpclient:
        connect-timeout: 10000
        response-timeout: 3600s  # 流式接口需要长时间超时
        pool:
          type: elastic
          max-idle-time: 15s
          max-life-time: 60s

# 监控配置  
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,gateway
  endpoint:
    gateway:
      enabled: true
  metrics:
    export:
      prometheus:
        enabled: true

# 日志配置
logging:
  level:
    org.springframework.cloud.gateway: INFO
    reactor.netty: INFO
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
```

### 自定义网关过滤器

```java
@Component
public class StreamingGatewayFilterFactory extends AbstractGatewayFilterFactory<StreamingGatewayFilterFactory.Config> {

    public StreamingGatewayFilterFactory() {
        super(Config.class);
    }

    @Override
    public GatewayFilter apply(Config config) {
        return (exchange, chain) -> {
            ServerHttpRequest request = exchange.getRequest();
            
            // 检查是否为流式请求
            if (isStreamingRequest(request)) {
                return handleStreamingRequest(exchange, chain, config);
            }
            
            return chain.filter(exchange);
        };
    }

    private Mono<Void> handleStreamingRequest(ServerWebExchange exchange, 
                                            GatewayFilterChain chain, 
                                            Config config) {
        ServerHttpRequest request = exchange.getRequest();
        ServerHttpResponse response = exchange.getResponse();
        
        // 设置流式响应头
        response.getHeaders().set(HttpHeaders.CONTENT_TYPE, "text/event-stream");
        response.getHeaders().set(HttpHeaders.CACHE_CONTROL, "no-cache");
        response.getHeaders().set(HttpHeaders.CONNECTION, "keep-alive");
        response.getHeaders().set("X-Accel-Buffering", "no"); // Nginx禁用缓冲
        
        // 添加会话跟踪
        String sessionId = extractSessionId(request);
        if (sessionId != null) {
            response.getHeaders().set("X-Session-ID", sessionId);
        }
        
        // 记录流式请求开始
        long startTime = System.currentTimeMillis();
        logStreamingStart(request, sessionId);
        
        return chain.filter(exchange)
            .doFinally(signalType -> {
                // 记录流式请求结束
                long duration = System.currentTimeMillis() - startTime;
                logStreamingEnd(request, sessionId, duration, signalType);
            });
    }

    private boolean isStreamingRequest(ServerHttpRequest request) {
        return request.getPath().value().contains("StreamIncr") ||
               request.getHeaders().getFirst("Accept").contains("text/event-stream");
    }

    private String extractSessionId(ServerHttpRequest request) {
        // 从请求头或请求体中提取会话ID
        String sessionId = request.getHeaders().getFirst("X-Session-ID");
        if (sessionId == null) {
            sessionId = request.getQueryParams().getFirst("sessionId");
        }
        return sessionId;
    }

    private void logStreamingStart(ServerHttpRequest request, String sessionId) {
        log.info("流式请求开始: path={}, sessionId={}, clientIP={}", 
                request.getPath().value(), 
                sessionId, 
                getClientIP(request));
    }

    private void logStreamingEnd(ServerHttpRequest request, String sessionId, 
                                long duration, SignalType signalType) {
        log.info("流式请求结束: path={}, sessionId={}, duration={}ms, signalType={}", 
                request.getPath().value(), 
                sessionId, 
                duration, 
                signalType);
    }

    private String getClientIP(ServerHttpRequest request) {
        String xForwardedFor = request.getHeaders().getFirst("X-Forwarded-For");
        if (xForwardedFor != null && !xForwardedFor.isEmpty()) {
            return xForwardedFor.split(",")[0].trim();
        }
        
        String xRealIP = request.getHeaders().getFirst("X-Real-IP");
        if (xRealIP != null && !xRealIP.isEmpty()) {
            return xRealIP;
        }
        
        return request.getRemoteAddress() != null ? 
               request.getRemoteAddress().getAddress().getHostAddress() : "unknown";
    }

    @Data
    public static class Config {
        private boolean enableSessionTracking = true;
        private boolean enableMetrics = true;
        private long maxStreamingDuration = 3600000; // 1小时
    }
}

@Component
public class IpKeyResolver implements KeyResolver {
    @Override
    public Mono<String> resolve(ServerWebExchange exchange) {
        return Mono.just(getClientIP(exchange.getRequest()));
    }

    private String getClientIP(ServerHttpRequest request) {
        String xForwardedFor = request.getHeaders().getFirst("X-Forwarded-For");
        if (xForwardedFor != null && !xForwardedFor.isEmpty()) {
            return xForwardedFor.split(",")[0].trim();
        }
        return request.getRemoteAddress().getAddress().getHostAddress();
    }
}
```

## 容器化环境下的流式通信管理

### 流式通信专用Docker配置

```dockerfile
# Dockerfile - 针对流式通信优化
FROM openjdk:17-jdk-slim

# 安装必要的系统工具和流式通信相关工具
RUN apt-get update && apt-get install -y \
    curl \
    netcat-openbsd \
    procps \
    tcpdump \
    ss \
    && rm -rf /var/lib/apt/lists/*

# 创建应用用户
RUN groupadd -r appgroup && useradd -r -g appgroup appuser

# 设置工作目录
WORKDIR /app

# 复制应用文件
COPY target/streaming-app.jar app.jar
COPY docker-entrypoint.sh /usr/local/bin/
COPY streaming-healthcheck.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/docker-entrypoint.sh /usr/local/bin/streaming-healthcheck.sh

# 流式通信专用JVM参数优化
ENV JAVA_OPTS="-Xms512m -Xmx2g \
    -XX:+UseG1GC \
    -XX:MaxGCPauseMillis=50 \
    -XX:+HeapDumpOnOutOfMemoryError \
    -XX:HeapDumpPath=/app/logs/ \
    -Dspring.profiles.active=docker \
    -Dreactor.netty.ioWorkerCount=8 \
    -Dreactor.netty.pool.maxConnections=1000 \
    -Dspring.webflux.multipart.streaming=true"

# 暴露端口 - 8080主服务，8081管理端口，8082 WebSocket端口
EXPOSE 8080 8081 8082

# 流式通信专用健康检查
HEALTHCHECK --interval=15s --timeout=5s --start-period=60s --retries=5 \
    CMD /usr/local/bin/streaming-healthcheck.sh

# 设置系统级别的连接优化
RUN echo "net.core.somaxconn = 65535" >> /etc/sysctl.conf && \
    echo "net.core.netdev_max_backlog = 5000" >> /etc/sysctl.conf && \
    echo "net.ipv4.tcp_max_syn_backlog = 65535" >> /etc/sysctl.conf

# 切换到非root用户
USER appuser

ENTRYPOINT ["docker-entrypoint.sh"]
CMD ["java", "-jar", "app.jar"]
```

### 流式通信专用启动脚本

```bash
#!/bin/bash
# docker-entrypoint.sh - 流式通信应用启动脚本

set -e

# 流式通信环境检查
check_streaming_environment() {
    echo "检查流式通信环境..."
    
    # 检查网络连接能力
    if ! nc -z localhost 8080 2>/dev/null; then
        echo "端口8080检查通过"
    fi
    
    # 检查系统资源
    MEMORY_MB=$(free -m | awk 'NR==2{printf "%d", $7}')
    if [ $MEMORY_MB -lt 256 ]; then
        echo "警告: 可用内存不足256MB，可能影响流式通信性能"
    fi
    
    # 检查文件描述符限制
    ULIMIT_N=$(ulimit -n)
    if [ $ULIMIT_N -lt 65535 ]; then
        echo "警告: 文件描述符限制为 $ULIMIT_N，建议设置为65535以支持更多并发连接"
        ulimit -n 65535 2>/dev/null || echo "无法调整文件描述符限制"
    fi
}

# 流式通信专用JVM调优
setup_streaming_jvm() {
    echo "配置流式通信JVM参数..."
    
    # 添加流式通信专用参数
    export JAVA_OPTS="$JAVA_OPTS \
        -Dserver.netty.connection-timeout=30000 \
        -Dspring.webflux.multipart.max-in-memory-size=10MB \
        -Dspring.webflux.multipart.max-disk-usage-per-part=100MB \
        -Dreactor.netty.pool.maxIdleTime=60000 \
        -Dreactor.netty.pool.maxLifeTime=3600000"
    
    echo "JVM参数配置完成: $JAVA_OPTS"
}

# 启动流式通信监控
start_streaming_monitor() {
    echo "启动流式通信连接监控..."
    
    # 后台监控脚本
    (
        while true; do
            CONNECTIONS=$(ss -tan | grep :8080 | wc -l)
            WEBSOCKET_CONNECTIONS=$(ss -tan | grep :8082 | wc -l)
            echo "$(date): HTTP连接数: $CONNECTIONS, WebSocket连接数: $WEBSOCKET_CONNECTIONS" >> /app/logs/connections.log
            sleep 30
        done
    ) &
}

# 主启动流程
main() {
    echo "启动流式通信应用..."
    
    # 创建日志目录
    mkdir -p /app/logs
    
    # 环境检查
    check_streaming_environment
    
    # JVM配置
    setup_streaming_jvm
    
    # 启动监控
    start_streaming_monitor
    
    echo "流式通信应用启动完成"
    
    # 执行传入的命令
    exec "$@"
}

main "$@"
```

### 流式通信专用健康检查脚本

```bash
#!/bin/bash
# streaming-healthcheck.sh - 流式通信健康检查

# 基本HTTP健康检查
if ! curl -f -s http://localhost:8080/actuator/health >/dev/null 2>&1; then
    echo "HTTP健康检查失败"
    exit 1
fi

# 流式端点检查
if ! curl -f -s -H "Accept: text/event-stream" http://localhost:8080/actuator/health/streaming >/dev/null 2>&1; then
    echo "流式端点健康检查失败"
    exit 1
fi

# WebSocket连接检查
if ! nc -z localhost 8082 2>/dev/null; then
    echo "WebSocket端口检查失败"
    exit 1
fi

# 检查活跃连接数
ACTIVE_CONNECTIONS=$(ss -tan | grep :8080 | grep ESTABLISHED | wc -l)
if [ $ACTIVE_CONNECTIONS -gt 1000 ]; then
    echo "警告: 活跃连接数过高 ($ACTIVE_CONNECTIONS)"
    # 不退出，只是警告
fi

# 检查内存使用
MEMORY_USAGE=$(ps -o pid,ppid,cmd,%mem --sort=-%mem | grep java | head -1 | awk '{print $4}')
if [ $(echo "$MEMORY_USAGE > 90" | bc -l 2>/dev/null || echo 0) -eq 1 ]; then
    echo "警告: 内存使用率过高 ($MEMORY_USAGE%)"
fi

echo "流式通信健康检查通过"
exit 0
```

### 流式通信专用Docker Compose配置

```yaml
# docker-compose.yml - 流式通信服务编排
version: '3.8'

services:
  # 流式通信应用主服务
  streaming-app:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: streaming-app
    ports:
      - "8080:8080"   # HTTP/SSE端口
      - "8082:8082"   # WebSocket端口
      - "8081:8081"   # 管理端口
    environment:
      - SPRING_PROFILES_ACTIVE=docker,streaming
      - SPRING_REDIS_HOST=redis
      - SPRING_REDIS_PORT=6379
      - MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE=health,metrics,prometheus,streaming
    volumes:
      - ./logs:/app/logs
      - streaming_temp:/tmp
    networks:
      - streaming-network
    depends_on:
      - redis
      - nginx
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.5'
        reservations:
          memory: 512M
          cpus: '0.5'
    restart: unless-stopped
    sysctls:
      - net.core.somaxconn=65535
      - net.core.netdev_max_backlog=5000
      - net.ipv4.tcp_max_syn_backlog=65535
    ulimits:
      nofile:
        soft: 65535
        hard: 65535

  # Redis - 用于流式会话管理和消息队列
  redis:
    image: redis:7-alpine
    container_name: streaming-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - streaming-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    restart: unless-stopped

  # Nginx - 流式通信负载均衡和代理
  nginx:
    image: nginx:alpine
    container_name: streaming-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/streaming.conf:/etc/nginx/conf.d/streaming.conf
      - ./ssl:/etc/nginx/ssl
    networks:
      - streaming-network
    depends_on:
      - streaming-app
    restart: unless-stopped

  # Prometheus - 流式通信监控
  prometheus:
    image: prom/prometheus:latest
    container_name: streaming-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - streaming-network
    restart: unless-stopped

  # Grafana - 流式通信指标可视化
  grafana:
    image: grafana/grafana:latest
    container_name: streaming-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=streaming_admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
    networks:
      - streaming-network
    restart: unless-stopped

volumes:
  redis_data:
  prometheus_data:
  grafana_data:
  streaming_temp:

networks:
  streaming-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

### 流式通信专用Nginx配置

```nginx
# nginx/streaming.conf - 流式通信专用代理配置
upstream streaming_backend {
    # 使用ip_hash确保WebSocket连接的会话粘性
    ip_hash;
    
    server streaming-app:8080 max_fails=3 fail_timeout=30s;
    # 可以添加更多后端实例
    # server streaming-app-2:8080 max_fails=3 fail_timeout=30s;
    
    keepalive 64;
}

upstream websocket_backend {
    # WebSocket专用后端
    server streaming-app:8082 max_fails=3 fail_timeout=30s;
    keepalive 64;
}

# HTTP/SSE服务配置
server {
    listen 80;
    server_name streaming.example.com;
    
    # 流式通信专用配置
    client_max_body_size 100M;
    client_body_timeout 60s;
    client_header_timeout 60s;
    keepalive_timeout 65s;
    send_timeout 60s;
    
    # 禁用缓冲，确保流式数据实时传输
    proxy_buffering off;
    proxy_cache off;
    proxy_request_buffering off;
    
    # SSE/EventStream专用路径
    location /api/stream/ {
        proxy_pass http://streaming_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "keep-alive";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # SSE专用头部
        proxy_set_header Cache-Control "no-cache";
        proxy_set_header Connection "keep-alive";
        
        # 流式传输超时设置
        proxy_connect_timeout 60s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
        
        # 禁用所有缓冲
        proxy_buffering off;
        proxy_cache off;
        proxy_request_buffering off;
        
        # Nginx特殊指令，禁用缓冲
        add_header X-Accel-Buffering no;
    }
    
    # WebSocket专用路径
    location /ws/ {
        proxy_pass http://websocket_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # WebSocket超时设置
        proxy_connect_timeout 60s;
        proxy_send_timeout 3600s;
        proxy_read_timeout 3600s;
        
        # WebSocket保活
        proxy_set_header Sec-WebSocket-Extensions $http_sec_websocket_extensions;
        proxy_set_header Sec-WebSocket-Key $http_sec_websocket_key;
        proxy_set_header Sec-WebSocket-Version $http_sec_websocket_version;
    }
    
    # 普通HTTP API
    location /api/ {
        proxy_pass http://streaming_backend;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }
    
    # 健康检查端点
    location /health {
        proxy_pass http://streaming_backend/actuator/health;
        access_log off;
    }
    
    # 流式通信指标监控
    location /metrics {
        proxy_pass http://streaming_backend/actuator/prometheus;
        allow 172.20.0.0/16;  # 只允许内网访问
        deny all;
    }
}
```

### 流式通信专用Redis配置

```conf
# redis.conf - 流式通信优化配置
port 6379
bind 0.0.0.0

# 流式会话管理优化
maxmemory 256mb
maxmemory-policy allkeys-lru

# 持久化配置 - 适合流式会话数据
save 900 1
save 300 10
save 60 10000

# 网络优化
tcp-keepalive 300
timeout 0

# 流式消息队列优化
stream-node-max-bytes 4096
stream-node-max-entries 100

# 连接池优化
maxclients 10000

# 日志配置
loglevel notice
logfile "/var/log/redis/redis-server.log"

# AOF配置 - 确保流式数据不丢失
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec

# 客户端缓冲区限制
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
```

### Kubernetes部署配置

```yaml
# k8s-streaming-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: streaming-app
  labels:
    app: streaming-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: streaming-app
  template:
    metadata:
      labels:
        app: streaming-app
    spec:
      containers:
      - name: streaming-app
        image: streaming-app:latest
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 8082
          name: websocket
        - containerPort: 8081
          name: management
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "k8s,streaming"
        - name: SPRING_REDIS_HOST
          value: "redis-service"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1500m"
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8081
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8081
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        # 流式通信专用启动探针
        startupProbe:
          httpGet:
            path: /actuator/health/streaming
            port: 8081
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 10
        volumeMounts:
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: logs
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: streaming-app-service
spec:
  selector:
    app: streaming-app
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  - name: websocket
    port: 8082
    targetPort: 8082
  - name: management
    port: 8081
    targetPort: 8081
  type: ClusterIP
---
# 流式通信专用Ingress配置
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: streaming-ingress
  annotations:
    nginx.ingress.kubernetes.io/proxy-buffering: "off"
    nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    nginx.ingress.kubernetes.io/server-snippet: |
      location ~* ^/api/stream/ {
        proxy_buffering off;
        proxy_cache off;
        proxy_set_header Connection "";
        add_header X-Accel-Buffering no;
      }
    # WebSocket支持
    nginx.ingress.kubernetes.io/websocket-services: "streaming-app-service"
spec:
  rules:
  - host: streaming.example.com
    http:
      paths:
      - path: /api/stream
        pathType: Prefix
        backend:
          service:
            name: streaming-app-service
            port:
              number: 8080
      - path: /ws
        pathType: Prefix
        backend:
          service:
            name: streaming-app-service
            port:
              number: 8082
      - path: /
        pathType: Prefix
        backend:
          service:
            name: streaming-app-service
            port:
              number: 8080
```

### 流式通信专用监控配置

```yaml
# prometheus/prometheus.yml - 流式通信监控配置
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "streaming_rules.yml"

scrape_configs:
  - job_name: 'streaming-app'
    static_configs:
      - targets: ['streaming-app:8081']
    scrape_interval: 5s
    metrics_path: '/actuator/prometheus'
    
  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
    scrape_interval: 10s

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx:9113']
    scrape_interval: 10s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

```yaml
# prometheus/streaming_rules.yml - 流式通信告警规则
groups:
- name: streaming_alerts
  rules:
  - alert: HighStreamingConnections
    expr: streaming_active_connections > 800
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "流式连接数过高"
      description: "当前活跃流式连接数为 {{ $value }}，超过阈值800"
      
  - alert: StreamingLatencyHigh
    expr: histogram_quantile(0.95, streaming_request_duration_seconds_bucket) > 5
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "流式响应延迟过高"
      description: "95%的流式请求延迟超过5秒"
      
  - alert: WebSocketConnectionDrop
    expr: increase(websocket_connection_errors_total[5m]) > 10
    for: 1m
    labels:
      severity: warning
    annotations:
      summary: "WebSocket连接异常断开"
      description: "5分钟内WebSocket连接错误数量: {{ $value }}"
      
  - alert: StreamingMemoryUsageHigh
    expr: process_resident_memory_bytes / 1024 / 1024 > 1800
    for: 3m
    labels:
      severity: warning
    annotations:
      summary: "流式应用内存使用率过高"
      description: "内存使用量: {{ $value }}MB，超过1.8GB阈值"
```

## 流式通信部署最佳实践

### 1. 容器资源配置建议

```yaml
# 生产环境资源配置
resources:
  requests:
    memory: "1Gi"      # 基础内存需求
    cpu: "500m"        # 基础CPU需求
  limits:
    memory: "4Gi"      # 最大内存限制
    cpu: "2000m"       # 最大CPU限制

# 流式通信专用配置
sysctls:
  - net.core.somaxconn=65535           # 增加连接队列长度
  - net.core.netdev_max_backlog=5000   # 网络设备队列长度
  - net.ipv4.tcp_max_syn_backlog=65535 # TCP SYN队列长度

ulimits:
  nofile:
    soft: 65535    # 软限制：文件描述符数量
    hard: 65535    # 硬限制：文件描述符数量
```

### 2. 环境变量配置

```bash
# 流式通信专用环境变量
SPRING_PROFILES_ACTIVE=production,streaming
REACTOR_NETTY_IOWORKERCOUNT=8
REACTOR_NETTY_POOL_MAXCONNECTIONS=1000
SPRING_WEBFLUX_MULTIPART_STREAMING=true

# JVM优化参数
JAVA_OPTS="-Xms1g -Xmx3g \
  -XX:+UseG1GC \
  -XX:MaxGCPauseMillis=50 \
  -XX:+HeapDumpOnOutOfMemoryError \
  -Dreactor.netty.pool.maxIdleTime=60000 \
  -Dreactor.netty.pool.maxLifeTime=3600000"
```

### 3. 部署验证脚本

```bash
#!/bin/bash
# 流式通信部署验证脚本

echo "开始验证流式通信部署..."

# 1. 检查基本健康状态
echo "1. 检查应用健康状态..."
curl -f http://localhost:8080/actuator/health || exit 1

# 2. 检查流式端点
echo "2. 检查SSE流式端点..."
timeout 10 curl -N -H "Accept: text/event-stream" \
  http://localhost:8080/api/stream/test || echo "SSE端点检查完成"

# 3. 检查WebSocket连接
echo "3. 检查WebSocket连接..."
nc -z localhost 8082 || exit 1

# 4. 检查连接数
echo "4. 检查活跃连接数..."
CONNECTIONS=$(ss -tan | grep :8080 | grep ESTABLISHED | wc -l)
echo "当前活跃连接数: $CONNECTIONS"

# 5. 检查内存使用
echo "5. 检查内存使用情况..."
MEMORY_USAGE=$(ps -o pid,ppid,cmd,%mem --sort=-%mem | grep java | head -1 | awk '{print $4}')
echo "当前内存使用率: $MEMORY_USAGE%"

echo "流式通信部署验证完成！"
```

## 总结

通过以上配置，我们实现了一个完整的流式通信企业级部署方案：

### 核心特性

1. **流式通信专用优化**：
   - JVM参数针对流式处理优化
   - 网络连接池配置
   - 系统级连接数限制调整

2. **容器化支持**：
   - 专用的Docker镜像配置
   - 流式通信健康检查
   - 连接监控和日志记录

3. **负载均衡和代理**：
   - Nginx专用配置支持SSE和WebSocket
   - 禁用缓冲确保实时传输
   - 会话粘性支持

4. **监控和告警**：
   - Prometheus指标收集
   - 流式通信专用告警规则
   - Grafana可视化面板

5. **Kubernetes部署**：
   - 多实例部署支持
   - 资源限制和调度策略
   - 优雅关闭和滚动更新

这样的配置确保了流式通信应用在企业级环境中的稳定运行和高性能表现。
